这一页 PPT 讲的是：**模型算完以后，你该如何判断“它到底靠不靠谱”。**
拟合出一个 $\hat{\beta}$ 只是开始，真正成熟的建模一定要做**诊断**。

我按“残差 → 杠杆 → 量化指标”这条逻辑线来解释。

---

### 一、残差分析：模型有没有系统性犯错？

残差是：
$$
r = y - X\hat{\beta}
$$

你可以把残差看成模型**解释不了的那部分世界**。

在一个“模型假设正确”的理想情况下：

* 残差应该围绕 0 上下波动
* 没有明显结构
* 看起来像随机噪声

这就是 PPT 里说的：

> “围绕 0 随机分布的白噪声”

---

如果你在残差图里看到下面这些现象，就要警惕了：

**1️⃣ 喇叭口形状**

* 小预测值时残差小
* 大预测值时残差越来越大

这通常说明：

* 误差方差不恒定（异方差）
* 普通最小二乘的假设被破坏

解决方式可能是：

* 加权最小二乘
* 变量变换（如取对数）

**2️⃣ 弯曲或波浪形结构**

* 残差随预测值呈现曲线

这说明：

* 线性模型不够
* 缺少非线性项或交互项

---

### 二、杠杆点：谁在“决定”这条直线？

有些点并不是残差很大，但它们**位置很特殊**。

这就是“杠杆点”。

直觉上：

* 离数据中心很远的点
* 能“撬动”整条回归线

数学上，它们由帽子矩阵给出：

$$
H = X(X^T X)^{-1}X^T
$$

这个矩阵有一个非常形象的性质：

> **$\hat{y} = Hy$**
> 每个预测值都是对所有 $y$ 的加权平均。

其中：

* $H_{ii}$ 大
  → 第 $i$ 个点对自己的预测影响很大
  → 高杠杆点

---

### 三、影响点：既“远”，又“不合群”

真正危险的是：

> **既是高杠杆，又有大残差的点。**

这种点：

* 自己位置极端
* 还不符合整体趋势
* 却能强烈拉偏模型

Cook’s Distance 正是用来量化这一点的：

* 衡量“删掉这个点，模型会变多少”

这些点不是一定要删除，但一定要：

* 单独检查
* 回到数据来源

---

### 四、评估指标：用数字总结整体表现

在做完“图形诊断”之后，我们再用指标做总结。

**$R^2$：解释力**

* 表示模型解释了多少比例的总方差
* 越大越好，但**不能单独使用**

它不告诉你：

* 是否过拟合
* 是否违反假设

**MSE：预测误差大小**

* 直接衡量预测偏差
* 对大误差非常敏感

在工程里：

* MSE 更偏向“预测质量”
* $R^2$ 更偏向“解释能力”

---

### 五、这页 PPT 的核心思想

这一页的关键词不是“算”，而是**“查”**。

* 残差 → 查模型假设
* 杠杆 → 查数据点是否异常
* 指标 → 查整体表现

所以最后那句话：

> “将理论与实践闭环到质量控制”

意思是：

> **模型不是一次性产物，而是需要不断诊断、修正的工程系统。**

如果你愿意，下一页可以自然接到
**交叉验证、训练/测试划分、以及模型选择**，
这正好是“评估”的下一步。

----

这一部分可以看成是：
**当你有不止一个模型时，如何公平、可靠地选出“未来表现最好”的那个。**
它把前面的理论、诊断，真正落地到“决策”上。

我按顺序讲：训练/测试划分 → 交叉验证 → 模型选择，它们是一条完整的逻辑链。

---

### 一、训练 / 测试划分：先把“未来”留出来

最基本的问题是：

> **模型在没见过的数据上表现如何？**

为此，我们把数据分成两部分：

* **训练集 (Training Set)**：用来拟合模型
* **测试集 (Test Set)**：完全不参与训练，只用于最终评估

关键思想是：

> **测试集必须像“未来数据”，绝不能提前偷看。**

如果你用测试集来调参数、选模型：

* 测试集就被“污染”了
* 评估结果会过于乐观

因此：

* 测试集只用一次
* 用在所有决定都做完之后

---

### 二、为什么只用一次划分还不够？

问题在于：
**一次随机划分，可能运气好，也可能运气差。**

* 有的划分刚好很容易
* 有的划分刚好很难

这会让评估结果不稳定。

于是我们引入交叉验证。

---

### 三、交叉验证（Cross-Validation）：让数据轮流当“未来”

最常见的是 **K 折交叉验证**：

1. 把数据分成 K 份
2. 每次用 K−1 份训练
3. 剩下 1 份验证
4. 重复 K 次，取平均性能

这样做的好处是：

* 每个样本都当过一次“没见过的数据”
* 评估结果更稳定、更公平

交叉验证的角色是：

> **用来比较模型和调超参数，而不是给最终成绩。**

---

### 四、模型选择：我们到底在“选”什么？

模型选择并不只是：

* 选线性还是非线性
* 选哪个算法

更多时候是在选：

* 正则化强度 $\lambda$
* 特征子集
* 模型复杂度

典型流程是：

1. 设定一组候选模型（或 $\lambda$ 值）
2. 对每个候选，用交叉验证计算验证误差
3. 选择平均验证误差最小的那个

这里的原则是：

> **用最简单、但验证误差已经足够低的模型。**

这正是偏差–方差权衡在实践中的体现。

---

### 五、训练集 / 验证集 / 测试集的分工

在更规范的流程中，数据其实分成三份：

* **训练集**：拟合参数
* **验证集（或交叉验证）**：选模型、调超参数
* **测试集**：最终一次性评估

它们各司其职：

* 训练集负责“学习”
* 验证集负责“选择”
* 测试集负责“证明”

---

### 六、常见误区（非常重要）

几个典型错误需要特别提醒：

* 用测试集来选 $\lambda$
* 反复查看测试集结果
* 在全数据上做特征选择再交叉验证

这些都会导致：

> **信息泄漏（Data Leakage）**

结果是：
看起来模型很好，实际一上线就崩。

---

### 七、把这一页和前面内容连起来

你前面讲过：

* 病态
* 正则化
* 残差与稳定性

现在可以把它们统一起来：

* 正则化 → 控制模型复杂度
* 交叉验证 → 决定“控制到什么程度”
* 测试集 → 验证最终是否可信

---

### 最后一句话总结这一部分

训练/测试划分是底线，
交叉验证是工具，
模型选择是目标。

真正专业的建模不是“哪个模型拟合最好”，
而是：

> **在所有没见过的数据里，哪个模型最不容易出错。**
