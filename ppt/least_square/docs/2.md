这一页的核心问题只有一个：

> **怎样把“让误差最小”这件事，变成一个可以直接算出来的方程？**

---

先从“为什么会出现正规方程”说起。

上一页我们说过，我们想做的是最小化这个东西：
$$
L(\beta) = |y - X\beta|^2
$$

这在数学上就是一个关于 $\beta$ 的“碗形函数”。
最低点在哪里？
答案是：**在导数等于 0 的地方**。

在一元函数里你学过：

* 求导
* 令导数等于 0
* 解方程

这里只是把“一个变量”升级成了“一个向量”，思路完全一样。

---

接下来发生的关键一步是：

> 对 $L(\beta)$ 关于 $\beta$ 求导，并令导数为 0

做完这件事（中间的矩阵微积分细节可以先不管），**神奇地**就得到了这一行：

$$
X^T X \beta = X^T y
$$

这就是所谓的 **正规方程（Normal Equation）**。

你可以把它理解为：

> “最优的 $\beta$ 必须满足的条件”

---

我们来拆开看看这个方程在说什么。

* $X^T$：把原来 $n \times p$ 的矩阵转置成 $p \times n$
* $X^T X$：变成一个 $p \times p$ 的“特征相关矩阵”
* $\beta$：还是那个 $p \times 1$ 的系数向量
* $X^T y$：把所有样本的信息压缩成一个 $p \times 1$ 的向量

所以这整件事其实是：

> **用数据构造一个线性方程组，然后解这个方程组。**

---

现在来到 PPT 里的第一条重点：**什么时候这个方程能解？**

这里出现了一个关键词：**列满秩**。

$$
\mathrm{rank}(X) = p
$$

直观理解是：

* $X$ 的每一列（每个特征）都**不是多余的**
* 没有哪一列可以由其他列“拼出来”
* 每个特征都提供了独立的信息

如果你有两个完全一样的特征，比如：

* “学习时间（小时）”
* “学习时间（分钟）”

那它们本质上就是重复的，矩阵就**不满秩**。

---

为什么满秩这么重要？

因为只有在这个条件下：

$$
X^T X
$$

才是：

* 正定的
* **可逆的**

而“可逆”在数学里意味着一句话：

> **这个方程只有一个解，而且解是稳定、明确的。**

---

于是就来到了 PPT 的第二个重点：**闭式解**。

既然
$$
X^T X \beta = X^T y
$$

而且 $X^T X$ 可逆，那我们就可以像解普通方程一样：

两边同时乘以 $(X^T X)^{-1}$

得到：
$$
\hat{\beta} = (X^T X)^{-1} X^T y
$$

这就是线性回归中最经典、最漂亮的公式。

---

为什么大家都说这是“闭式解”？

因为：

* 不需要猜
* 不需要迭代
* 不需要一步步逼近

只要：

1. 算矩阵乘法
2. 算一个逆矩阵

**一步到位，答案直接写出来。**

---

最后，用一句话总结这一页 PPT 的逻辑链：

我们想最小化平方误差。
最小值出现在导数为 0 的地方。
把“导数等于 0”写成矩阵形式，就得到了正规方程。
如果特征矩阵没有冗余（列满秩），这个方程就有唯一解。
于是我们可以直接写出 $\beta$ 的解析解。

> 简单介绍矩阵求导，包括矩阵求导的意义和基本规则

下面我用**尽量直观、不追求技术细节**的方式，给你一个“够用版”的矩阵求导介绍，目标是：
**看得懂线性回归里为什么要对矩阵求导，而不是马上去算复杂公式。**

---

先说一个最核心的问题：**为什么需要矩阵求导？**

在高中里，你求导的对象通常是
$$
f(x)
$$
一个数 → 一个数。

但在机器学习和线性代数里，情况变成了：

* 自变量是一个**向量或矩阵**（比如 $\beta)$
* 函数的结果是一个**标量**（比如损失函数）

例如：
$$
L(\beta) = |y - X\beta|^2
$$

这里：

* 输入：一整组参数 $\beta$
* 输出：一个误差大小（一个数）

矩阵求导的意义就在于一句话：

> **告诉我们：当每个参数同时发生微小变化时，函数整体是怎么变的。**

---

接下来是一个重要观念：**“对向量求导”是什么意思？**

假设：
$$
\beta =
\begin{pmatrix}
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_p
\end{pmatrix}
$$

那么：
$$
\frac{\partial L}{\partial \beta}
$$

本质上就是把你熟悉的偏导数“打包”起来：

$$
\frac{\partial L}{\partial \beta}
=

\begin{pmatrix}
\frac{\partial L}{\partial \beta_1} \\
\frac{\partial L}{\partial \beta_2} \\
\vdots \\
\frac{\partial L}{\partial \beta_p}
\end{pmatrix}
$$

也就是一个**梯度向量**。

所以你可以记住一句话：

> **矩阵求导 = 同时对每一个分量求偏导。**

---

然后来看几个**最常用、最重要的基本规则**（只讲你在线性回归里一定会见到的）。

---

第一条规则：**线性项的求导**

如果
$$
f(\beta) = a^T \beta
$$
其中 $a$ 是一个常向量，那么

$$
\frac{\partial f}{\partial \beta} = a
$$

直觉解释：
这和一元函数里
$$
f(x) = ax \Rightarrow f'(x) = a
$$
是完全一样的，只是把“数”升级成了“向量”。

---

第二条规则：**二次型的求导**

这是最关键的一条。

如果
$$
f(\beta) = \beta^T A \beta
$$

* 当 $A$ 是对称矩阵时：
  $$
  \frac{\partial f}{\partial \beta} = 2A\beta
  $$

* 更一般地：
  $$
  \frac{\partial f}{\partial \beta} = (A + A^T)\beta
  $$

在线性回归中，那个 $X^T X$ 正好是对称的，所以公式会特别干净。

---

第三条规则：**平方范数的求导**

这是线性回归里最直接用到的形式：

$$
|X\beta - y|^2
= (X\beta - y)^T (X\beta - y)
$$

对 $\beta$ 求导的结果是：

$$
\frac{\partial}{\partial \beta} |X\beta - y|^2
= 2X^T(X\beta - y)
$$

你可以把它当成一个“标准模板”，不需要每次重新推。

---

第四条规则：**常数项对求导没影响**

* 与 $\beta$ 无关的项
* 在求导时全部消失

这和一元函数里“常数的导数是 0”完全一致。

---

最后，把“矩阵求导”这件事用一句话收尾：

矩阵求导并不是一种全新的数学，而是
**把“对很多变量同时求导”这件事，用线性代数的语言一次性写完。**

在线性回归中，它的作用非常明确：

> **帮我们把“误差最小”这个想法，精确地变成一个可以解的矩阵方程。**

如果你愿意，下一步我可以**完整走一遍**
$$
|y - X\beta|^2 \Rightarrow X^T X \beta = X^T y
$$
中间每一步是怎么来的，一步都不跳。

---

太好了，那我们就**一步不跳**，把这条线完整走完，从
$$
|y - X\beta|^2
$$
一步一步走到
$$
X^T X \beta = X^T y
$$

你可以把下面当成“标准推导模板”，以后看到最小二乘基本都是这一套。

---

**第一步：把目标函数写开**

我们从平方范数开始：
$$
L(\beta) = |y - X\beta|^2
$$

平方范数在矩阵里的定义是：
$$
|v|^2 = v^T v
$$

所以直接改写为：
$$
L(\beta) = (y - X\beta)^T (y - X\beta)
$$

这一步只是“换个写法”，没有任何高深内容。

---

**第二步：把括号乘开（像高中学的完全平方）**

利用转置和分配律：

$$
(y - X\beta)^T = y^T - \beta^T X^T
$$

于是：
$$
L(\beta)
= y^T y

* y^T X\beta
* \beta^T X^T y

- \beta^T X^T X \beta
$$

这里有一个小细节，但直觉上很好接受：

* $y^T X\beta$ 是一个数
* $\beta^T X^T y$ 也是同一个数

所以这两项可以合并：

$$
L(\beta)
= y^T y

* 2,\beta^T X^T y

- \beta^T X^T X \beta
$$

---

**第三步：对 $\beta$ 求导**

现在开始真正“求最小值”。

我们对每一项分别求导。

先看第一项：
$$
y^T y
$$

它和 $\beta$ **完全没关系**，所以：
$$
\frac{\partial}{\partial \beta}(y^T y) = 0
$$

---

第二项：
$$
-2,\beta^T X^T y
$$

这是“线性项”，可以直接用规则：
$$
\frac{\partial}{\partial \beta}(\beta^T a) = a
$$

所以：
$$
\frac{\partial}{\partial \beta}(-2,\beta^T X^T y)
= -2 X^T y
$$

---

第三项：
$$
\beta^T X^T X \beta
$$

这是一个标准的二次型。

因为 $X^T X$ 是对称矩阵，所以直接用结论：
$$
\frac{\partial}{\partial \beta}(\beta^T A \beta) = 2A\beta
$$

得到：
$$
\frac{\partial}{\partial \beta}(\beta^T X^T X \beta)
= 2 X^T X \beta
$$

---

**第四步：把导数加起来**

把三部分合在一起：

$$
\frac{\partial L}{\partial \beta}
= -2 X^T y + 2 X^T X \beta
$$

整理一下顺序：
$$
\frac{\partial L}{\partial \beta}
= 2\left( X^T X \beta - X^T y \right)
$$

---

**第五步：令导数等于 0**

这是“找最小值”的关键一步：

$$
2\left( X^T X \beta - X^T y \right) = 0
$$

2 没有任何影响，直接去掉：

$$
X^T X \beta - X^T y = 0
$$

移项，得到：

$$
\boxed{X^T X \beta = X^T y}
$$

这就是 **正规方程**。

---

**最后，用一句话总结整个推导的逻辑**

1. 把平方误差写成矩阵形式
2. 展开成“常数项 + 线性项 + 二次项”
3. 对参数向量 $\beta$ 求导
4. 令导数为 0
5. 得到一个线性方程组

这一步之后，剩下的事情就只剩下**线性代数**了：
如果 $X^T X$ 可逆，就直接解出 $\beta$。

