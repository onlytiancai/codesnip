> 分别从求导，矩阵，几何和最大似然估计的角度来讲解和推导最小二乘法，并把它们统一起来。

下面我分别从**求导、矩阵、几何、最大似然估计（MLE）**四个角度系统地讲解最小二乘法的推导，最后把它们**统一到同一个数学本质**上。为了清晰，我用同一套符号贯穿始终。

我们考虑经典线性回归模型
$ y = X\beta + \varepsilon $
其中
$y \in \mathbb{R}^n$ 是观测值，
$X \in \mathbb{R}^{n\times p}$ 是设计矩阵，
$\beta \in \mathbb{R}^p$ 是待估参数，
$\varepsilon$ 是误差。

————————————————
一、从“求导”的角度
————————————————

最小二乘法的出发点是：我们手里只有观测值 $y$ 和设计矩阵 $X$，但不知道参数 $\beta$ 到底是多少。  
为了“猜”一个靠谱的 $\beta$，我们定下一个**直观标准**——让“预测值” $X\beta$ 与真实值 $y$ 的**总误差平方和**最小。  
用数学语言写出来，就是下面这个**优化问题**：

$$
\min_{\beta}  \|y - X\beta\|_2^2
$$

这里 $\| \cdot \|_2^2$ 表示向量的 2-范数平方，也就是把每个分量的差先平方再全部加起来。

把它展开成纯标量形式，方便后续求导：

$$
f(\beta) = (y - X\beta)^\top (y - X\beta)
$$

你可以把 $f(\beta)$ 想象成一个关于 $\beta$ 的“损失面”，我们要找的就是这个面的**最低点**。

>[推导过程](1.md)


展开：
$$
f(\beta) = y^\top y - 2\beta^\top X^\top y + \beta^\top X^\top X \beta
$$

> [详细展开步骤](2.md)

对 $\beta$ 求梯度：
$$
\nabla_\beta f = -2X^\top y + 2X^\top X \beta
$$

> [详细过程](3.md)

令梯度为零（极值条件）：
$$
X^\top X \beta = X^\top y
$$

这就是**正规方程（Normal Equation）**。

如果 $X^\top X$ 可逆，则解为
$$
\hat{\beta} = (X^\top X)^{-1}X^\top y
$$

> [具体解释](4.md)


这一路径强调的是：
**最小二乘解是一个二次函数的驻点**。

————————————————
二、从“矩阵”的角度
————————————————

矩阵视角把问题理解为**线性代数方程的最优近似解**。

我们希望解
$$
X\beta = y
$$
但当方程组**超定** $(n>p)$ 时，一般无精确解。

最小二乘法的思想是：
找一个 $\beta$，使得残差
$$
r = y - X\beta
$$
在二范数意义下最小。

关键条件是：
**残差必须与 $X$ 的列空间正交**
$$
X^\top (y - X\hat{\beta}) = 0
$$

这正是正规方程：
$$
X^\top X\hat{\beta} = X^\top y
$$

矩阵角度强调的是：
**最小二乘解是一个满足正交条件的线性代数解**。

————————————————
三、从“几何”的角度
————————————————

这是最小二乘法最直观、也最本质的解释。

向量 $X\beta$ 的所有可能取值，构成了 $X$ 的**列空间**
$$
\mathcal{C}(X) = {X\beta : \beta \in \mathbb{R}^p}
$$

最小二乘问题等价于：
在子空间 $\mathcal{C}(X)$ 中，寻找一个向量，使它**距离 $y$ 最近**。

几何结论是：
$$
\hat{y} = X\hat{\beta}
$$
是 $y$ 在 $\mathcal{C}(X)$ 上的**正交投影**。

因此：
$$
y - \hat{y} \perp \mathcal{C}(X)
$$

投影矩阵为
$$
P = X(X^\top X)^{-1}X^\top
$$

> [具体解释](5.md)

于是
$$
\hat{y} = Py
$$

几何视角强调的是：
**最小二乘 = 正交投影**。

————————————————
四、从“最大似然估计”的角度
————————————————

现在引入统计假设。

假设误差
$$
\varepsilon \sim \mathcal{N}(0, \sigma^2 I)
$$

则
$$
y \mid \beta \sim \mathcal{N}(X\beta, \sigma^2 I)
$$

似然函数为
$$
L(\beta) \propto \exp\left(-\frac{1}{2\sigma^2}|y - X\beta|_2^2\right)
$$

最大化似然等价于最小化负对数似然：
$$
\min_{\beta} |y - X\beta|_2^2
$$

这与最小二乘问题**完全一致**，解仍然满足正规方程。

MLE 角度强调的是：
**最小二乘是高斯噪声假设下的最优估计（极大似然）**。

————————————————
五、统一视角：它们本质上在做什么？
————————————————

这四种推导其实都在描述同一件事，只是语言不同：

1. 求导角度：
   在一个二次损失函数上找驻点

2. 矩阵角度：
   解一个满足正交条件的线性方程

3. 几何角度：
   把 $y$ 正交投影到 $\mathcal{C}(X)$ 的列空间

4. 概率角度（MLE）：
   在高斯噪声模型下找最可能的参数

它们统一在一个核心等价关系上：
$$
\boxed{
\hat{\beta}
= \arg\min_\beta |y - X\beta|^2
\iff
X^\top (y - X\hat{\beta}) = 0
\iff
X\hat{\beta} = \mathrm{Proj}_{\mathcal{C}(X)}(y)
}
$$

一句话总结：
**最小二乘法的本质，是在欧几里得空间中做正交投影；
求导、矩阵和概率，只是对“正交”这一事实的不同表达。**

如果你愿意，我也可以继续把它推广到
加权最小二乘、广义最小二乘、岭回归，或从希尔伯特空间的角度统一理解。
