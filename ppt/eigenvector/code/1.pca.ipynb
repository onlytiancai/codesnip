{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "697e3d1e-1d10-405f-91f2-14946d0a786c",
   "metadata": {},
   "source": [
    "# 特征分解在主成分分解 PCA 中的应用\n",
    "\n",
    "2025-12\n",
    "wawa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb964447-67ab-470b-af1e-9704282064c0",
   "metadata": {},
   "source": [
    "### 第一步：构造一个“有冗余信息”的二维数据\n",
    "\n",
    "虽然是二维数据，但两个维度高度相关，直觉上“有效信息可能只有一维”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99792584-ff4e-4ed5-9bf9-f94ec6cd5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n = 200\n",
    "x1 = np.random.normal(0, 1, n) + 5      # 故意加偏移\n",
    "x2 = 2 * x1 + np.random.normal(0, 0.5, n) + 10\n",
    "\n",
    "X = np.column_stack((x1, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d68276-1c01-4f34-b4eb-13114f38f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.6)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Original Data Distribution\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a0b5fe-3a3d-41a4-8998-773a04471ab3",
   "metadata": {},
   "source": [
    "- 点云明显沿着某个方向拉伸\n",
    "- 两个特征不是“独立”的\n",
    "- PCA 就是要找这条“最重要的方向”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45bf84e-c75c-4324-902c-e2fc09f8db51",
   "metadata": {},
   "source": [
    "### 第二步：数据中心化（减去均值）\n",
    "\n",
    "PCA 只关心“变化”，不关心数据整体在什么位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed8d7c1-135f-4dd6-ae8a-1e708aff063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = X.mean(axis=0)\n",
    "X_centered = X - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec587698-cc6b-4eea-9802-241d2485653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.6)\n",
    "plt.title(\"Before Centering\")\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.6)\n",
    "plt.axhline(0)\n",
    "plt.axvline(0)\n",
    "plt.title(\"After Centering\")\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620ce1e3-cd17-48ae-9448-9e6c8dedeaec",
   "metadata": {},
   "source": [
    "- 中心化后，数据以原点为中心\n",
    "- 后面算协方差、特征值才有明确几何意义\n",
    "\n",
    "我们可以看到，中心化之前数据整体偏离原点\n",
    "- 协方差描述的其实是“围绕均值的波动”\n",
    "- 如果不中心化，主成分方向会被“位置”干扰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a6f21-ac42-493d-9f4a-fd8ef23b584d",
   "metadata": {},
   "source": [
    "### 第三步：协方差矩阵（数据结构的数学描述）\n",
    "\n",
    "协方差矩阵描述的是“沿不同方向的变化程度”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253b0fb-7cd2-4103-ac78-9fe4bb734d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = np.cov(X_centered, rowvar=False)\n",
    "cov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb255ad-b9f9-4072-88ee-6f712bc66dfc",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{pmatrix}\n",
    "1.0485 & 2.1337 \\\\\n",
    "2.1337 & 4.5642\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "先看对角线：每个特征自己的波动大小\n",
    "- 第一个特征的方差是 1.05：说明它本身的波动程度不大，数据相对集中\n",
    "- 第二个特征的方差是 4.56：明显比第一个大，说明它的取值范围更宽、变化更剧烈\n",
    "\n",
    "两个维度本身“尺度就不一样”，第二维更“活跃”。\n",
    "\n",
    "再看非对角线：两个特征之间的关系\n",
    "\n",
    "- 协方差是 2.13，而且是正的\n",
    "    - 两个特征相对于各自均值的“偏离方向”通常是同号的。    \n",
    "- 数值还不小，这说明：\n",
    "    - 当第一个特征变大时，第二个特征通常也会变大\n",
    "    - 这里的“更大 / 更小”，都是相对于各自的均值，不是绝对值。\n",
    "- 举个例子\n",
    "    - 一个人：身高 160 cm（不算大）体重 80 kg（很大）\n",
    "    - 另一个人：身高 190 cm（很大）体重 90 kg（也大）\n",
    "    - 协方差关心的是：\n",
    "        - 160 是否低于“平均身高”\n",
    "        - 80 是否高于“平均体重”\n",
    "        - 而不是 160 或 80 本身“大不大”。\n",
    "\n",
    "两个特征之间存在强烈的正相关\n",
    "\n",
    "相关系数：\n",
    "\n",
    "$$\n",
    "\\rho = \\frac{2.1337}{\\sqrt{1.0485 \\times 4.5642}} \\approx 0.98\n",
    "$$\n",
    "\n",
    "这几乎是“接近线性关系”的级别。\n",
    "\n",
    "> 这两个特征在很大程度上其实在重复表达同一件事。\n",
    "\n",
    "从几何角度看这个协方差矩阵（这是 PCA 的直觉核心）\n",
    "\n",
    "这个矩阵告诉我们三件事：\n",
    "\n",
    "1. 数据云不是“圆形”，而是一个被拉伸的椭圆\n",
    "2. 拉伸的主方向不是 x 轴或 y 轴\n",
    "3. 真正的“变化方向”是某个斜着的方向\n",
    "\n",
    "> 在二维里，一个“圆形分布”的协方差矩阵必须满足两个条件：\n",
    "> - 两个方向的方差相等\n",
    "> - 不存在相关性（协方差为 0）\n",
    ">\n",
    "> 在二维高斯数据中，有一个非常重要的事实：协方差矩阵的等概率线（或等方差线）一定是椭圆。\n",
    "> - 特征值 → 椭圆两条轴的长度\n",
    "> - 特征向量 → 椭圆的朝向\n",
    ">\n",
    "> 如果椭圆的主轴刚好对齐 x、y 轴，会发生什么？\n",
    "> - x 和 y 方向是“相互独立的”\n",
    "> - 协方差必须是 0\n",
    "> - 对应的矩阵一定是对角矩阵：\n",
    "> \n",
    "> 但我们的协方差 = 2.13 ≠ 0，这意味着：x 方向和 y 方向不是“自然的主方向”，是某个混合了 x 和 y 的斜方向。\n",
    "> 如果我只沿着 x 或 y 看数据，都看不全；必须斜着看，才能看到最大的拉伸方向。\n",
    ">\n",
    "> 协方差矩阵的“快速判断口诀”\n",
    "> - 协方差矩阵是对角的 → 主轴对齐坐标轴\n",
    "> - 协方差矩阵有非零非对角项 → 主轴一定是斜的\n",
    "> - 特征值相等 → 圆\n",
    "> - 特征值不等 → 椭圆\n",
    "\n",
    "这正是 PCA 要找的主成分方向。\n",
    "\n",
    "因为存在很大的协方差，沿着原始坐标轴看方差并不高效，我们应该旋转坐标系，让一个轴对齐这个“共同变化方向”。\n",
    "\n",
    "这个协方差矩阵表明：\n",
    "\n",
    "数据在第二维度上波动更大，两个特征高度正相关，因此存在一个占据绝大部分方差的一维主方向，非常适合用 PCA 进行降维。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3f31b4-7b02-449a-9f73-53a5c2b9e1b8",
   "metadata": {},
   "source": [
    "#### 3.1 协方差矩阵的特征值和每个维度的方差是什么关系？\n",
    "\n",
    "一句话结论\n",
    "\n",
    "> **协方差矩阵的特征值不是“某个原始维度的方差”，\n",
    "> 而是“在某个方向（主成分方向）上的方差”。**\n",
    "\n",
    "原始维度的方差在对角线上，特征值对应的是“旋转之后的新维度”的方差。\n",
    "\n",
    "第一层：从定义上区分这两件事\n",
    "\n",
    "给定一个二维随机向量 $(X_1, X_2)$，协方差矩阵是：\n",
    "\n",
    "$$\n",
    "\\Sigma =\n",
    "\\begin{pmatrix}\n",
    "\\mathrm{Var}(X_1) & \\mathrm{Cov}(X_1, X_2) \\\\\n",
    "\\mathrm{Cov}(X_1, X_2) & \\mathrm{Var}(X_2)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "所以：\n",
    "\n",
    "* 对角线元素\n",
    "  → **原始特征轴上的方差**\n",
    "* 非对角线元素\n",
    "  → 特征之间的相关性\n",
    "\n",
    "第二层：特征值在干什么（数学含义）\n",
    "\n",
    "设 $v$ 是协方差矩阵的一个单位特征向量，对应特征值 $\\lambda$：\n",
    "\n",
    "$$\n",
    "\\Sigma v = \\lambda v\n",
    "$$\n",
    "\n",
    "一个非常重要、但很多教材一笔带过的事实是：\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(X \\cdot v) = v^\\top \\Sigma v = \\lambda\n",
    "$$\n",
    "\n",
    "这句话的含义是：\n",
    "\n",
    "> **把数据投影到方向 $v$ 上，\n",
    "> 投影后的方差正好等于对应的特征值。**\n",
    "\n",
    "所以：\n",
    "\n",
    "* 特征向量 → 方向\n",
    "* 特征值 → 沿这个方向的数据方差\n",
    "\n",
    "第三层：几何直觉（为什么要旋转）\n",
    "\n",
    "想象你有一团“斜着拉长”的数据云：\n",
    "\n",
    "* 沿 x 轴看 → 方差一般\n",
    "* 沿 y 轴看 → 方差也一般\n",
    "* 沿某条斜方向看 → 方差最大\n",
    "\n",
    "协方差矩阵的特征分解做的事情是：\n",
    "\n",
    "> 找到一组“最合适的坐标轴”，\n",
    "> 让每个新轴上的方差彼此独立、且尽量集中。\n",
    "\n",
    "在这组新坐标轴下：\n",
    "\n",
    "* 坐标轴方向 = 特征向量\n",
    "* 每个轴上的方差 = 特征值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b3cd70-f8f3-43f4-a616-77e409416d6e",
   "metadata": {},
   "source": [
    "#### 3.2 如果两个特征各自的方差都很大，但协方差很小，适合做PCA吗？\n",
    "\n",
    "一句话结论\n",
    "\n",
    "> **如果两个特征各自方差都很大，但协方差很小（接近 0），\n",
    "> 那么一般“不适合”用 PCA 来做降维。**\n",
    "\n",
    "更准确一点说：\n",
    "\n",
    "> **PCA 不会带来明显的降维收益。**\n",
    "\n",
    "为什么？从 PCA 想解决的问题说起\n",
    "\n",
    "PCA 的目标不是“压缩数值大小”，而是：\n",
    "\n",
    "> 用更少的维度，\n",
    "> 表达原始数据中的“主要变化结构”。\n",
    "\n",
    "这个“主要变化结构”出现的前提是：\n",
    "\n",
    "* 不同特征之间存在**冗余**\n",
    "* 即：它们在某种方向上“一起变化”\n",
    "\n",
    "而“协方差很小”恰恰说明：\n",
    "\n",
    "> 两个特征的变化基本是彼此独立的。\n",
    "\n",
    "用协方差矩阵来直观看\n",
    "\n",
    "如果两个特征：\n",
    "\n",
    "* Var(X₁) 很大\n",
    "* Var(X₂) 很大\n",
    "* Cov(X₁, X₂) ≈ 0\n",
    "\n",
    "协方差矩阵大致是：\n",
    "\n",
    "$$\n",
    "\\Sigma \\approx\n",
    "\\begin{pmatrix}\n",
    "\\sigma_1^2 & 0 \\\\\n",
    "0 & \\sigma_2^2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "这时会发生什么？\n",
    "\n",
    "* 特征向量 ≈ 原始坐标轴\n",
    "* 特征值 ≈ 原始各维度的方差\n",
    "\n",
    "也就是说：\n",
    "\n",
    "> PCA 找不到一个“比原始坐标轴更好”的方向。\n",
    "\n",
    "\n",
    "从特征值角度看“是否值得降维”\n",
    "\n",
    "判断 PCA 是否“有用”，一个非常实用的标准是：\n",
    "\n",
    "> **最大的特征值是否远大于其他特征值。**\n",
    "\n",
    "你的设定是：\n",
    "\n",
    "* 两个特征方差都很大\n",
    "* 协方差很小\n",
    "\n",
    "通常意味着：\n",
    "\n",
    "* 两个特征值都不小\n",
    "* 也差不多大\n",
    "\n",
    "例如：\n",
    "\n",
    "$$\n",
    "\\lambda_1 \\approx 10,\\quad \\lambda_2 \\approx 8\n",
    "$$\n",
    "\n",
    "这时：\n",
    "\n",
    "* PC1 只能解释 ~55% 的方差\n",
    "* 丢掉 PC2 会损失大量信息\n",
    "\n",
    "所以**不适合降到 1 维**。\n",
    "\n",
    "总结\n",
    "\n",
    "- PCA 适合用在“高方差 + 高相关”的特征组合上；\n",
    "- 如果各维度彼此独立，即使方差很大，PCA 也不会带来有效的信息压缩。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f7881-7991-4308-a99c-b996a9907c88",
   "metadata": {},
   "source": [
    "#### 3.3 为什么正态分布数据的协方差矩阵的等方差线是个椭圆？\n",
    "\n",
    "从**概率密度 → 二次型 → 几何**一步一步来解释。\n",
    "\n",
    "\n",
    "> **二维正态分布的等概率（等方差）线一定是椭圆，\n",
    "> 因为它们是一个正定二次型等于常数的集合。**\n",
    "\n",
    "\n",
    "第一步：从二维正态分布的概率密度函数看\n",
    "\n",
    "二维正态分布的概率密度是：\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{1}{2\\pi |\\Sigma|^{1/2}}\n",
    "\\exp\\left(\n",
    "-\\frac{1}{2}\n",
    "(x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "关键点只有一个：\n",
    "\n",
    "> 概率密度只取决于\n",
    "> $(x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)$\n",
    "\n",
    "---\n",
    "\n",
    "第二步：什么是“等概率 / 等方差线”\n",
    "\n",
    "所谓“等概率线”，就是：$p(x) = \\text{常数}$\n",
    "\n",
    "这等价于：$(x-\\mu)^\\top \\Sigma^{-1} (x-\\mu) = c$\n",
    "\n",
    "注意：\n",
    "这是一个**二次型等于常数**。\n",
    "\n",
    "---\n",
    "\n",
    "第三步：二次型在二维空间里是什么形状？\n",
    "\n",
    "在二维中，任何正定对称矩阵 $A$，满足$x^\\top A x = c$\n",
    "\n",
    "所描述的几何形状一定是： **椭圆**\n",
    "\n",
    "原因是：\n",
    "\n",
    "* $A$ 可以正交对角化\n",
    "* 对角化后就是  $  \\lambda_1 y_1^2 + \\lambda_2 y_2^2 = c  $\n",
    "* 再做变量替换，就变成标准椭圆方程\n",
    "\n",
    "---\n",
    "\n",
    "第四步：协方差矩阵为什么一定给出“正定二次型”\n",
    "\n",
    "协方差矩阵 $\\Sigma$ 有三个重要性质：\n",
    "\n",
    "1. 对称\n",
    "2. 半正定（实际数据中通常是正定）\n",
    "3. 可正交特征分解\n",
    "\n",
    "所以：\n",
    "\n",
    "* $\\Sigma^{-1}$ 也是对称正定的\n",
    "* 对应的二次型一定是椭圆，而不可能是双曲线或抛物线\n",
    "\n",
    "---\n",
    "\n",
    "第五步：特征分解让“椭圆”完全可解释\n",
    "\n",
    "对协方差矩阵做特征分解：$\\Sigma = Q \\Lambda Q^\\top$\n",
    "\n",
    "代回等概率方程：$(x-\\mu)^\\top Q \\Lambda^{-1} Q^\\top (x-\\mu) = c$\n",
    "\n",
    "令：$y = Q^\\top (x-\\mu)$\n",
    "\n",
    "就得到：$\\frac{y_1^2}{\\lambda_1}* \\frac{y_2^2}{\\lambda_2} = c$\n",
    "\n",
    "这已经是**标准椭圆方程**了。\n",
    "\n",
    "于是可以清楚地对应：\n",
    "\n",
    "* 特征向量 → 椭圆的主轴方向\n",
    "* 特征值 → 主轴长度的平方（严格说是成反比关系）\n",
    "\n",
    "---\n",
    "\n",
    "第六步：为什么不是圆？\n",
    "\n",
    "只有在一个特殊情况下才是圆： $\\lambda_1 = \\lambda_2$\n",
    "\n",
    "也就是： 协方差矩阵 ∝ 单位矩阵， 各个方向的方差完全一样，这时椭圆退化成圆。\n",
    "\n",
    "总结\n",
    "\n",
    "- 正态分布的概率密度由一个二次型决定；\n",
    "- 二维空间中，正定二次型的等值集合必然是椭圆；\n",
    "- 协方差矩阵的特征分解，决定了椭圆的方向和拉伸程度。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47da7ec-3562-42cd-9791-10782422801d",
   "metadata": {},
   "source": [
    "### 第四步：特征值与特征向量（PCA 核心）\n",
    "\n",
    "特征向量给方向，特征值给这个方向上有多少信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab46094-cbab-4cd0-8dc0-448cb0a0fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "\n",
    "# 按特征值大小排序\n",
    "idx = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "eigenvalues, eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9800985d-b5b9-4d0e-84da-d5c66aa02aec",
   "metadata": {},
   "source": [
    "特征值 $5.57,0.042$ 对应的特征向量：第一列：$\\begin{pmatrix}-0.4267 \\\\-0.9044\\end{pmatrix}$ 第二列：$\\begin{pmatrix}-0.9044 \\\\0.4267\\end{pmatrix}$\n",
    "\n",
    "---\n",
    "\n",
    "先看特征值：数据“往哪拉得最开”\n",
    "\n",
    "特征值的含义是：**沿着对应特征向量方向的方差大小**。\n",
    "\n",
    "这里两个数差距非常大，这意味着：\n",
    "\n",
    "* 数据几乎所有的变化都集中在第一个方向上\n",
    "* 垂直方向上的变化非常小\n",
    "\n",
    "可以说：第一主成分解释了$ \\frac{5.57}{5.57 + 0.042} \\approx 99.3% $ 的方差。\n",
    "\n",
    "几何直觉就是：数据云是一条被稍微“抖动”的细长椭圆，非常接近一条直线。\n",
    "\n",
    "---\n",
    "\n",
    "再看第一个特征向量：第一主成分（PC1）$(-0.4267, -0.9044)$\n",
    "\n",
    "> 先说一个容易忽略但很重要的点：**正负号本身没有意义**。\n",
    "> \n",
    "> $(-0.4267, -0.9044) \\equiv (0.4267, 0.9044)$ 表示的是同一条方向，只是朝向相反。\n",
    "\n",
    "\n",
    "这个向量在“说什么”\n",
    "\n",
    "* 第二个分量的绝对值明显更大，方向大致是：x 向右一点，y 向上很多\n",
    "- 这是那条“两个特征一起涨、一起跌”的斜方向。\n",
    "\n",
    "\n",
    "第二个特征向量：被丢掉的方向（PC2）$(-0.9044, 0.4267)$\n",
    "\n",
    "它和第一个特征向量：\n",
    "- 完全正交，表示“变化最小”的方向\n",
    "- 几何上就是：数据云的“厚度方向”。\n",
    "\n",
    "这也是为什么：\n",
    "- PCA 降维时直接丢掉它\n",
    "- 损失的信息非常小\n",
    "\n",
    "协方差矩阵的特征分解表明：\n",
    "\n",
    "- 数据几乎全部沿着一个斜方向变化，该方向由第一个特征向量给出；\n",
    "- 垂直于它的方向方差极小，因此可以安全地进行一维降维。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941dfa7-215f-450e-9f93-8216c94d8dc3",
   "metadata": {},
   "source": [
    "### 第五步：把“主成分方向”画出来\n",
    "\n",
    "这是整个 PCA 演示最关键的一张图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5039d-f980-46ea-9da1-600344bf845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# 数据点\n",
    "plt.scatter(X_centered[:, 0], X_centered[:, 1],\n",
    "            alpha=0.3, s=20)\n",
    "\n",
    "# 原点\n",
    "plt.scatter(0, 0, color=\"black\", s=50)\n",
    "\n",
    "pc1 = eigenvectors[:, 0]\n",
    "pc2 = eigenvectors[:, 1]\n",
    "\n",
    "# 用 sqrt(特征值) 缩放长度\n",
    "len1 = np.sqrt(eigenvalues[0])\n",
    "len2 = np.sqrt(eigenvalues[1])\n",
    "\n",
    "scale = 2  # 视觉缩放\n",
    "\n",
    "plt.quiver(0, 0,\n",
    "           pc1[0]*len1, pc1[1]*len1,\n",
    "           angles='xy', scale_units='xy', scale=1/scale,\n",
    "           color='red', width=0.012)\n",
    "\n",
    "plt.quiver(0, 0,\n",
    "           pc2[0]*len2, pc2[1]*len2,\n",
    "           angles='xy', scale_units='xy', scale=1/scale,\n",
    "           color='blue', width=0.008)\n",
    "\n",
    "# 标注\n",
    "plt.text(pc1[0]*len1*scale*1.1,\n",
    "         pc1[1]*len1*scale*1.1,\n",
    "         \"PC1 (large variance)\",\n",
    "         color='red', fontsize=11, weight='bold')\n",
    "\n",
    "plt.text(pc2[0]*len2*scale*1.1,\n",
    "         pc2[1]*len2*scale*1.1,\n",
    "         \"PC2 (small variance)\",\n",
    "         color='blue', fontsize=11)\n",
    "\n",
    "plt.axhline(0, color='gray')\n",
    "plt.axvline(0, color='gray')\n",
    "\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Principal Components with Variance Magnitude\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f72102-d4d0-41c6-8d04-faeaa663e897",
   "metadata": {},
   "source": [
    "- 红色 PC1 非常长，说明沿这个方向数据被拉得很开；\n",
    "- 蓝色 PC2 非常短，说明垂直方向上几乎没有变化。\n",
    "- PCA 本质是在旋转坐标系\n",
    "\n",
    "一个很重要但容易忽略的小提醒\n",
    "- PC1 / PC2 的正负方向是不唯一的\n",
    "- 如果你哪次看到箭头反过来，不是算错，而是数学上等价\n",
    "\n",
    "在 PCA 里：\n",
    "- 特征向量 → 椭圆主轴的方向\n",
    "- √特征值 → 主轴的半径长度，不是特征值本身，而是平方根。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671bb458-b87a-4cb8-8916-a88fedb01e1f",
   "metadata": {},
   "source": [
    "### 第六步：方差解释率（为什么可以降维）\n",
    "\n",
    "如果第一主成分已经解释了大部分方差，就可以丢掉其他方向。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162e824-df06-41ae-80eb-78569d651d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_ratio = eigenvalues / eigenvalues.sum()\n",
    "explained_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa39ebc7-0f22-4627-96da-176713750389",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.bar([\"PC1\", \"PC2\"], explained_ratio)\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.title(\"Variance Explained by Each PC\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e864b-de4d-411d-a4fd-a03540834016",
   "metadata": {},
   "source": [
    "- PC1 占了绝大多数信息\n",
    "- PC2 信息量很小\n",
    "- 降维损失是“可控的”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0dba60-9082-452e-abc2-3fa73e38a126",
   "metadata": {},
   "source": [
    "### 第七步：投影到第一主成分（真正的降维）\n",
    "\n",
    "降维不是“扔点”，而是“投影”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7935873-af15-49b0-8bc0-ccd11a05efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1 = eigenvectors[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1438cd-75e7-4936-a99a-13521bf8df4c",
   "metadata": {},
   "source": [
    "- eigenvectors 是一个 2×2 的矩阵，每一列是一个特征向量 `[:, 0]` 取的是第一列\n",
    "- pc1 是第一主成分方向，一个单位向量，相当于原始二维平面里，我们选定的一条新的坐标轴。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d250249b-a2fb-4306-b1b6-f75695e72f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1d = X_centered @ pc1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ca308-ac40-41a2-ba2c-0d6a3bd18fe9",
   "metadata": {},
   "source": [
    "- `X_centered`：形状是 `(n_samples, 2)`， `pc1`：形状是 `(2,)`\n",
    "- `@` 是点积操作，每个数据点与 PC1 做点积，结果是每个二维样本 → 变成了一个标量。\n",
    "- 点积在几何上意味着 = 投影长度\n",
    "    - 这一步等价于：把每个二维点沿着 PC1 的方向投影到这条轴上，只保留在这条轴上的“坐标值”。\n",
    "- 这一步就叫“降维”，因为：原来一个点需要两个数（x, y）描述，现在只需要一个数（在 PC1 上的位置）\n",
    "- 降维过程，本质上就是：选择最大方差方向作为新坐标轴，然后把数据投影到这条轴上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd7e2c2-29cf-44e9-9710-d5964556f0d9",
   "metadata": {},
   "source": [
    "### 第八步：投影过程的几何可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8174c0c8-6eee-4728-adbd-d556ffcf532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.3)\n",
    "\n",
    "# 主成分方向\n",
    "line = np.outer(X_1d, pc1)\n",
    "plt.scatter(line[:, 0], line[:, 1], alpha=0.6)\n",
    "\n",
    "plt.title(\"Projection onto First Principal Component\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ca6102-e02d-444e-a95a-bff3fa751811",
   "metadata": {},
   "source": [
    "- 所有点都“压”到一条线上\n",
    "- 丢掉的是垂直方向的信息\n",
    "- 保留的是变化最大的方向"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8766f7f-70ac-4ab2-bd50-09b078ef4241",
   "metadata": {},
   "source": [
    "### 第九步：一维结果展示（降维后的数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3d8d81-4b16-4047-8050-ffc46a1bbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 2))\n",
    "plt.scatter(X_1d, np.zeros_like(X_1d), alpha=0.6)\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"PC1 Value\")\n",
    "plt.title(\"1D Data after PCA\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563d09f-c5bc-4573-a0c3-311871ca045b",
   "metadata": {},
   "source": [
    "PCA 通过特征值和特征向量，把“相关的高维数据”\n",
    "映射到“信息最集中的低维空间”。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
