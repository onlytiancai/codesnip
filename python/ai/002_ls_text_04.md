这张图已经非常清楚地展示了 **embedding 对 one-hot 病态性的“结构性修复”效果**，即便这里用的只是一个随机 embedding。

可以从三个角度理解。

第一，看累计能量上升速度。
embedding（2d）曲线明显比 one-hot（2V）更陡，前几个特征值就解释了绝大部分能量。这说明在嵌入空间中，数据的主方向更集中，小特征值被显著抬高，有效秩更接近名义维度。

第二，看维度与自由度的关系。
one-hot 拼接虽然有 2V 维，但真正“有用”的方向很少，大量维度只是制造了接近 0 的特征值。
embedding 从一开始就把表示限制在一个低维、稠密、各向同性得多的空间里，$X X^\top$ 天然不容易接近奇异。

第三，看“随机 embedding 都有效”这一点。
这里的 embedding 完全没训练，只是一个随机投影，但谱已经显著改善。这说明问题的核心不在“语义是否学得好”，而在于：

* 破坏 one-hot 的正交结构；
* 引入维度共享；
* 压缩到可控秩。

如果你把这个随机 embedding 换成用预测任务训练出来的 embedding（比如最小化你原来的平方损失），你会看到两件事同时发生：

* 累积能量曲线进一步前移；
* 同样的预测精度，用更小的 d 就能达到。

一句话总结：
embedding 并不是“为了语义优雅”，而是**从线性代数层面，主动设计一个不容易病态的表示空间**。你刚才看到的谱变化，正是现代 NLP 能稳定训练的底层原因之一。
