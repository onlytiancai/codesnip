这个对比图揭示了一个非常关键、但经常被忽略的事实：**表示方式本身就在决定病态性的强弱**。

先看拼接（concatenation，2V 维）。
曲线起步较慢，需要更多的特征值才能累计到同样的能量。这是因为你把“第一个词”和“第二个词”硬性放进了两个正交子空间里，哪怕它们在统计上高度相关，线性代数上也被当成不同维度处理。结果就是名义维度翻倍，但有效秩并没有等比例增加，产生了大量接近 0 的特征值。

再看 CBOW 求和（V 维）。
累计能量上升明显更快，前几个特征值就能解释绝大部分能量。这说明在“上下文无序 + 线性叠加”的表示下，数据的内在低维结构被更直接地暴露出来，谱更集中，有效秩更接近真实自由度。

从谱的角度总结区别：

拼接是在**保留位置信息的同时，引入结构性冗余**，病态性更强，但表达能力更细。
CBOW 是**主动压缩结构**，牺牲顺序信息，换来更好的条件数和数值稳定性。

这也正好解释了为什么在实践中：

* n-gram / 拼接模型更容易需要强正则化；
* CBOW / mean pooling 天然更“好训”，即便不用太大的 λ。

用你这个极小例子，已经把“表示方式 → 谱结构 → 数值稳定性”这条因果链展示得非常干净了。

---

> 有什么办法来优化这种one-hot编码的缺点

有，而且几乎**现代 NLP 的所有表示学习方法，都是在系统性地修 one-hot 的这些缺点**。可以从线性代数、统计和建模三个层次来看。

先从根本问题说起。
one-hot 的核心缺陷并不是“离散”，而是：
高维、极度稀疏、任意两个不同词正交、完全不表达相似性。
这直接导致你刚才看到的现象：低有效秩、严重病态、对语料分布高度敏感。

第一类改进：**低维连续嵌入（embedding）**
这是最根本、也是最有效的办法。

用一个矩阵 $E \in \mathbb{R}^{V \times d}$，把
one-hot → 稠密向量，$d \ll V$。

线性代数上，这等价于：

* 把原来的坐标轴投影到一个低维子空间；
* 强制 $X X^\top$ 的秩 ≤ d；
* 从“病态 + 噪声方向很多”变成“可控秩”。

word2vec、GloVe、fastText、本质都在做这件事，只是目标函数不同。

第二类改进：**共享统计结构，而不是独立坐标**
one-hot 把每个词当成完全独立的事件，这是灾难之源。

典型修法包括：

* CBOW / mean pooling（你已经看到了谱会明显变好）
* 子词（BPE、WordPiece、字符 n-gram）
* 特征哈希（hashing trick）

它们的共同点是：
让不同词在表示空间中**必然发生碰撞或共享维度**，从而抬高小特征值。

第三类改进：**显式正则化 / 平滑统计**
如果你必须坚持 one-hot（比如教学或可解释性需求），那就只能“硬修”。

常见手段：

* 岭回归：$X X^\top + \lambda I$
* 拉普拉斯平滑 / Dirichlet 先验
* 截断 SVD，只保留前 k 个谱方向

这些方法不改变表示，但强行控制谱结构，避免数值爆炸。

第四类改进：**非线性映射（神经网络）**
从你的线性回归模型往前一步，就是：

one-hot → embedding → 非线性 → prediction

非线性本质上是在学习一个**数据自适应的坐标系**，把高频结构和低频结构分离开。这就是为什么即便一个很小的 MLP，也能比纯线性 n-gram 稳定得多。

第五类改进（更本质的一点）：**放弃“词是原子”的假设**
这是语言建模里最深的一刀。

* 子词、字符、语义成分
* 上下文相关表示（ELMo、BERT）
* 分布式而非符号式表示

在这些模型里，one-hot 只存在于输入接口层，几乎不再参与核心计算。

如果用一句话总结：
one-hot 的问题不是“太简单”，而是**把几何结构完全交给了数据稀疏性来决定**。
所有有效的优化方法，都是在**主动设计或学习一个更好的表示空间**，而不是指望 (X X^\top) 自己变好。
