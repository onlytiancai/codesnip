我已经把 $X X^\top$ 的特征值谱画出来了，用的是对数纵轴（semilogy），这样病态性会非常直观。

从图里你可以清楚看到几个关键信号。

第一，特征值呈现明显的“断崖式”下降。前面只有一小撮特征值在 $10^0$ 量级，之后迅速跌到 $10^{-15}$ 甚至更小，这说明有效秩远小于 $2V4。

第二，存在大量接近机器精度的特征值$(10^{-15}\sim10^{-30})$。这些方向在数值上线性相关或几乎相关，直接用 $(X X^\top)^{-1}$ 会极度不稳定，这也是你必须用 `pinv` 的根本原因。

第三，这种病态性来源非常“结构性”：

* one-hot + 拼接导致特征高度稀疏；
* 语料很小，2-gram 组合覆盖极不均匀；
* 第一词和第二词的子空间本身就高度相关，进一步压低秩。

如果你愿意进一步玩，可以做三件事来加深直觉：
1）画累计能量曲线，看前 k 个特征值解释了多少能量；
2）对比不用拼接、而是求和（CBOW 风格）的 $X X^\top$；
3）给 $X X^\top + \lambda I$ 加不同 $\lambda$，观察谱被“抬平”的过程，这正是岭回归在干什么。

这个例子本身就是一个非常漂亮的“为什么语言模型里的线性回归天然病态”的最小玩具模型。
