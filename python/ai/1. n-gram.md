# **1. 问题背景：预测下一个词的概率**

给定一个词序列：

$$
w_1, w_2, w_3, \dots, w_T
$$

语言模型的目标是估计整个句子的概率：

$$
P(w_1, w_2, \dots, w_T)
$$

根据概率链式法则（chain rule）：

$$
P(w_1,\dots,w_T)=\prod_{t=1}^{T}P(w_t \mid w_1,\dots,w_{t-1})
$$

**问题：**计算条件概率 $P(w_t \mid w_1,\dots,w_{t-1})$ 很难，因为它依赖非常长的历史。

---

# **2. n-gram 的核心假设（马尔可夫假设）**

n-gram 模型做了一个简化的假设：

$$
\boxed{
P(w_t \mid w_1,\dots,w_{t-1})
\approx
P(w_t \mid w_{t-n+1}, \dots, w_{t-1})
}
$$

即：**下一个词只依赖前面 n−1 个词，而不是整个历史**。

例：

* uni-gram（1-gram）：
  $$
  P(w_t)
  $$
* bi-gram（2-gram）：
  $$
  P(w_t \mid w_{t-1})
  $$
* tri-gram（3-gram）：
  $$
  P(w_t \mid w_{t-2}, w_{t-1})
  $$

---

# **3. 使用计数估计概率**

n-gram 是一个**统计模型**，通过语料库中的计数来估计概率。

对于 n-gram：

$$
P(w_t \mid w_{t-n+1}, \dots, w_{t-1})
=

\frac{\text{Count}(w_{t-n+1}, \dots, w_{t-1}, w_t)}
{\text{Count}(w_{t-n+1}, \dots, w_{t-1})}
$$

解释：

* 分子 = 特定 n 个词连续出现的次数
* 分母 = 前 n−1 个词出现的次数
  这样得到一个**频率估计的条件概率**。

---

# **4. 句子概率的数学表达式**

综合链式法则与 n-gram 假设：

$$
P(w_1,\dots,w_T)
\approx
\prod_{t=1}^{T}
P(w_t \mid w_{t-n+1},\dots,w_{t-1})
$$

例：bigram（2-gram）时：

$$
P(w_1,\dots,w_T)=\prod_{t=1}^{T}P(w_t \mid w_{t-1})
$$

---

# **5. 平滑（smoothing）问题（简述）**

现实中，许多 n-gram 组合在语料库中 **从未出现**，直接计数会导致：

$$
P = 0
$$

因此需要平滑方法，例如：

* Laplace smoothing（加一平滑）
* Good–Turing
* Kneser–Ney（最常用）

但这超出基础数学解释，这里不展开。

---

# **6. n-gram 的数学本质**

n-gram 本质上是：

* 一个 **(n−1) 阶马尔可夫链**
* 使用最大似然估计（MLE）计算条件概率
* 基于离散序列的统计语言模型

---

# **数学总结**

$$
\boxed{
\text{n-gram} = \text{用计数估计的 } (n-1)\text{阶马尔可夫语言模型}
}
$$

公式核心：

1. 链式法则：

$$
P(w_1,\dots,w_T)=\prod_{t}P(w_t\mid w_1,\dots,w_{t-1})
$$

2. 马尔可夫假设（n-gram）：

$$
P(w_t\mid w_1,\dots,w_{t-1}) \approx P(w_t\mid w_{t-n+1},\dots,w_{t-1})
$$

3. 计数估计（MLE）：

$$
P(w_t\mid history)=\frac{count(history, w_t)}{count(history)}
$$
