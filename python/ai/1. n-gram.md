# **1. 问题背景：预测下一个词的概率**

给定一个词序列：

$$
w_1, w_2, w_3, \dots, w_T
$$

语言模型的目标是估计整个句子的概率：

$$
P(w_1, w_2, \dots, w_T)
$$

根据概率链式法则（chain rule）：

$$
P(w_1,\dots,w_T)=\prod_{t=1}^{T}P(w_t \mid w_1,\dots,w_{t-1})
$$

**问题：**计算条件概率 $P(w_t \mid w_1,\dots,w_{t-1})$ 很难，因为它依赖非常长的历史。

---

# **2. n-gram 的核心假设（马尔可夫假设）**

n-gram 模型做了一个简化的假设：

$$
\boxed{
P(w_t \mid w_1,\dots,w_{t-1})
\approx
P(w_t \mid w_{t-n+1}, \dots, w_{t-1})
}
$$

即：**下一个词只依赖前面 n−1 个词，而不是整个历史**。

例：

* uni-gram（1-gram）：
  $$
  P(w_t)
  $$
* bi-gram（2-gram）：
  $$
  P(w_t \mid w_{t-1})
  $$
* tri-gram（3-gram）：
  $$
  P(w_t \mid w_{t-2}, w_{t-1})
  $$

---

# **3. 使用计数估计概率**

n-gram 是一个**统计模型**，通过语料库中的计数来估计概率。

对于 n-gram：

$$
P(w_t \mid w_{t-n+1}, \dots, w_{t-1})
=

\frac{\text{Count}(w_{t-n+1}, \dots, w_{t-1}, w_t)}
{\text{Count}(w_{t-n+1}, \dots, w_{t-1})}
$$

解释：

* 分子 = 特定 n 个词连续出现的次数
* 分母 = 前 n−1 个词出现的次数
  这样得到一个**频率估计的条件概率**。

---

# **4. 句子概率的数学表达式**

综合链式法则与 n-gram 假设：

$$
P(w_1,\dots,w_T)
\approx
\prod_{t=1}^{T}
P(w_t \mid w_{t-n+1},\dots,w_{t-1})
$$

例：bigram（2-gram）时：

$$
P(w_1,\dots,w_T)=\prod_{t=1}^{T}P(w_t \mid w_{t-1})
$$

---

# **5. 平滑（smoothing）问题（简述）**

现实中，许多 n-gram 组合在语料库中 **从未出现**，直接计数会导致：

$$
P = 0
$$

因此需要平滑方法，例如：

* Laplace smoothing（加一平滑）
* Good–Turing
* Kneser–Ney（最常用）

但这超出基础数学解释，这里不展开。

---

# **6. n-gram 的数学本质**

n-gram 本质上是：

* 一个 **(n−1) 阶马尔可夫链**
* 使用最大似然估计（MLE）计算条件概率
* 基于离散序列的统计语言模型

---

# **数学总结**

$$
\boxed{
\text{n-gram} = \text{用计数估计的 } (n-1)\text{阶马尔可夫语言模型}
}
$$

公式核心：

1. 链式法则：

$$
P(w_1,\dots,w_T)=\prod_{t}P(w_t\mid w_1,\dots,w_{t-1})
$$

2. 马尔可夫假设（n-gram）：

$$
P(w_t\mid w_1,\dots,w_{t-1}) \approx P(w_t\mid w_{t-n+1},\dots,w_{t-1})
$$

3. 计数估计（MLE）：

$$
P(w_t\mid history)=\frac{count(history, w_t)}{count(history)}
$$


> Since word probabilities are estimated using word and n-gram counts collected from text corpora, 
> the model needs to deal with data sparsity (i.e.,assigning zero probabilities to unseen words or n-grams) by using smoothing, 
> where some probability mass of the model is reserved for unseen n-grams 


下面用**直观 + 数学**方式解释这段话的意思，帮助你彻底理解 *数据稀疏 (data sparsity)* 和 *平滑 (smoothing)* 的本质。

---

# 🔍 **先看问题：为什么会出现概率为 0？**

n-gram 模型用统计计数估计概率：

$$
P(w_t \mid w_{t-1}) = \frac{\text{Count}(w_{t-1}, w_t)}{\text{Count}(w_{t-1})}
$$

但是语料库再大，总有组合没出现过。例如：

```
"I love natural language"
从未出现 "love pizza"
```

那么：

$$
Count(love, pizza)=0 \Rightarrow P(pizza \mid love)=0
$$

**问题：**
人类语言无限复杂，语料库有限，因此**许多合理的 n-gram 会从未出现，但它们的概率不应该是 0**。

举例：
你没在训练语料中见过 “deep coffee learning”，但它并不是“绝对不可能”。

---

# 🔥 **这就是 data sparsity（数据稀疏）**

**数据稀疏**意味着：

* 很多合法的词组合没有被语料覆盖
* count = 0
* 概率 = 0 → 模型崩溃，例如句子概率整体变成 0

---

# 🧯 **解决：使用 smoothing（平滑）**

平滑的核心思想非常简单：

> **给所有没见过的 n-gram 分一点概率。**

也就是这句话说的：

> 模型预留一些概率质量（probability mass）给未出现的 n-grams。

换句话说：

* 不让任何词组合的概率为 0
* 小小“加油打气”：虽然你没见过这个组合，但不是完全不可能的！

---

# 🌟 举个非常直观的例子

假设 bigram “love + pizza” 没在训练出现过：

原始计数法：

```
P(pizza | love) = 0
```

加一平滑（最简单的平滑方法）：

$$
P(w_t \mid w_{t-1}) = \frac{Count + 1}{Total + |V|}
$$

即使 Count = 0，也会变成一个小概率，而不是 0。

---

# 📦 为什么要“保留概率质量”？（probability mass）

总概率必须满足：

$$
\sum_{w \in V} P(w \mid history) = 1
$$

如果你给“未见过的 n-gram”增加了一点概率，那么必须从“出现过的 n-gram”中扣一点——这就是：

> 预留一点 total probability mass
> 给从未见过但可能出现的组合。

如果不预留，概率会超过 1 或无法归一化。

---

# 🧠 **一句话总结**

这段话的意思是：

> 因为基于计数的 n-gram 模型会把没出现过的词组合赋予 0 概率（数据稀疏问题），所以必须通过“平滑”方法预留一部分概率给这些未出现的组合，以避免模型认为它们完全不可能出现。
