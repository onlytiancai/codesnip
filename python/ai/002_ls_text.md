这个脚本**完整演示**：

* one-hot 编码
* n-gram 构造
* 最小二乘解析解
* 用 argmax 做预测

1）这是**严格的最小二乘法**

* 没有 softmax
* 没有交叉熵
* 没有神经网络

2）预测本质是

> 在 one-hot 空间里做线性打分，然后 argmax

3）它学到的是

* 高频共现
* 模板结构
  而不是“语言理解”。

————————
五、非常适合的课堂延伸

你可以顺着这个例子自然引出：

* 为什么平方误差不适合语言
* 为什么要 softmax
* 为什么需要词向量
* 为什么线性模型不够

如果你愿意，我可以下一步帮你：

* 把这个脚本改成**岭回归版本**
* 画出 W 中某些权重的可解释含义
* 或给你一版**板书级数学推导对应代码**

---

> RuntimeWarning: divide by zero encountered in matmul W = Y @ X.T @ np.linalg.inv(X @ X.T + lam * I)

一、结论先说清楚

你这个 warning **不是 bug，也不是你代码写错了**，而是：

1）`X X^T` 极度病态（rank 严重不足）

2）`pinv` 在数值上产生了 `inf / nan`

3）后续矩阵乘法就全部“炸掉”了

这是**经典的最小二乘数值不稳定问题**，在你这个设置下几乎是必然的。

——
二、为什么一定会出问题（这是重点）

我们来量化一下你的矩阵规模：

* 词表 V = 23
* 输入维度 = 2V = 46
* 训练样本 T ≈ 30 左右

于是：

X ∈ ℝ^(46 × 30)

XXᵀ ∈ ℝ^(46 × 46)

但问题是：

1）X 的每一列只有 **2 个 1，其余全是 0**

2）很多上下文组合从未出现

3）大量维度在所有样本中始终为 0

结果就是：

* rank(X) ≪ 46
* XXᵀ 有大量 **零特征值**
* 数值伪逆时 → 除以接近 0 的奇异值
* 直接产生 overflow / divide by zero

⚠️ 这在“理论最小二乘”里是允许的，在“数值计算”里是灾难。

——
三、教学上**正确**的修复方式（推荐）

不是“硬消 warning”，而是**引入岭回归（L2 正则）**，并且明确告诉学生：

> 这是为了数值稳定性，而不是为了提升语言能力。

数学上只改一行：

W = Y Xᵀ (X Xᵀ + λI)⁻¹

这一步在教学上**非常自然、非常合理**。

——
五、这是一个**非常好的教学切入点**

你现在正好可以顺着这个报错讲清楚几件事：

1）理论最小二乘 ≠ 数值最小二乘
2）高维 + 稀疏 + 小样本 ⇒ 病态矩阵
3）为什么“正则化不是魔法，而是稳定性工具”
4）为什么语言模型不能只靠线性回归

这比“模型预测得准不准”要重要得多。