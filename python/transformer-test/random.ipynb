{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5dba118-6c3c-4d9f-8b54-29562479e338",
   "metadata": {},
   "source": [
    "# 最大似然估计学习\n",
    "\n",
    "onlytiancai 2025-08\n",
    "\n",
    "1. 生成随机数据并可视化\n",
    "2. 最大似然估计的数学推导及直接求解析解\n",
    "3. 理解概率密度和联合概率密度\n",
    "4. 梯度下降法求最大似然\n",
    "5. 什么情况下不能用最大似然估计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2664cfda-6b60-43a2-bbaf-928d993db975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # Windows 黑体，如果在 Mac 可换成 ['PingFang SC']\n",
    "plt.rcParams['axes.unicode_minus'] = False   # 解决负号显示问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb73da8-9569-4349-830a-ef13a436f000",
   "metadata": {},
   "source": [
    "## 一、生成随机数据并可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af736d-793f-43de-88a3-cbc1fe75c24f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mu_true = 5.0\n",
    "sigma_true = 2.0\n",
    "n_samples = 1000\n",
    "\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(loc=mu_true, scale=sigma_true, size=n_samples)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# 直方图\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(data, bins=30, density=True, alpha=0.6, color='b')\n",
    "plt.title('Histogram')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True)\n",
    "\n",
    "# KDE 图\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.kdeplot(data, fill=True, color='purple')\n",
    "plt.title('Kernel Density Estimation (KDE)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Boxplot 图\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(x=data, color='orange')\n",
    "plt.title('Boxplot')\n",
    "plt.xlabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1b823-c05e-437e-84c4-d69a5b0ad212",
   "metadata": {},
   "source": [
    "\n",
    "### 1. **直方图（Histogram）**\n",
    "\n",
    "* **原理**：\n",
    "  把数据的取值范围分成很多小区间（称为“桶”或“bin”），统计每个区间内数据点的数量。然后用矩形条形表示每个区间的频数（或频率、密度）。\n",
    "* **作用**：\n",
    "  展示数据的分布形状（比如是否偏态、是否有多峰），直观体现数据在哪些值附近密集。\n",
    "* **缺点**：\n",
    "  结果对区间宽度和起点比较敏感，不同的分箱方式可能看起来差别很大。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **核密度估计图（KDE, Kernel Density Estimation）**\n",
    "\n",
    "* **原理**：\n",
    "  KDE 是一种平滑的概率密度估计方法。它在每个数据点上放一个小“核函数”（通常是高斯核），然后把这些核函数叠加起来，形成一个连续的平滑曲线，估计数据的概率密度函数。\n",
    "* **作用**：\n",
    "  更平滑地展示数据分布，避免了直方图分箱带来的不连续感，可以更清楚看到数据的整体趋势和潜在的多峰结构。\n",
    "* **缺点**：\n",
    "  KDE 的平滑程度取决于带宽（kernel bandwidth）参数，选择不当会导致过于平滑或过于嘈杂。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **箱型图（Boxplot）**\n",
    "\n",
    "* **原理**：\n",
    "  箱型图通过五数概括（最小值、第一四分位数Q1、中位数Q2、第三四分位数Q3、最大值）来总结数据的分布。盒子显示中间50%的数据范围（Q1到Q3），中间的线表示中位数，盒子外的“须”通常表示数据的变动范围，超出“须”的点被认为是异常值。\n",
    "* **作用**：\n",
    "  快速显示数据的集中趋势、离散程度、偏态及异常值，非常适合做不同组数据间的比较。\n",
    "* **缺点**：\n",
    "  只显示统计摘要，不能像直方图或KDE那样详细展示数据的整体形态。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedc4a4e-2d47-4150-ac3d-30abc3b4919f",
   "metadata": {},
   "source": [
    "# 二、最大似然估计的数学推导及直接求解析解\n",
    "\n",
    "**正态分布 $X_i\\overset{\\text{iid}}{\\sim}N(\\mu,\\sigma^2)$** 的最大似然估计（MLE）详细推导，逐步求导并检验极值性质。\n",
    "\n",
    "## 1. 写出似然函数与对数似然\n",
    "\n",
    "样本为 $x_1,\\dots,x_n$。联合密度（似然）为\n",
    "\n",
    "$$\n",
    "L(\\mu,\\sigma^2)=\\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\Big(-\\frac{(x_i-\\mu)^2}{2\\sigma^2}\\Big).\n",
    "$$\n",
    "\n",
    "取对数得对数似然\n",
    "\n",
    "$$\n",
    "\\ell(\\mu,\\sigma^2)=\\log L(\\mu,\\sigma^2)\n",
    "= -\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i-\\mu)^2.\n",
    "$$\n",
    "\n",
    "## 2. 对 $\\mu$ 求偏导并令其为 0\n",
    "\n",
    "对 $\\mu$ 求偏导：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial \\mu}\n",
    "= -\\frac{1}{2\\sigma^2}\\cdot 2\\sum_{i=1}^n (x_i-\\mu)(-1)\n",
    "= \\frac{1}{\\sigma^2}\\sum_{i=1}^n (x_i-\\mu).\n",
    "$$\n",
    "\n",
    "令其为 0：\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n (x_i-\\mu)=0 \\quad\\Rightarrow\\quad n\\mu=\\sum_{i=1}^n x_i.\n",
    "$$\n",
    "\n",
    "于是得到\n",
    "\n",
    "$$\n",
    "\\hat\\mu_{\\text{MLE}}=\\bar x=\\frac{1}{n}\\sum_{i=1}^n x_i.\n",
    "$$\n",
    "\n",
    "（可检验二阶导数：$\\frac{\\partial^2\\ell}{\\partial\\mu^2}=-\\frac{n}{\\sigma^2}<0$，为极大值。）\n",
    "\n",
    "## 3. 对 $\\sigma^2$ 求偏导并令其为 0\n",
    "\n",
    "先对 $\\sigma^2$ 求偏导（把 $\\mu$ 用上一行的 $\\hat\\mu$ 替代或先一般求导再代入）：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial \\sigma^2}\n",
    "= -\\frac{n}{2}\\cdot\\frac{1}{\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^n (x_i-\\mu)^2.\n",
    "$$\n",
    "\n",
    "令其为 0 得\n",
    "\n",
    "$$\n",
    "-\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^n (x_i-\\mu)^2 = 0.\n",
    "$$\n",
    "\n",
    "两边乘以 $2(\\sigma^2)^2$：\n",
    "\n",
    "$$\n",
    "-n\\sigma^2 + \\sum_{i=1}^n (x_i-\\mu)^2 = 0\n",
    "$$\n",
    "\n",
    "解得\n",
    "\n",
    "$$\n",
    "\\hat\\sigma^2_{\\text{MLE}}=\\frac{1}{n}\\sum_{i=1}^n (x_i-\\mu)^2.\n",
    "$$\n",
    "\n",
    "通常把 $\\mu$ 用其 MLE $\\bar x$ 代入，所以\n",
    "\n",
    "$$\n",
    "\\boxed{\\;\\hat\\mu_{\\text{MLE}}=\\bar x,\\qquad \\hat\\sigma^2_{\\text{MLE}}=\\frac{1}{n}\\sum_{i=1}^n (x_i-\\bar x)^2\\;}\n",
    "$$\n",
    "\n",
    "## 4. 关于有无偏性的补充\n",
    "\n",
    "* $\\hat\\mu_{\\text{MLE}}=\\bar x$ 是无偏且是样本均值的常见估计。\n",
    "* $\\hat\\sigma^2_{\\text{MLE}}$（除以 $n$）**是有偏的**：其期望为 $\\mathbb{E}[\\hat\\sigma^2_{\\text{MLE}}]=\\frac{n-1}{n}\\sigma^2$。若要无偏估计方差，应使用\n",
    "\n",
    "  $$\n",
    "  s^2=\\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar x)^2,\n",
    "  $$\n",
    "\n",
    "  这是通常在样本方差中采用的 $n-1$ 分母（贝塞尔校正）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a159e817-6191-4a96-90fd-b6aa42c331df",
   "metadata": {},
   "source": [
    "## 5、打印似然估计值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77597dd9-2cbb-47c1-8eb2-b3cb6a103720",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mle = np.mean(data)\n",
    "sigma2_mle = np.mean((data - mu_mle)**2)\n",
    "sigma_mle = np.sqrt(sigma2_mle)\n",
    "\n",
    "print(mu_mle, sigma2_mle, sigma_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2898d92-cfc0-4818-8753-924a5e9eae6e",
   "metadata": {},
   "source": [
    "## 6、可视化估计值，真实值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b41f23-5888-4695-9b13-4f20fee98098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 绘制直方图 + PDF 曲线\n",
    "plt.figure(figsize=(8,5))\n",
    "# 数据直方图\n",
    "count, bins, _ = plt.hist(data, bins=30, density=True, alpha=0.6, color='skyblue', label='随机数据直方图')\n",
    "\n",
    "# x 轴范围\n",
    "x = np.linspace(min(data), max(data), 200)\n",
    "\n",
    "# 真实分布 PDF\n",
    "plt.plot(x, norm.pdf(x, mu_true, sigma_true), 'r-', lw=2, label='真实分布 PDF')\n",
    "\n",
    "# MLE 估计的 PDF\n",
    "plt.plot(x, norm.pdf(x, mu_mle, sigma_mle), 'g--', lw=2, label='MLE 估计 PDF')\n",
    "\n",
    "plt.title('正态分布随机点及其 MLE 拟合', fontsize=14)\n",
    "plt.xlabel('值')\n",
    "plt.ylabel('概率密度')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb1341-2304-48f5-9e31-d33a4844f1d2",
   "metadata": {},
   "source": [
    "# 三、理解概率密度和联合概率密度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a73b62d-0b3e-4f6c-aae7-141996410149",
   "metadata": {},
   "source": [
    "## 1、概率密度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e826bb10-1244-4f25-aaa8-0f0723d5acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, multivariate_normal\n",
    "\n",
    "# ======================\n",
    "# 1. 一维概率密度\n",
    "# ======================\n",
    "x = np.linspace(-4, 4, 200)\n",
    "mu = 0\n",
    "sigma = 1\n",
    "pdf_1d = norm.pdf(x, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57a159c-730e-44a2-a53b-21bf6d1659dc",
   "metadata": {},
   "source": [
    "计算标准正态分布（均值0，标准差1）在一组点上的概率密度函数（PDF）值。\n",
    "\n",
    "```python\n",
    "x = np.linspace(-4, 4, 200)\n",
    "```\n",
    "\n",
    "* 这行代码生成了一个包含200个点的一维数组 `x`，这些点均匀地分布在 -4 到 4 的区间内。\n",
    "* 这些点可以看作我们要计算概率密度的自变量（横轴坐标）。\n",
    "\n",
    "```python\n",
    "mu = 0\n",
    "sigma = 1\n",
    "```\n",
    "\n",
    "* 定义了正态分布的参数：均值（`mu`）为0，标准差（`sigma`）为1。\n",
    "* 这代表我们用的是标准正态分布。\n",
    "\n",
    "```python\n",
    "pdf_1d = norm.pdf(x, mu, sigma)\n",
    "```\n",
    "\n",
    "* 使用 `scipy.stats.norm.pdf` 函数计算正态分布在 `x` 中每个点对应的概率密度函数值（PDF）。\n",
    "* 结果是一个数组 `pdf_1d`，包含了每个 `x` 点对应的概率密度。\n",
    "* 这些值可以用来画出正态分布的曲线。\n",
    "\n",
    "这三行代码生成了一组点 `x`，然后计算了标准正态分布在这些点上的概率密度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c9b0d8-c987-4356-ad68-1d9bde77038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, pdf_1d, 'b-', lw=2)\n",
    "plt.title(\"一维正态分布概率密度\", fontsize=14)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"概率密度 f(x)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03db9f8-78c3-4c7a-9e8a-a57729288342",
   "metadata": {},
   "source": [
    "## 2、联合概率密度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2399474-a54d-480b-a35a-70fcc39f8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_vec = np.array([0, 0])\n",
    "cov_mat = np.array([[1, 0.5],\n",
    "                    [0.5, 1]])\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "pos = np.dstack((X, Y))\n",
    "rv = multivariate_normal(mean=mu_vec, cov=cov_mat)\n",
    "pdf_2d = rv.pdf(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb443bc-504c-4818-98db-b12009bfa49c",
   "metadata": {},
   "source": [
    "```python\n",
    "mu_vec = np.array([0, 0])\n",
    "cov_mat = np.array([[1, 0.5],\n",
    "                    [0.5, 1]])\n",
    "```\n",
    "\n",
    "* 定义二维正态分布的参数：\n",
    "\n",
    "  * 均值向量 `mu_vec` 是 `[0, 0]`，表示两个维度的均值都是0。\n",
    "  * 协方差矩阵 `cov_mat` 是一个2x2矩阵，表示两个维度的方差和协方差。对角线元素1表示两个维度的方差都是1，非对角线的0.5表示两个维度之间的正相关。\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "X, Y = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "```\n",
    "\n",
    "* 生成一个二维网格坐标矩阵：\n",
    "\n",
    "  * `np.linspace(-3, 3, 100)` 在区间 -3 到 3 内生成100个点。\n",
    "  * `np.meshgrid` 根据这两个一维数组生成二维网格的X和Y坐标矩阵，分别存储每个网格点的横纵坐标。\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "pos = np.dstack((X, Y))\n",
    "```\n",
    "\n",
    "* 把两个二维坐标矩阵 `X` 和 `Y` 按最后一个维度堆叠起来，形成一个形状为 `(100, 100, 2)` 的数组 `pos`。\n",
    "* 每个元素 `pos[i,j]` 是一个二维点的坐标 `[x, y]`，对应网格上的一个点。\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "rv = multivariate_normal(mean=mu_vec, cov=cov_mat)\n",
    "pdf_2d = rv.pdf(pos)\n",
    "```\n",
    "\n",
    "* 创建一个二维正态分布对象 `rv`，使用之前定义的均值和协方差矩阵。\n",
    "* 调用 `rv.pdf(pos)` 计算二维正态分布在每个网格点 `pos[i,j]` 上的概率密度值，结果 `pdf_2d` 是一个形状为 `(100, 100)` 的二维数组，存储对应的概率密度。\n",
    "\n",
    "---\n",
    "\n",
    "这段代码生成了一个二维平面上的点阵（网格），然后计算了二维高斯分布在这些点上的概率密度，用来绘制二维分布的等高线或热力图。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32bcdc2-61ff-4c3e-91f6-f1dc370d1d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建图形和子图\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 左图：等高线图\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "contour = ax1.contourf(X, Y, pdf_2d, cmap='viridis')\n",
    "fig.colorbar(contour, ax=ax1)\n",
    "ax1.set_title('2D Gaussian Contour')\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "\n",
    "# 右图：3D曲面图\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax2.plot_surface(X, Y, pdf_2d, cmap='viridis', edgecolor='none')\n",
    "ax2.set_title('3D Gaussian Surface')\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('Y')\n",
    "ax2.set_zlabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b614aa-9d53-4cdf-83ca-c3af8f0b1f3c",
   "metadata": {},
   "source": [
    "\n",
    "### 最大似然估计（MLE）和联合概率密度的关系\n",
    "\n",
    "当我们做最大似然估计时，目标是**找到参数，使得在给定参数下，观测到的数据出现的概率最大**。换句话说，就是最大化数据的**似然函数**。\n",
    "\n",
    "---\n",
    "\n",
    "### 为什么用联合概率密度？\n",
    "\n",
    "* 你有一组样本数据： $x = (x_1, x_2, \\dots, x_n)$。\n",
    "* 如果样本是独立同分布（i.i.d.），那么这组数据同时出现的概率（或概率密度）是各个样本点概率（密度）的乘积：\n",
    "\n",
    "$$\n",
    "p(x_1, x_2, \\dots, x_n \\mid \\theta) = \\prod_{i=1}^n p(x_i \\mid \\theta)\n",
    "$$\n",
    "\n",
    "这里 $\\theta$ 是我们想估计的参数，比如正态分布的均值和方差。\n",
    "\n",
    "* 这就是**联合概率密度**（联合分布），它表示在给定参数条件下，整个样本集合同时出现的概率（密度）。\n",
    "\n",
    "---\n",
    "\n",
    "### 最大化联合概率密度的理由\n",
    "\n",
    "* 我们观察到整组数据 $x$，想找到让这组数据“最可能”出现的参数 $\\theta$，自然要用**联合概率密度**作为衡量标准。\n",
    "* 单个点的概率（密度）不够，必须考虑所有数据一起的可能性，这就是联合概率。\n",
    "\n",
    "---\n",
    "\n",
    "### 为什么通常用负对数似然？\n",
    "\n",
    "* 乘积形式计算往往非常小且数值不稳定，\n",
    "* 所以通常取对数（对数似然），把乘积变成求和，方便计算和优化：\n",
    "\n",
    "$$\n",
    "\\log p(x_1, ..., x_n \\mid \\theta) = \\sum_{i=1}^n \\log p(x_i \\mid \\theta)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516c949a-9bff-48e0-a5ed-aa01290dc8cf",
   "metadata": {},
   "source": [
    "# 四、梯度下降法求最大似然估计"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf54c027-7116-4459-8b8b-9e6f05d07395",
   "metadata": {},
   "source": [
    "## 1、负对数似然及梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e04e8dc-806b-4722-8864-475e773c07fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_and_grads(x, mu, rho):\n",
    "    \"\"\"\n",
    "    x: 样本\n",
    "    mu: 均值参数\n",
    "    rho: log(sigma)，保证sigma>0\n",
    "    \"\"\"\n",
    "    n = x.size\n",
    "    dif = x - mu\n",
    "    S = np.sum(dif**2)\n",
    "    exp_minus_2rho = np.exp(-2.0 * rho)\n",
    "    L = n * rho + 0.5 * S * exp_minus_2rho  # 去掉常数项\n",
    "    dL_dmu = (n * mu - np.sum(x)) * exp_minus_2rho\n",
    "    dL_drho = n - S * exp_minus_2rho\n",
    "    return L, dL_dmu, dL_drho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90173cb1-8cfb-4d3c-bcd6-65ba9691c623",
   "metadata": {},
   "source": [
    "这段代码实现了\\*\\*一维正态分布负对数似然（Negative Log-Likelihood，NLL）\\*\\*及其对参数的梯度计算。参数是均值 `mu` 和一个经过变换的标准差参数 `rho`，其中 `rho = log(sigma)`，用来保证标准差 `sigma` 始终大于0。\n",
    "\n",
    "---\n",
    "\n",
    "### 输入参数\n",
    "\n",
    "* `x`：样本数据，形状为一维数组，包含 `n` 个数据点。\n",
    "* `mu`：正态分布的均值参数。\n",
    "* `rho`：对数标准差参数，`rho = log(sigma)`，保证 `sigma = exp(rho)` 总是正数。\n",
    "\n",
    "---\n",
    "\n",
    "### 代码核心含义\n",
    "\n",
    "```python\n",
    "n = x.size\n",
    "```\n",
    "\n",
    "* 样本数量。\n",
    "\n",
    "```python\n",
    "dif = x - mu\n",
    "S = np.sum(dif**2)\n",
    "```\n",
    "\n",
    "* 计算每个样本与均值的差值，然后计算平方差的和 $S = \\sum_{i=1}^n (x_i - \\mu)^2$。\n",
    "\n",
    "```python\n",
    "exp_minus_2rho = np.exp(-2.0 * rho)\n",
    "```\n",
    "\n",
    "* 计算 $e^{-2\\rho} = \\frac{1}{\\sigma^2}$，因为 $\\sigma = e^{\\rho}$，所以 $\\sigma^2 = e^{2\\rho}$，逆即 $e^{-2\\rho}$。\n",
    "\n",
    "---\n",
    "\n",
    "### 负对数似然函数 $L$\n",
    "\n",
    "```python\n",
    "L = n * rho + 0.5 * S * exp_minus_2rho  # 去掉常数项\n",
    "```\n",
    "\n",
    "* 标准正态分布的负对数似然（忽略常数项）为：\n",
    "\n",
    "$$\n",
    "L(\\mu, \\sigma) = n \\log \\sigma + \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "* 这里用 `rho = log(sigma)`，所以\n",
    "\n",
    "$$\n",
    "L = n \\cdot \\rho + \\frac{1}{2} S e^{-2\\rho}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 梯度计算\n",
    "\n",
    "```python\n",
    "dL_dmu = (n * mu - np.sum(x)) * exp_minus_2rho\n",
    "```\n",
    "\n",
    "* 负对数似然对均值的梯度：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mu} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (\\mu - x_i) = \\frac{n\\mu - \\sum x_i}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "* 用 $e^{-2\\rho} = \\frac{1}{\\sigma^2}$ 表示。\n",
    "\n",
    "```python\n",
    "dL_drho = n - S * exp_minus_2rho\n",
    "```\n",
    "\n",
    "* 负对数似然对 `rho` 的梯度：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\rho} = n - \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2 = n - S e^{-2\\rho}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 返回值\n",
    "\n",
    "```python\n",
    "return L, dL_dmu, dL_drho\n",
    "```\n",
    "\n",
    "* 返回负对数似然值 `L`，以及对均值 `mu` 和对数标准差参数 `rho` 的梯度。\n",
    "\n",
    "---\n",
    "\n",
    "### 总结\n",
    "\n",
    "* 这个函数计算基于样本数据的一维正态分布的负对数似然（忽略常数项）及其对均值和对数标准差的梯度。\n",
    "* 使用对数标准差 `rho` 保证标准差始终正值，有利于优化时参数的稳定性和数值安全。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d72a8-3068-46dd-b480-fd00390eb993",
   "metadata": {},
   "source": [
    "## 2、梯度下降-基础版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1836164-a11e-4b6e-9254-48cab4368520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_mle_basic(x, mu0=None, rho0=None, lr_mu=0.01, lr_rho=0.01,\n",
    "                               max_iters=5000, tol=1e-8):\n",
    "    if mu0 is None:\n",
    "        mu = np.mean(x) + 0.1 * np.std(x)\n",
    "    else:\n",
    "        mu = float(mu0)\n",
    "    if rho0 is None:\n",
    "        rho = np.log(np.std(x))\n",
    "    else:\n",
    "        rho = float(rho0)\n",
    "\n",
    "    prev_L = None\n",
    "    mu_hist, sigma_hist, loss_hist = [], [], []\n",
    "    for it in range(1, max_iters+1):\n",
    "        L, g_mu, g_rho = nll_and_grads(x, mu, rho)\n",
    "\n",
    "        mu -= lr_mu * g_mu\n",
    "        rho -= lr_rho * g_rho\n",
    "\n",
    "        mu_hist.append(mu)\n",
    "        sigma_hist.append(np.exp(rho))\n",
    "        loss_hist.append(L)\n",
    "\n",
    "        if prev_L is not None and abs(prev_L - L) < tol:\n",
    "            break\n",
    "        prev_L = L\n",
    "\n",
    "    return mu, np.exp(rho), L, mu_hist, sigma_hist, loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060493b3-503f-4315-ae36-89392aac32b2",
   "metadata": {},
   "source": [
    "\n",
    "### 函数作用\n",
    "\n",
    "这个函数实现了基于**负对数似然（NLL）**的**梯度下降法**，用于估计一维正态分布的两个参数：均值 `mu` 和标准差（用对数参数 `rho = log(sigma)` 表示），使得模型拟合样本数据 `x`。\n",
    "\n",
    "---\n",
    "\n",
    "### 输入参数说明\n",
    "\n",
    "* `x`: 样本数据（一维 numpy 数组）。\n",
    "* `mu0`: 初始均值参数（默认用样本均值附近初始化）。\n",
    "* `rho0`: 初始对数标准差参数（默认用样本标准差初始化）。\n",
    "* `lr_mu`: 均值参数的学习率（更新步长）。\n",
    "* `lr_rho`: 对数标准差参数的学习率。\n",
    "* `max_iters`: 最大迭代次数。\n",
    "* `tol`: 收敛阈值，当损失函数变化小于该值时停止迭代。\n",
    "\n",
    "---\n",
    "\n",
    "### 代码流程详解\n",
    "\n",
    "```python\n",
    "if mu0 is None:\n",
    "    mu = np.mean(x) + 0.1 * np.std(x)\n",
    "else:\n",
    "    mu = float(mu0)\n",
    "if rho0 is None:\n",
    "    rho = np.log(np.std(x))\n",
    "else:\n",
    "    rho = float(rho0)\n",
    "```\n",
    "\n",
    "* 初始化参数。\n",
    "* 如果用户没给初始值，默认：\n",
    "\n",
    "  * `mu` 初始化为样本均值稍微偏移一点（避免刚开始梯度为0等情况）。\n",
    "  * `rho` 初始化为样本标准差的对数（保证开始时的标准差合理）。\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "prev_L = None\n",
    "mu_hist, sigma_hist, loss_hist = [], [], []\n",
    "```\n",
    "\n",
    "* 记录之前的损失函数值（用于判断收敛）。\n",
    "* 准备保存每次迭代的 `mu`、`sigma`（`exp(rho)`）和损失值，用于后续分析或画图。\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "for it in range(1, max_iters+1):\n",
    "    L, g_mu, g_rho = nll_and_grads(x, mu, rho)\n",
    "```\n",
    "\n",
    "* 进入迭代循环，最多执行 `max_iters` 次。\n",
    "* 调用之前定义的 `nll_and_grads` 函数，计算当前参数下的负对数似然值 `L`，以及对 `mu` 和 `rho` 的梯度 `g_mu` 和 `g_rho`。\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "mu -= lr_mu * g_mu\n",
    "rho -= lr_rho * g_rho\n",
    "```\n",
    "\n",
    "* 根据梯度更新参数。\n",
    "* 沿梯度负方向移动，使负对数似然（损失）减小。\n",
    "* 学习率控制步长大小。\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "mu_hist.append(mu)\n",
    "sigma_hist.append(np.exp(rho))\n",
    "loss_hist.append(L)\n",
    "```\n",
    "\n",
    "* 记录当前迭代的参数和损失值，方便查看收敛过程。\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "if prev_L is not None and abs(prev_L - L) < tol:\n",
    "    break\n",
    "prev_L = L\n",
    "```\n",
    "\n",
    "* 判断是否收敛：\n",
    "\n",
    "  * 如果当前损失与上一次变化小于阈值 `tol`，认为已经收敛，跳出循环。\n",
    "* 更新 `prev_L` 以备下一轮比较。\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "return mu, np.exp(rho), L, mu_hist, sigma_hist, loss_hist\n",
    "```\n",
    "\n",
    "* 返回最后得到的参数估计：\n",
    "\n",
    "  * `mu`（均值）\n",
    "  * `exp(rho)`（标准差）\n",
    "  * `L`（最终负对数似然值）\n",
    "  * 以及迭代过程中记录的参数和损失历史，方便分析和绘图。\n",
    "\n",
    "---\n",
    "\n",
    "该函数用梯度下降法，基于负对数似然函数，迭代更新参数 `mu` 和 `rho`，最终找到最优的正态分布参数拟合样本数据。简单、直观，适合理解和教学。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50baff64-d505-4ab1-9c06-b882553a017b",
   "metadata": {},
   "source": [
    "## 3、梯度下降-adam优化器版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c601dc12-9fa9-4ec1-b910-568748a7283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_mle_adam(x, mu0=None, rho0=None, lr_mu=0.01, lr_rho=0.01,\n",
    "                              max_iters=5000, tol=1e-8):\n",
    "    if mu0 is None:\n",
    "        mu = np.mean(x) + 0.1 * np.std(x)\n",
    "    else:\n",
    "        mu = float(mu0)\n",
    "    if rho0 is None:\n",
    "        rho = np.log(np.std(x))\n",
    "    else:\n",
    "        rho = float(rho0)\n",
    "\n",
    "    beta1, beta2 = 0.9, 0.999\n",
    "    eps = 1e-8\n",
    "    m_mu = 0.0; v_mu = 0.0\n",
    "    m_rho = 0.0; v_rho = 0.0\n",
    "\n",
    "    prev_L = None\n",
    "    mu_hist, sigma_hist, loss_hist = [], [], []\n",
    "    for it in range(1, max_iters+1):\n",
    "        L, g_mu, g_rho = nll_and_grads(x, mu, rho)\n",
    "\n",
    "        m_mu = beta1 * m_mu + (1 - beta1) * g_mu\n",
    "        v_mu = beta2 * v_mu + (1 - beta2) * (g_mu ** 2)\n",
    "        m_mu_hat = m_mu / (1 - beta1 ** it)\n",
    "        v_mu_hat = v_mu / (1 - beta2 ** it)\n",
    "        mu -= lr_mu * m_mu_hat / (np.sqrt(v_mu_hat) + eps)\n",
    "\n",
    "        m_rho = beta1 * m_rho + (1 - beta1) * g_rho\n",
    "        v_rho = beta2 * v_rho + (1 - beta2) * (g_rho ** 2)\n",
    "        m_rho_hat = m_rho / (1 - beta1 ** it)\n",
    "        v_rho_hat = v_rho / (1 - beta2 ** it)\n",
    "        rho -= lr_rho * m_rho_hat / (np.sqrt(v_rho_hat) + eps)\n",
    "\n",
    "        mu_hist.append(mu)\n",
    "        sigma_hist.append(np.exp(rho))\n",
    "        loss_hist.append(L)\n",
    "\n",
    "        if prev_L is not None and abs(prev_L - L) < tol:\n",
    "            break\n",
    "        prev_L = L\n",
    "\n",
    "    return mu, np.exp(rho), L, mu_hist, sigma_hist, loss_hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32459d3-d29c-4cbd-bf79-e20227024517",
   "metadata": {},
   "source": [
    "## 4、训练并可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c264f-a8af-49b2-97df-bb9b5135de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data\n",
    "\n",
    "# 解析解\n",
    "mu_mle_closed = np.mean(x)\n",
    "sigma2_mle_closed = np.mean((x - mu_mle_closed)**2)\n",
    "sigma_mle_closed = np.sqrt(sigma2_mle_closed)\n",
    "\n",
    "# 对比 Adam 和普通梯度下降的收敛曲线\n",
    "\n",
    "# 普通梯度下降\n",
    "mu_gd, sigma_gd, final_loss_gd, mu_hist_gd, sigma_hist_gd, loss_hist_gd = gradient_descent_mle_basic(\n",
    "    x, lr_mu=0.0005, lr_rho=0.0005, max_iters=20000\n",
    ")\n",
    "\n",
    "# Adam\n",
    "mu_adam, sigma_adam, final_loss_adam, mu_hist_adam, sigma_hist_adam, loss_hist_adam = gradient_descent_mle_adam(\n",
    "    x, lr_mu=0.05, lr_rho=0.05, max_iters=20000\n",
    ")\n",
    "\n",
    "# 绘制对比\n",
    "fig, axes = plt.subplots(3, 1, figsize=(8, 10))\n",
    "\n",
    "# mu 收敛\n",
    "axes[0].plot(mu_hist_gd, label=\"普通GD\")\n",
    "axes[0].plot(mu_hist_adam, label=\"Adam\")\n",
    "axes[0].axhline(mu_mle_closed, color='r', linestyle='--', label='解析解 μ')\n",
    "axes[0].set_ylabel(r\"$\\mu$\")\n",
    "axes[0].set_title(\"均值参数收敛对比\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# sigma 收敛\n",
    "axes[1].plot(sigma_hist_gd, label=\"普通GD\")\n",
    "axes[1].plot(sigma_hist_adam, label=\"Adam\")\n",
    "axes[1].axhline(sigma_mle_closed, color='r', linestyle='--', label='解析解 σ')\n",
    "axes[1].set_ylabel(r\"$\\sigma$\")\n",
    "axes[1].set_title(\"标准差参数收敛对比\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Loss 收敛\n",
    "axes[2].plot(loss_hist_gd, label=\"普通GD\")\n",
    "axes[2].plot(loss_hist_adam, label=\"Adam\")\n",
    "axes[2].set_ylabel(\"Loss\")\n",
    "axes[2].set_xlabel(\"迭代次数\")\n",
    "axes[2].set_title(\"负对数似然收敛对比\")\n",
    "axes[2].grid(alpha=0.3)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print((mu_gd, sigma_gd, final_loss_gd), (mu_adam, sigma_adam, final_loss_adam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a8de9-a089-4399-96ef-f382e1256ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e29e04ee-2370-4bda-a3d2-47d3052eddde",
   "metadata": {},
   "source": [
    "# 五、什么情况下不能用最大似然估计\n",
    "\n",
    "## 1、数据分布形状未知怎么办？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd67b35-2dfc-4e38-bc05-2f7dff2a3eac",
   "metadata": {},
   "source": [
    "### **如果分布未知，先要估计分布形状**\n",
    "\n",
    "* 经典方法是用**非参数方法**估计密度，比如核密度估计（KDE），但这类方法一般不方便直接求参数梯度。\n",
    "* 另一条路是**假设某个参数化的模型族**（例如高斯混合模型、神经网络生成模型等），用这些模型去拟合数据。\n",
    "* 这时“参数”是模型内部参数，虽然不知道真实分布，但假设模型可拟合。\n",
    "\n",
    "---\n",
    "\n",
    "### **基于样本的优化（无模型参数梯度）**\n",
    "\n",
    "* 如果没有明确的概率模型，梯度计算不能依赖概率密度函数的解析式。\n",
    "* 可以用**基于样本的目标函数**，例如最小化某种损失函数，比如重构误差、对比损失等。\n",
    "* 这样梯度是对损失函数的梯度，不是对概率密度函数的梯度。\n",
    "\n",
    "---\n",
    "\n",
    "### **使用概率无关的优化方法**\n",
    "\n",
    "* 进化算法、强化学习、蒙特卡洛方法这类无梯度或近似梯度的优化方法可用。\n",
    "* 例如使用**梯度估计方法**，比如强化学习中的策略梯度、REINFORCE算法，用采样和回报信号估计梯度。\n",
    "\n",
    "---\n",
    "\n",
    "### **现代机器学习中的方法**\n",
    "\n",
    "* **生成模型（GAN、VAE）**：虽然分布未知，但模型学到一个参数化的生成分布，利用神经网络梯度求参数。\n",
    "* **能量模型（Energy-Based Models）**：用能量函数替代概率密度，间接优化模型。\n",
    "* **基于核方法或核嵌入的方法**，利用样本和核函数计算梯度。\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **总结**\n",
    "\n",
    "* **没有明确分布函数，不能直接写出负对数似然和梯度。**\n",
    "* 需要假设或学习一个参数化的模型，或设计其他损失函数。\n",
    "* 梯度来自模型参数对目标函数的导数，而不是直接对概率密度。\n",
    "* 采样和近似梯度估计也是重要手段。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb73834-f055-496f-919d-2617b34e0cd7",
   "metadata": {},
   "source": [
    "## 2、神经网络如何处理未知分布和参数？\n",
    "\n",
    "**用神经网络参数化分布**\n",
    "\n",
    "* 神经网络可以作为一个函数逼近器，学习一个从输入（例如随机噪声、条件变量）到数据空间的映射。\n",
    "* 这种映射可以定义一个参数化的概率模型，比如生成对抗网络（GAN）、变分自编码器（VAE）等。\n",
    "* 网络参数就是我们需要优化的“分布参数”，但它不是传统意义上的概率分布参数，而是网络权重和偏置。\n",
    "\n",
    "**通过样本和目标函数学习参数**\n",
    "\n",
    "* 你不需要事先知道数据的真实分布形式，只用样本数据训练神经网络。\n",
    "* 通过定义合适的损失函数（例如GAN里的对抗损失，VAE里的变分下界，或者最大似然变分近似），网络通过**梯度下降**学习参数。\n",
    "* 这里的梯度是通过**反向传播**自动计算的，无需显式写出概率密度梯度。\n",
    "\n",
    "**损失函数起到“代理目标”的作用**\n",
    "\n",
    "* 损失函数衡量神经网络输出与真实数据的“差异”或拟合程度。\n",
    "* 通过不断最小化这个损失，神经网络参数迭代更新，间接学习数据的潜在分布。\n",
    "\n",
    "**梯度计算和优化**\n",
    "\n",
    "* 反向传播结合自动微分库（如PyTorch、TensorFlow）自动求梯度。\n",
    "* 梯度用于梯度下降或其变体（Adam、RMSProp等）优化网络参数。\n",
    "* 这里并不直接对概率密度求导，而是对损失函数求导。\n",
    "\n",
    "---\n",
    "\n",
    "### 举个简单例子\n",
    "\n",
    "* 你有一组数据样本，但不知道分布。\n",
    "* 建一个神经网络 $f_\\theta(z)$，输入随机噪声 $z$，输出生成样本。\n",
    "* 设计一个损失函数（比如GAN中的判别器判断生成样本和真实样本差异）。\n",
    "* 用梯度下降优化网络参数 $\\theta$，让生成样本越来越像真实样本。\n",
    "\n",
    "---\n",
    "\n",
    "### 总结\n",
    "\n",
    "* **神经网络用参数化函数替代传统概率分布**，让“分布学习”成为参数学习问题。\n",
    "* 通过定义可优化的损失函数和反向传播自动计算梯度。\n",
    "* 不需要事先知道分布解析形式，只需样本数据即可进行训练。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f35c82-e69a-4c46-8bbd-0d93d6a0016c",
   "metadata": {},
   "source": [
    "# 六、正态分布概率密度函数推导\n",
    "\n",
    "## 目标\n",
    "\n",
    "推导一维正态分布概率密度函数的解析式：\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 从高斯函数开始\n",
    "\n",
    "假设概率密度函数形状满足：\n",
    "\n",
    "$$\n",
    "f(x) = A \\exp\\left(-B(x - \\mu)^2\\right)\n",
    "$$\n",
    "\n",
    "其中，$A > 0, B > 0$ 是常数，$\\mu$ 是均值。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 要满足的条件\n",
    "\n",
    "* **概率密度函数积分为1**：\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{+\\infty} f(x) dx = 1\n",
    "$$\n",
    "\n",
    "* **均值是 $\\mu$**，即\n",
    "\n",
    "$$\n",
    "E[X] = \\int_{-\\infty}^{+\\infty} x f(x) dx = \\mu\n",
    "$$\n",
    "\n",
    "* **方差是 $\\sigma^2$**，即\n",
    "\n",
    "$$\n",
    "Var(X) = \\int_{-\\infty}^{+\\infty} (x - \\mu)^2 f(x) dx = \\sigma^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 求积分确定 $A$\n",
    "\n",
    "计算积分：\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{+\\infty} A \\exp(-B (x-\\mu)^2) dx = 1\n",
    "$$\n",
    "\n",
    "变量替换：令 $y = x - \\mu$，则\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{+\\infty} A \\exp(-B y^2) dy = 1\n",
    "$$\n",
    "\n",
    "利用高斯积分公式：\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{+\\infty} e^{-a y^2} dy = \\sqrt{\\frac{\\pi}{a}}, \\quad a > 0\n",
    "$$\n",
    "\n",
    "得\n",
    "\n",
    "$$\n",
    "A \\sqrt{\\frac{\\pi}{B}} = 1 \\implies A = \\sqrt{\\frac{B}{\\pi}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 确定 $B$ 使方差是 $\\sigma^2$\n",
    "\n",
    "计算方差：\n",
    "\n",
    "$$\n",
    "Var(X) = \\int (x - \\mu)^2 f(x) dx = \\int y^2 A e^{-B y^2} dy\n",
    "$$\n",
    "\n",
    "利用积分结果（高斯二阶矩）：\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{+\\infty} y^2 e^{-a y^2} dy = \\frac{\\sqrt{\\pi}}{2 a^{3/2}}\n",
    "$$\n",
    "\n",
    "代入：\n",
    "\n",
    "$$\n",
    "Var(X) = A \\frac{\\sqrt{\\pi}}{2 B^{3/2}} = \\sqrt{\\frac{B}{\\pi}} \\times \\frac{\\sqrt{\\pi}}{2 B^{3/2}} = \\frac{1}{2B}\n",
    "$$\n",
    "\n",
    "所以：\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\frac{1}{2B} \\implies B = \\frac{1}{2\\sigma^2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 最终概率密度函数\n",
    "\n",
    "带入 $A$ 和 $B$ 的表达式：\n",
    "\n",
    "$$\n",
    "A = \\sqrt{\\frac{1}{2\\pi\\sigma^2}} = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}, \\quad\n",
    "B = \\frac{1}{2\\sigma^2}\n",
    "$$\n",
    "\n",
    "所以\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n",
    "}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be831bcd-9ca0-483e-b434-ec11b074942f",
   "metadata": {},
   "source": [
    "## 为什么用到高斯函数\n",
    "\n",
    "为什么推导正态分布时，我们会先**假设概率密度函数是一个指数的平方型函数**，即\n",
    "\n",
    "$$\n",
    "f(x) = A \\exp\\left(-B(x - \\mu)^2\\right)\n",
    "$$\n",
    "\n",
    "这背后其实有几个深层的理由和历史背景：\n",
    "\n",
    "---\n",
    "\n",
    "### 1. 正态分布的定义和性质来源\n",
    "\n",
    "* **正态分布最初是通过中心极限定理引入的**，它描述大量独立随机变量和的分布趋近于某种特定形式。\n",
    "* 数学家发现，这种极限分布满足**对称、单峰、且在远离均值时快速衰减的性质**。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 高斯函数的自然形式\n",
    "\n",
    "* 形如 $\\exp(-B(x-\\mu)^2)$ 的函数叫**高斯函数**，它的形状满足：\n",
    "\n",
    "  * 对称于均值 $\\mu$\n",
    "  * 单峰，且远离 $\\mu$ 时快速趋近于0\n",
    "  * 二阶函数的指数保证了平滑且极易计算积分（解析性强）\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 最大熵原理\n",
    "\n",
    "* 如果已知一个连续随机变量的均值和方差，但对其它信息一无所知，**最大熵分布**（即在所有满足这两个约束的分布中熵最大的分布）就是正态分布。\n",
    "* 这个最大熵分布自然对应于二次型指数函数。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 数学上的便利性\n",
    "\n",
    "* 假设指数中是平方项，能方便求导和积分（高斯积分公式），方便验证概率密度函数的性质。\n",
    "* 也是对称单峰分布的最简单解析表达。\n",
    "\n",
    "---\n",
    "\n",
    "### 5. 经验与实验\n",
    "\n",
    "* 经验上许多自然现象（误差、噪声、测量误差）符合或近似符合这种平方指数形式。\n",
    "* 所以假设先从这个形状入手，再确定系数 $A$ 和 $B$。\n",
    "\n",
    "---\n",
    "\n",
    "### 总结\n",
    "\n",
    "* 这是一个经验、数学方便性和理论（最大熵、中心极限定理）共同作用的结果。\n",
    "* 通过假设这种形式，我们能导出独一无二且满足概率密度性质的函数，即正态分布。\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea460cc-69e2-45ad-acbb-38f3d6f846f9",
   "metadata": {},
   "source": [
    "## 高斯函数的其它用途\n",
    "\n",
    "高斯函数可不只是“拟合正态分布”的工具，它其实是数学、信号处理、物理、机器学习等领域的**超级多面手**。我帮你按领域整理一下：\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 信号与图像处理\n",
    "\n",
    "* **平滑滤波（Gaussian smoothing）**\n",
    "  在图像处理中，用高斯卷积核去平滑图像，可以有效去除高频噪声，同时保留边缘的大致形状。\n",
    "* **边缘检测（DoG / LoG）**\n",
    "  先用高斯平滑，再求导（高斯差分、拉普拉斯-高斯），能提取出清晰的边缘。\n",
    "* **抗混叠（Anti-aliasing）**\n",
    "  在采样前用高斯低通滤波减少高频信号，避免混叠失真。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 概率与统计\n",
    "\n",
    "* **核密度估计（Gaussian KDE）**\n",
    "  用高斯核估计未知分布的概率密度，常用于数据分布可视化和非参数统计。\n",
    "* **高斯混合模型（GMM）**\n",
    "  把数据看成多个高斯分布的加权和，用于聚类、密度建模、异常检测。\n",
    "* **贝叶斯推断**\n",
    "  高斯分布是共轭分布，推导方便，很多后验分布推出来还是高斯。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 机器学习与模式识别\n",
    "\n",
    "* **RBF核（径向基函数核）**\n",
    "  支持向量机、核回归等方法中，高斯函数用作相似度度量，能把非线性问题映射到高维。\n",
    "* **神经网络激活函数**\n",
    "  RBF网络（Radial Basis Function Network）直接用高斯函数作为激活函数。\n",
    "* **特征加权**\n",
    "  在某些算法中，用高斯权重对距离较近的样本赋予更大影响。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 物理与工程\n",
    "\n",
    "* **热传导方程的解**\n",
    "  一维热传导的瞬时解就是高斯函数形状（热扩散曲线）。\n",
    "* **波包分析**\n",
    "  在量子力学中，高斯波包用于表示粒子位置和动量的概率分布。\n",
    "* **光学衍射与成像**\n",
    "  高斯光束模型描述激光的强度分布和传播特性。\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 数学与其他\n",
    "\n",
    "* **卷积的好性质**\n",
    "  两个高斯函数卷积还是高斯，方差相加——这在信号叠加、概率建模里非常方便。\n",
    "* **傅里叶变换**\n",
    "  高斯函数的傅里叶变换还是高斯（自相似性），在频域分析中很重要。\n",
    "* **权重函数**\n",
    "  在数值积分、加权最小二乘等地方，高斯函数常用来给“中间值”更多权重，远处值权重逐渐衰减。\n",
    "\n",
    "---\n",
    "\n",
    "✅ 小结：\n",
    "高斯函数之所以这么万能，核心原因是：\n",
    "\n",
    "1. 数学形式简洁，推导方便\n",
    "2. 平滑且无限可微，适合近似和优化\n",
    "3. 在卷积和变换下稳定（自相似）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6fb41-94f7-49fc-bb3a-6beedc8fe0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
