# wawa_transformer.py

模型说明：

1. **核心组件**：包含位置编码、多头注意力、前馈神经网络等Transformer关键模块
2. **结构划分**：分为编码器层（自注意力+前馈）和解码器层（掩蔽自注意力+交叉注意力+前馈）
3. **参数说明**：
   - `d_model`：模型隐藏层维度（默认512）
   - `num_layers`：编码器/解码器堆叠层数（默认6）
   - `num_heads`：注意力头数（默认8）
   - `d_ff`：前馈网络中间层维度（默认2048）

实际使用时，需根据任务（如翻译、文本生成）调整掩码策略，并添加适当的训练流程（损失函数、优化器等）。

# test.py

下面设计一个简单的**文本翻译任务用例**来测试Transformer模型，模拟从"简单数字序列"到"字母序列"的翻译（便于直观验证）。


### 任务设定
- **输入（源序列）**：数字序列（如 `[1, 2, 3]`）
- **输出（目标序列）**：对应字母序列（如 `[A, B, C]`，其中1→A，2→B，…，5→E）
- **词汇表**：
  - 源词汇表：`{0: <PAD>, 1:1, 2:2, 3:3, 4:4, 5:5}`（0为填充符）
  - 目标词汇表：`{0: <PAD>, 1:A, 2:B, 3:C, 4:D, 5:E, 6:<SOS>, 7:<EOS>}`（6为起始符，7为结束符）



### 用例说明
1. **任务简化**：用数字→字母的简单映射验证模型是否能学习序列转换，避免复杂文本预处理
2. **模型轻量化**：减小 `d_model`、`num_layers` 等参数，加快训练速度
3. **核心验证点**：
   - 能否通过自注意力捕捉输入序列的依赖关系
   - 能否通过交叉注意力关联源序列和目标序列
   - 解码器是否会生成符合规则的输出（含起始符和结束符）

如果输出不符合预期，可增加训练轮次（如200轮）或调整模型参数（如增大 `d_model`）。