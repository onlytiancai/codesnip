# wawa_transformer.py

模型说明：

1. **核心组件**：包含位置编码、多头注意力、前馈神经网络等Transformer关键模块
2. **结构划分**：分为编码器层（自注意力+前馈）和解码器层（掩蔽自注意力+交叉注意力+前馈）
3. **参数说明**：
   - `d_model`：模型隐藏层维度（默认512）
   - `num_layers`：编码器/解码器堆叠层数（默认6）
   - `num_heads`：注意力头数（默认8）
   - `d_ff`：前馈网络中间层维度（默认2048）

实际使用时，需根据任务（如翻译、文本生成）调整掩码策略，并添加适当的训练流程（损失函数、优化器等）。

# test.py

一个简单的**文本翻译任务用例**来测试Transformer模型，模拟从"简单数字序列"到"字母序列"的翻译（便于直观验证）。


### 任务设定
- **输入（源序列）**：数字序列（如 `[1, 2, 3]`）
- **输出（目标序列）**：对应字母序列（如 `[A, B, C]`，其中1→A，2→B，…，5→E）
- **词汇表**：
  - 源词汇表：`{0: <PAD>, 1:1, 2:2, 3:3, 4:4, 5:5}`（0为填充符）
  - 目标词汇表：`{0: <PAD>, 1:A, 2:B, 3:C, 4:D, 5:E, 6:<SOS>, 7:<EOS>}`（6为起始符，7为结束符）



### 用例说明
1. **任务简化**：用数字→字母的简单映射验证模型是否能学习序列转换，避免复杂文本预处理
2. **模型轻量化**：减小 `d_model`、`num_layers` 等参数，加快训练速度
3. **核心验证点**：
   - 能否通过自注意力捕捉输入序列的依赖关系
   - 能否通过交叉注意力关联源序列和目标序列
   - 解码器是否会生成符合规则的输出（含起始符和结束符）

如果输出不符合预期，可增加训练轮次（如200轮）或调整模型参数（如增大 `d_model`）。

输出
```
$ python test.py
开始训练，总训练样本数: 28
Epoch 20/300, Loss: 0.3004
Epoch 40/300, Loss: 0.0045
Epoch 60/300, Loss: 0.3947
Epoch 80/300, Loss: 0.0629
Epoch 100/300, Loss: 0.7253
Epoch 120/300, Loss: 0.1047
Epoch 140/300, Loss: 0.0028
Epoch 160/300, Loss: 0.0482
Epoch 180/300, Loss: 0.0843
Epoch 200/300, Loss: 0.0038
Epoch 220/300, Loss: 0.1479
Epoch 240/300, Loss: 0.2682
Epoch 260/300, Loss: 0.0026
Epoch 280/300, Loss: 0.2107
Epoch 300/300, Loss: 0.0448

测试用例 1:
输入（数字）：[3, 4]
模型输出（字母）：['< SOS >', 'C', 'D', '<EOS>']
预期输出：['< SOS >', 'C', 'D', '<EOS>']
是否正确：✓

测试用例 2:
输入（数字）：[1, 2]
模型输出（字母）：['< SOS >', 'A', 'B', '<EOS>']
预期输出：['< SOS >', 'A', 'B', '<EOS>']
是否正确：✓

测试用例 3:
输入（数字）：[5]
模型输出（字母）：['< SOS >', 'E', '<EOS>']
预期输出：['< SOS >', 'E', '<EOS>']
是否正确：✓

测试用例 4:
输入（数字）：[2, 3, 4]
模型输出（字母）：['< SOS >', 'B', 'C', 'D', '<EOS>']
预期输出：['< SOS >', 'B', 'C', 'D', '<EOS>']
是否正确：✓
```
## QA

### 位置编码测试输出

positional_encoding_test.py

```
$ python positional_encoding_test.py
============================================================
位置编码(PositionalEncoding)原理测试
============================================================
模型维度 d_model: 512
最大序列长度 max_len: 100
位置编码矩阵形状: torch.Size([100, 1, 512])

==================================================
测试1: 位置编码的基本性质
==================================================
输入序列形状: torch.Size([10, 2, 512])
输出序列形状: torch.Size([10, 2, 512])
位置编码是否改变了序列长度: True

==================================================
测试2: 验证不同位置的编码唯一性
==================================================
前5个位置编码的余弦相似度矩阵:
位置0: ['1.000', '0.973', '0.905', '0.827', '0.768']
位置1: ['0.973', '1.000', '0.973', '0.905', '0.827']
位置2: ['0.905', '0.973', '1.000', '0.973', '0.905']
位置3: ['0.827', '0.905', '0.973', '1.000', '0.973']
位置4: ['0.768', '0.827', '0.905', '0.973', '1.000']

==================================================
测试3: 三角函数编码的周期性分析
==================================================
前20个位置在不同维度的编码值:
位置\维度       dim_0   dim_1   dim_2   dim_3   dim_10  dim_11
pos_0           0.000   1.000   0.000   1.000   0.000   1.000
pos_1           0.841   0.540   0.822   0.570   0.742   0.671
pos_2           0.909   -0.416  0.936   -0.351  0.995   -0.100
pos_3           0.141   -0.990  0.245   -0.970  0.594   -0.805
pos_4           -0.757  -0.654  -0.657  -0.754  -0.199  -0.980
pos_5           -0.959  0.284   -0.994  0.111   -0.860  -0.510
pos_6           -0.279  0.960   -0.475  0.880   -0.955  0.295
pos_7           0.657   0.754   0.452   0.892   -0.422  0.907
pos_8           0.989   -0.146  0.991   0.136   0.389   0.921
pos_9           0.412   -0.911  0.676   -0.737  0.944   0.329

==================================================
测试4: 位置编码的可视化分析
==================================================
可视化数据形状: (50, 64)
生成位置编码热力图...
热力图已保存为 positional_encoding_heatmap.png

==================================================
测试5: 相对位置关系分析
==================================================
位置对之间的编码距离分析:
位置对          欧几里得距离    余弦相似度
(0, 1)          3.714           0.973
(0, 2)          6.967           0.905
(0, 5)          11.524          0.741
(0, 10)         12.823          0.679
(5, 6)          3.714           0.973
(5, 10)         11.524          0.741
(10, 20)                12.823          0.679

==================================================
测试6: 手动验证三角函数编码公式
==================================================
手动计算位置 5 的编码值:
维度    预期值(手动)    实际值(模型)    误差
0(sin)  -0.958924       -0.958924       0.00000002
1(cos)  0.283662        0.283662        0.00000001
2(sin)  -0.993855       -0.993855       0.00000002
3(cos)  0.110692        0.110692        0.00000011
4(sin)  -0.998229       -0.998229       0.00000002
5(cos)  -0.059494       -0.059494       0.00000004
6(sin)  -0.975027       -0.975027       0.00000005
7(cos)  -0.222086       -0.222086       0.00000008

==================================================
测试7: 不同模型维度对位置编码的影响
==================================================
在位置10处，不同d_model的编码前4个维度值:
d_model dim_0           dim_1           dim_2           dim_3
64      -0.5440         -0.8391         0.9376          0.3476
128     -0.5440         -0.8391         0.6926          -0.7213
256     -0.5440         -0.8391         0.1188          -0.9929
512     -0.5440         -0.8391         -0.2200         -0.9755

============================================================
位置编码测试完成!
关键发现:
1. 位置编码为每个位置生成唯一向量
2. 使用sin/cos函数确保平滑的位置关系
3. 不同频率的波形在不同维度上编码不同粒度的位置信息
4. 相邻位置的编码相似，距离远的位置编码差异大
5. 位置编码不需要训练，是固定的数学函数
============================================================

生成特定维度的位置编码曲线图...
维度曲线图已保存为 positional_encoding_curves.png

所有测试完成! 查看生成的图片以更好地理解位置编码的工作原理。
```

simple_pe_demo.py

```
$ python simple_pe_demo.py
============================================================
位置编码(PositionalEncoding)核心概念演示
============================================================
简化参数: d_model=8, max_len=10

完整的位置编码矩阵 (位置 × 维度):
每行代表一个位置的编码向量
pos\dim dim_0   dim_1   dim_2   dim_3   dim_4   dim_5   dim_6   dim_7
pos_0    0.000   1.000   0.000   1.000   0.000   1.000   0.000   1.000
pos_1    0.841   0.540   0.100   0.995   0.010   1.000   0.001   1.000
pos_2    0.909  -0.416   0.199   0.980   0.020   1.000   0.002   1.000
pos_3    0.141  -0.990   0.296   0.955   0.030   1.000   0.003   1.000
pos_4   -0.757  -0.654   0.389   0.921   0.040   0.999   0.004   1.000
pos_5   -0.959   0.284   0.479   0.878   0.050   0.999   0.005   1.000
pos_6   -0.279   0.960   0.565   0.825   0.060   0.998   0.006   1.000
pos_7    0.657   0.754   0.644   0.765   0.070   0.998   0.007   1.000
pos_8    0.989  -0.146   0.717   0.697   0.080   0.997   0.008   1.000
pos_9    0.412  -0.911   0.783   0.622   0.090   0.996   0.009   1.000

==================================================
观察规律:
1. 偶数列(0,2,4,6)是sin函数值
2. 奇数列(1,3,5,7)是cos函数值
3. 从左到右，波动频率越来越快
4. 每一行(位置)都有独特的编码模式
==================================================

手动验证位置编码公式:
公式: PE(pos,2i) = sin(pos/10000^(2i/d_model))
     PE(pos,2i+1) = cos(pos/10000^(2i/d_model))

验证位置 3 的编码:
维度0 (sin): 手动计算=0.141120, 模型输出=0.141120
维度1 (cos): 手动计算=-0.989992, 模型输出=-0.989992
维度2 (sin): 手动计算=0.295520, 模型输出=0.295520
维度3 (cos): 手动计算=0.955336, 模型输出=0.955337
维度4 (sin): 手动计算=0.029996, 模型输出=0.029995
维度5 (cos): 手动计算=0.999550, 模型输出=0.999550
维度6 (sin): 手动计算=0.003000, 模型输出=0.003000
维度7 (cos): 手动计算=0.999996, 模型输出=0.999996

生成简化的波形图...
简化演示图已保存为 simple_pe_demo.png

==================================================
位置编码的实际应用效果分析:
==================================================
假设我们有一个单词，它的embedding是固定的:
单词embedding: [ 0.92126817 -0.7753326  -1.0703268   0.02666496  2.203629   -0.14231408
  0.878323    0.92135483]

但这个单词出现在不同位置时，加上位置编码后的最终表示:
位置0: [ 0.92126817  0.22466737 -1.0703268   1.026665    2.203629    0.8576859
  0.878323    1.9213548 ]
位置2: [ 1.8305656  -1.1914794  -0.8716575   1.0067315   2.2236276   0.85748595
  0.880323    1.9213529 ]
位置5: [-0.03765613 -0.49167043 -0.59090126  0.9042475   2.2536082   0.8564362
  0.883323    1.9213424 ]

观察: 相同的单词在不同位置会有不同的最终表示!
这样Transformer就能区分'我爱你'和'你爱我'中'我'和'你'的不同位置。

============================================================
位置相似性分析
============================================================
以位置10为参考，计算其他位置与它的相似度:
位置    欧氏距离        余弦相似度
0       4.679           0.658
1       4.586           0.671
2       4.379           0.700
3       4.180           0.727
4       4.109           0.736
5       4.122           0.734
6       4.016           0.748
7       3.581           0.800
8       2.719           0.884
9       1.472           0.966
11      1.472           0.966
12      2.719           0.884
13      3.581           0.800
14      4.016           0.748
15      4.122           0.734
16      4.109           0.736
17      4.180           0.727
18      4.379           0.700
19      4.586           0.671

观察规律:
1. 离参考位置10越近的位置，欧氏距离越小，余弦相似度越大
2. 这种渐变的相似性帮助模型理解序列中的位置关系
3. 不会出现跳跃式的突变，保证了位置信息的平滑性

============================================================
总结: 位置编码的核心价值
============================================================
1. 给每个位置分配独特但相关的'身份证'
2. 相近位置有相似的编码，远程位置编码差异较大
3. 使用数学函数(sin/cos)确保编码的规律性和可预测性
4. 不需要训练参数，是纯数学计算
5. 可以处理训练时未见过的更长序列
============================================================
```

## 位置编码测试总结

我们创建了两个测试文件来深入理解位置编码的原理：

### 1. `positional_encoding_test.py` - 完整测试
- **基本性质验证**：确认位置编码不改变序列形状
- **唯一性测试**：验证不同位置有不同的编码
- **周期性分析**：展示sin/cos函数的周期性特征
- **可视化分析**：生成热力图和曲线图
- **相对位置关系**：分析位置间的距离和相似度
- **公式验证**：手动计算验证三角函数公式
- **维度影响**：测试不同模型维度的影响

### 2. `simple_pe_demo.py` - 简化演示
- **核心概念展示**：用8维小模型清晰展示编码矩阵
- **公式验证**：逐步验证数学公式的正确性
- **实际应用效果**：演示同一单词在不同位置的不同表示
- **位置相似性**：分析相邻位置的相似度关系

### 3. 生成的可视化图片
- `positional_encoding_heatmap.png` - 位置编码热力图
- `positional_encoding_curves.png` - 不同维度的曲线图
- `simple_pe_demo.png` - 简化的8维波形图

### 核心发现
1. **唯一性**：每个位置都有唯一的编码向量
2. **平滑性**：相邻位置的编码相似，距离远的差异大
3. **规律性**：使用sin/cos函数确保数学上的规律性
4. **免训练**：位置编码是固定的数学函数，不需要学习
5. **可扩展**：可以处理比训练时更长的序列

这些测试帮助我们理解了位置编码如何为Transformer提供位置信息，使模型能够区分序列中不同位置的相同单词，这是Transformer能够理解语言顺序的关键机制。