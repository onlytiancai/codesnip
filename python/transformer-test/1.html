<div class="available-content">
            <div dir="auto" class="body markup">
                <p>It has been seven years since the original GPT architecture was developed. At first glance, looking
                    back at GPT-2 (2019) and forward to DeepSeek-V3 and Llama 4 (2024-2025), one might be surprised at
                    how structurally similar these models still are.</p>
                <p>Sure, positional embeddings have evolved from absolute to rotational (RoPE), Multi-Head Attention has
                    largely given way to Grouped-Query Attention, and the more efficient SwiGLU has replaced activation
                    functions like GELU. But beneath these minor refinements, have we truly seen groundbreaking changes,
                    or are we simply polishing the same architectural foundations?</p>
                <p>Comparing LLMs to determine the key ingredients that contribute to their good (or not-so-good)
                    performance is notoriously challenging: datasets, training techniques, and hyperparameters vary
                    widely and are often not well documented.</p>
                <p>However, I think that there is still a lot of value in examining the structural changes of the
                    architectures themselves to see what LLM developers are up to in 2025. (A subset of them are shown
                    in Figure 1 below.)</p>
                <p></p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!iCn-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!iCn-!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 424w, https://substackcdn.com/image/fetch/$s_!iCn-!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 848w, https://substackcdn.com/image/fetch/$s_!iCn-!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 1272w, https://substackcdn.com/image/fetch/$s_!iCn-!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!iCn-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png" width="1456" height="1016" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1016,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1563062,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!iCn-!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 424w, https://substackcdn.com/image/fetch/$s_!iCn-!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 848w, https://substackcdn.com/image/fetch/$s_!iCn-!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 1272w, https://substackcdn.com/image/fetch/$s_!iCn-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 1456w" sizes="100vw" fetchpriority="high" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption">Figure 1: A subset of the architectures covered in this
                            article.</figcaption>
                    </figure>
                </div>
                <p></p>
                <p>So, in this article, rather than writing about benchmark performance or training algorithms, I will
                    focus on the architectural developments that define today's flagship open models.</p>
                <p><span>(As you may remember, </span><a href="https://magazine.sebastianraschka.com/p/understanding-multimodal-llms" rel="">I wrote
                        about multimodal LLMs</a><span> not too long ago; in this article, I will focus on the text
                        capabilities of recent models and leave the discussion of multimodal capabilities for another
                        time.)</span></p>
                <p><strong>Tip:</strong><span> This is a fairly comprehensive article, so I recommend using the
                        navigation bar to access the table of contents (just hover over the left side of the Substack
                        page).</span></p>
                <h1 class="header-anchor-post">1. DeepSeek V3/R1<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§deepseek-vr" class="pencraft pc-reset header-anchor offset-top"></div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/deepseek-vr" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h1>
                <p><span>As you have probably heard more than once by now, </span><a href="https://arxiv.org/abs/2501.12948" rel="">DeepSeek R1</a><span> made a big impact when it
                        was released in January 2025. DeepSeek R1 is a reasoning model built on top of the </span><a href="https://arxiv.org/abs/2412.19437" rel="">DeepSeek V3 architecture</a><span>, which was
                        introduced in December 2024.</span></p>
                <p>While my focus here is on architectures released in 2025, I think it’s reasonable to include DeepSeek
                    V3, since it only gained widespread attention and adoption following the launch of DeepSeek R1 in
                    2025.</p>
                <p>If you are interested in the training of DeepSeek R1 specifically, you may also find my article from
                    earlier this year useful: </p>
                <div data-component-name="DigestPostEmbed" class="digestPostEmbed-flwiST"><a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms" rel="noopener" target="_blank">
                        </a><div class="pencraft pc-display-flex pc-gap-16 pc-reset"><a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms" rel="noopener" target="_blank">
                            <div class="pencraft pc-reset" style="width: 70px; height: 70px;">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!QwUc!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png">
                                    <img src="https://substackcdn.com/image/fetch/$s_!QwUc!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png" sizes="100vw" alt="Understanding Reasoning LLMs" width="140" height="140" class="img-OACg1c smSquare-NGbPBa pencraft pc-reset">
                                </picture>
                            </div>
                            </a><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset"><a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms" rel="noopener" target="_blank">
                                <h4 class="pencraft pc-reset color-pub-primary-text-NyXPlw line-height-24-jnGwiv font-display-nhmvtD size-20-P_cSRT weight-bold-DmI9lw reset-IxiVJZ">
                                    Understanding Reasoning LLMs</h4>
                                </a><div class="pencraft pc-display-flex pc-gap-4 pc-alignItems-center pc-reset"><a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms" rel="noopener" target="_blank">
                                    </a><div class="pencraft pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA"><a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms" rel="noopener" target="_blank">
                                        </a><a href="https://substack.com/profile/27393275-sebastian-raschka-phd" class="inheritColor-WetTGJ">Sebastian Raschka, PhD</a></div>
                                    <div class="pencraft pc-reset color-pub-secondary-text-hGQ02T reset-IxiVJZ">·</div>
                                    <div class="pencraft pc-reset color-pub-secondary-text-hGQ02T line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA">
                                        Feb 5</div>
                                </div>
                                <div class="pencraft pc-display-flex pc-gap-16 pc-paddingTop-0 pc-paddingBottom-0 pc-alignItems-center pc-reset">
                                    <a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms" class="pencraft pc-reset align-center-y7ZD4w line-height-20-t4M0El font-text-qe4AeH size-13-hZTUKr weight-medium-fw81nC reset-IxiVJZ">
                                        <div class="pencraft pc-display-flex pc-gap-8 pc-alignItems-center pc-reset link-HREYZo">
                                            <span class="pencraft pc-reset color-accent-BVX_7M line-height-20-t4M0El font-text-qe4AeH size-14-MLPa7j weight-semibold-uqA4FV reset-IxiVJZ">Read
                                                full story</span><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-right">
                                                <path d="M5 12h14"></path>
                                                <path d="m12 5 7 7-7 7"></path>
                                            </svg></div>
                                    </a></div>
                            </div>
                        </div>
                    </div>
                <p>In this section, I’ll focus on two key architectural techniques introduced in DeepSeek V3 that
                    improved its computational efficiency and distinguish it from many other LLMs:</p>
                <ul>
                    <li>
                        <p>Multi-Head Latent Attention (MLA)</p>
                    </li>
                    <li>
                        <p>Mixture-of-Experts (MoE)</p>
                    </li>
                </ul>
                <h2 class="header-anchor-post"><strong>1.1 Multi-Head Latent Attention (MLA)</strong>
                    <div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§multi-head-latent-attention-mla" class="pencraft pc-reset header-anchor offset-top"></div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/multi-head-latent-attention-mla" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h2>
                <p>Before discussing Multi-Head Latent Attention (MLA), let's briefly go over some background to
                    motivate why it's used. For that, let's start with Grouped-Query Attention (GQA), which has become
                    the new standard replacement for a more compute- and parameter-efficient alternative to Multi-Head
                    Attention (MHA) in recent years.</p>
                <p>So, here's a brief GQA summary. Unlike MHA, where each head also has its own set of keys and values,
                    to reduce memory usage, GQA groups multiple heads to share the same key and value projections. </p>
                <p>For example, as further illustrated in Figure 2 below, if there are 2 key-value groups and 4
                    attention heads, then heads 1 and 2 might share one set of keys and values, while heads 3 and 4
                    share another. This reduces the total number of key and value computations, which leads to lower
                    memory usage and improved efficiency (without noticeably affecting the modeling performance,
                    according to ablation studies).</p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!uVhV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!uVhV!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 424w, https://substackcdn.com/image/fetch/$s_!uVhV!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 848w, https://substackcdn.com/image/fetch/$s_!uVhV!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 1272w, https://substackcdn.com/image/fetch/$s_!uVhV!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!uVhV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png" width="1023" height="474" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:474,&quot;width&quot;:1023,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!uVhV!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 424w, https://substackcdn.com/image/fetch/$s_!uVhV!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 848w, https://substackcdn.com/image/fetch/$s_!uVhV!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 1272w, https://substackcdn.com/image/fetch/$s_!uVhV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption"><em>Figure 2: A comparison between MHA and GQA. Here, the
                                group size is 2, where a key and value pair is shared among 2 queries.</em></figcaption>
                    </figure>
                </div>
                <p></p>
                <p>So, the core idea behind GQA is to reduce the number of key and value heads by sharing them across
                    multiple query heads. This (1) lowers the model's parameter count and (2) reduces the memory
                    bandwidth usage for key and value tensors during inference since fewer keys and values need to be
                    stored and retrieved from the KV cache.</p>
                <p><span>(If you are curious how GQA looks in code, see my</span><a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/converting-llama2-to-llama3.ipynb" rel=""> GPT-2 to Llama 3 conversion guide</a><span> for a version without KV cache and my
                        KV-cache variant </span><a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/pkg/llms_from_scratch/llama3.py" rel="">here</a><span>.)</span></p>
                <p><span>While GQA is mainly a computational-efficiency workaround for MHA, ablation studies (such as
                        those in the</span><a href="https://arxiv.org/abs/2305.13245" rel=""> original GQA
                        paper</a><span> and the </span><a href="https://arxiv.org/abs/2307.09288" rel="">Llama 2
                        paper</a><span>) show it performs comparably to standard MHA in terms of LLM modeling
                        performance.</span></p>
                <p>Now, Multi-Head Latent Attention (MLA) offers a different memory-saving strategy that also pairs
                    particularly well with KV caching. Instead of sharing key and value heads like GQA, MLA compresses
                    the key and value tensors into a lower-dimensional space before storing them in the KV cache. </p>
                <p>At inference time, these compressed tensors are projected back to their original size before being
                    used, as shown in the Figure 3 below. This adds an extra matrix multiplication but reduces memory
                    usage.</p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!jagJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!jagJ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 424w, https://substackcdn.com/image/fetch/$s_!jagJ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 848w, https://substackcdn.com/image/fetch/$s_!jagJ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 1272w, https://substackcdn.com/image/fetch/$s_!jagJ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!jagJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png" width="1456" height="806" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/eb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:806,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!jagJ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 424w, https://substackcdn.com/image/fetch/$s_!jagJ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 848w, https://substackcdn.com/image/fetch/$s_!jagJ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 1272w, https://substackcdn.com/image/fetch/$s_!jagJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption"><em>Figure 3: Comparison between MLA (used in DeepSeek V3 and
                                R1) and regular MHA.</em></figcaption>
                    </figure>
                </div>
                <p></p>
                <p>(As a side note, the queries are also compressed, but only during training, not inference.)</p>
                <p><span>By the way, MLA is not new in DeepSeek V3, as its </span><a href="https://arxiv.org/abs/2405.04434" rel="">DeepSeek-V2 predecessor</a><span> also used (and
                        even introduced) it. Also, the V2 paper contains a few interesting ablation studies that may
                        explain why the DeepSeek team chose MLA over GQA (see Figure 4 below).</span></p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!efDX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!efDX!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 424w, https://substackcdn.com/image/fetch/$s_!efDX!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 848w, https://substackcdn.com/image/fetch/$s_!efDX!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 1272w, https://substackcdn.com/image/fetch/$s_!efDX!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!efDX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png" width="644" height="610.4806201550388" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:856,&quot;width&quot;:903,&quot;resizeWidth&quot;:644,&quot;bytes&quot;:288103,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!efDX!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 424w, https://substackcdn.com/image/fetch/$s_!efDX!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 848w, https://substackcdn.com/image/fetch/$s_!efDX!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 1272w, https://substackcdn.com/image/fetch/$s_!efDX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption">Figure 4: Annotated tables from the DeepSeek-V2 paper,
                            https://arxiv.org/abs/2405.04434</figcaption>
                    </figure>
                </div>
                <p></p>
                <p></p>
                <p>As shown in Figure 4 above, GQA appears to perform worse than MHA, whereas MLA offers better modeling
                    performance than MHA, which is likely why the DeepSeek team chose MLA over GQA. (It would have been
                    interesting to see the "KV Cache per Token" savings comparison between MLA and GQA as well!)</p>
                <p>To summarize this section before we move on to the next architecture component, MLA is a clever trick
                    to reduce KV cache memory use while even slightly outperforming MHA in terms of modeling
                    performance.</p>
                <h2 class="header-anchor-post"><strong>1.2 Mixture-of-Experts (MoE)</strong>
                    <div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§mixture-of-experts-moe" class="pencraft pc-reset header-anchor offset-top"></div>
                            <button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/mixture-of-experts-moe" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h2>
                <p>The other major architectural component in DeepSeek worth highlighting is its use of
                    Mixture-of-Experts (MoE) layers. While DeepSeek did not invent MoE, it has seen a resurgence this
                    year, and many of the architectures we will cover later also adopt it.</p>
                <p>You are likely already familiar with MoE, but a quick recap may be helpful.</p>
                <p>The core idea in MoE is to replace each FeedForward module in a transformer block with multiple
                    expert layers, where each of these expert layers is also a FeedForward module. This means that we
                    swap a single FeedForward block for multiple FeedForward blocks, as illustrated in the Figure 5
                    below.</p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!e3O4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!e3O4!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png 424w, https://substackcdn.com/image/fetch/$s_!e3O4!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png 848w, https://substackcdn.com/image/fetch/$s_!e3O4!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png 1272w, https://substackcdn.com/image/fetch/$s_!e3O4!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!e3O4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png" width="1456" height="918" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:918,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!e3O4!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png 424w, https://substackcdn.com/image/fetch/$s_!e3O4!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png 848w, https://substackcdn.com/image/fetch/$s_!e3O4!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png 1272w, https://substackcdn.com/image/fetch/$s_!e3O4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption"><em>Figure 5: An illustration of the Mixture-of-Experts (MoE)
                                module in DeepSeek V3/R1 (right) compared to an LLM with a standard FeedForward block
                                (left).</em></figcaption>
                    </figure>
                </div>
                <p></p>
                <p>The FeedForward block inside a transformer block (shown as the dark gray block in the figure above)
                    typically contains a large number of the model's total parameters. (Note that the transformer block,
                    and thereby the FeedForward block, is repeated many times in an LLM; in the case of DeepSeek-V3, 61
                    times.)</p>
                <p><span>So, replacing </span><em>a single</em><span> FeedForward block with
                    </span><em>multiple</em><span> FeedForward blocks (as done in a MoE setup) substantially increases
                        the model's total parameter count. However, the key trick is that we don't use ("activate") all
                        experts for every token. Instead, a router selects only a small subset of experts per token. (In
                        the interest of time, or rather article space, I'll cover the router in more detail another
                        time.)</span></p>
                <p><span>Because only a few experts are active at a time, MoE modules are often referred to as
                    </span><em>sparse</em><span>, in contrast to </span><em>dense</em><span> modules that always use the
                        full parameter set. However, the large total number of parameters via an MoE increases the
                        capacity of the LLM, which means it can take up more knowledge during training. The sparsity
                        keeps inference efficient, though, as we don't use all the parameters at the same time.</span>
                </p>
                <p>For example, DeepSeek-V3 has 256 experts per MoE module and a total of 671 billion parameters. Yet
                    during inference, only 9 experts are active at a time (1 shared expert plus 8 selected by the
                    router). This means just 37 billion parameters are used per inference step as opposed to all 671
                    billion.</p>
                <p><span>One notable feature of DeepSeek-V3's MoE design is the use of a shared expert. This is an
                        expert that is always active for every token. This idea is not new and was already introduced in
                        the </span><a href="https://arxiv.org/abs/2401.06066" rel="">DeepSeek 2024 MoE</a><span> and
                    </span><a href="https://arxiv.org/abs/2201.05596" rel="">2022 DeepSpeedMoE paper</a><span>s. </span>
                </p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!i4ms!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!i4ms!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 424w, https://substackcdn.com/image/fetch/$s_!i4ms!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 848w, https://substackcdn.com/image/fetch/$s_!i4ms!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 1272w, https://substackcdn.com/image/fetch/$s_!i4ms!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!i4ms!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png" width="1039" height="569" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:569,&quot;width&quot;:1039,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!i4ms!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 424w, https://substackcdn.com/image/fetch/$s_!i4ms!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 848w, https://substackcdn.com/image/fetch/$s_!i4ms!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 1272w, https://substackcdn.com/image/fetch/$s_!i4ms!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption"><em>Figure 6: An annotated figure from "DeepSeekMoE: Towards
                                Ultimate Expert Specialization in Mixture-of-Experts Language Models",
                                https://arxiv.org/abs/2401.06066</em></figcaption>
                    </figure>
                </div>
                <p><span>The benefit of having a shared expert was first noted in the </span><a href="https://arxiv.org/abs/2201.05596" rel="">DeepSpeedMoE paper</a><span>, where they found
                        that it boosts overall modeling performance compared to no shared experts. This is likely
                        because common or repeated patterns don't have to be learned by multiple individual experts,
                        which leaves them with more room for learning more specialized patterns.</span></p>
                <h2 class="header-anchor-post"><strong>1.3 DeepSeek Summary</strong>
                    <div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§deepseek-summary" class="pencraft pc-reset header-anchor offset-top"></div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/deepseek-summary" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h2>
                <p>To summarize, DeepSeek-V3 is a massive 671-billion-parameter model that, at launch, outperformed
                    other open-weight models, including the 405B Llama 3. Despite being larger, it is much more
                    efficient at inference time thanks to its Mixture-of-Experts (MoE) architecture, which activates
                    only a small subset of (just 37B) parameters per token.</p>
                <p>Another key distinguishing feature is DeepSeek-V3's use of Multi-Head Latent Attention (MLA) instead
                    of Grouped-Query Attention (GQA). Both MLA and GQA are inference-efficient alternatives to standard
                    Multi-Head Attention (MHA), particularly when using KV caching. While MLA is more complex to
                    implement, a study in the DeepSeek-V2 paper has shown it delivers better modeling performance than
                    GQA.</p>
                <h1 class="header-anchor-post">2. OLMo 2<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§olmo" class="pencraft pc-reset header-anchor offset-top"></div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/olmo" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h1>
                <p>The OLMo series of models by the non-profit Allen Institute for AI is noteworthy due to its
                    transparency in terms of training data and code, as well as the relatively detailed technical
                    reports.</p>
                <p>While you probably won’t find OLMo models at the top of any benchmark or leaderboard, they are pretty
                    clean and, more importantly, a great blueprint for developing LLMs, thanks to their transparency.
                </p>
                <p><span>And while OLMo models are popular because of their transparency, they are not that bad either.
                        In fact, at the time of release in January (before Llama 4, Gemma 3, and Qwen 3), </span><a href="https://arxiv.org/abs/2501.00656" rel="">OLMo 2</a><span> models were sitting at the
                        Pareto frontier of compute to performance, as shown in Figure 7 below.</span></p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!7DYj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!7DYj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 424w, https://substackcdn.com/image/fetch/$s_!7DYj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 848w, https://substackcdn.com/image/fetch/$s_!7DYj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 1272w, https://substackcdn.com/image/fetch/$s_!7DYj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!7DYj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png" width="666" height="533.7078870496592" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:823,&quot;width&quot;:1027,&quot;resizeWidth&quot;:666,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!7DYj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 424w, https://substackcdn.com/image/fetch/$s_!7DYj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 848w, https://substackcdn.com/image/fetch/$s_!7DYj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 1272w, https://substackcdn.com/image/fetch/$s_!7DYj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption">Figure 7: Modeling benchmark performance (higher is better) vs
                            pre-training cost (FLOPs; lower is better) for different LLMs. This is an annotated figure
                            from the OLMo 2 paper, https://arxiv.org/abs/2501.00656</figcaption>
                    </figure>
                </div>
                <p></p>
                <p>As mentioned earlier in this article, I aim to focus only on the LLM architecture details (not
                    training or data) to keep it at a manageable length. So, what were the interesting architectural
                    design choices in OLMo2 ? It mainly comes down to normalizations: the placement of RMSNorm layers as
                    well as the addition of a QK-norm, which I will discuss below.</p>
                <p>Another thing worth mentioning is that OLMo 2 still uses traditional Multi-Head Attention (MHA)
                    instead of MLA or GQA.</p>
                <h2 class="header-anchor-post"><strong>2.1 Normalization Layer Placement</strong>
                    <div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§normalization-layer-placement" class="pencraft pc-reset header-anchor offset-top">
                            </div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/normalization-layer-placement" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h2>
                <p>Overall, OLMo 2 largely follows the architecture of the original GPT model, similar to other
                    contemporary LLMs. However, there are some noteworthy deviations. Let's start with the normalization
                    layers.</p>
                <p>Similar to Llama, Gemma, and most other LLMs, OLMo 2 switched from LayerNorm to RMSNorm.</p>
                <p><span>But since RMSNorm is old hat (it's basically a simplified version of LayerNorm with fewer
                        trainable parameters), I will skip the discussion of RMSNorm vs LayerNorm. (Curious readers can
                        find an RMSNorm code implementation in my </span><a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/converting-gpt-to-llama2.ipynb" rel="">GPT-2 to Llama conversion guide</a><span>.)</span></p>
                <p><span>However, it's worth discussing the placement of the RMSNorm layer. The original transformer
                        (from the "</span><a href="https://arxiv.org/abs/1706.03762" rel="">Attention is all you
                        need</a><span>" paper) placed the two normalization layers in the transformer block
                    </span><em>after</em><strong> </strong><span>the attention module and the FeedForward module,
                        respectively.</span></p>
                <p>This is also known as Post-LN or Post-Norm.</p>
                <p><span>GPT and most other LLMs that came after placed the normalization layers
                    </span><em>before</em><span> the attention and FeedForward modules, which is known as Pre-LN or
                        Pre-Norm. A comparison between Post- and Pre-Norm is shown in the figure below.</span></p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!wYj9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!wYj9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 424w, https://substackcdn.com/image/fetch/$s_!wYj9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 848w, https://substackcdn.com/image/fetch/$s_!wYj9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 1272w, https://substackcdn.com/image/fetch/$s_!wYj9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!wYj9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png" width="1444" height="789" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:789,&quot;width&quot;:1444,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!wYj9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 424w, https://substackcdn.com/image/fetch/$s_!wYj9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 848w, https://substackcdn.com/image/fetch/$s_!wYj9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 1272w, https://substackcdn.com/image/fetch/$s_!wYj9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption"><em>Figure 8: A comparison of Post-Norm, Pre-Norm, and OLMo
                                2's flavor of Post-Norm.</em></figcaption>
                    </figure>
                </div>
                <p></p>
                <p><span>In </span><a href="https://arxiv.org/abs/2002.04745" rel="">2020, Xiong et al.</a><span> showed
                        that Pre-LN results in more well-behaved gradients at initialization. Furthermore, the
                        researchers mentioned that Pre-LN even works well without careful learning rate warm-up, which
                        is otherwise a crucial tool for Post-LN.</span></p>
                <p><span>Now, the reason I am mentioning that is that OLMo 2 adopted a form of Post-LN (but with RMSNorm
                        instead of LayerNorm, so I am calling it </span><em>Post-Norm</em><span>).</span></p>
                <p>In OLMo 2, instead of placing the normalization layers before the attention and FeedForward layers,
                    they place them after, as shown in the figure above. However, notice that in contrast to the
                    original transformer architecture, the normalization layers are still inside the residual layers
                    (skip connections).</p>
                <p><span>So, why did they move the position of the normalization layers?</span><strong>
                    </strong><span>The reason is that it helped with training stability, as shown in the figure
                        below.</span></p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ebW0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ebW0!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 424w, https://substackcdn.com/image/fetch/$s_!ebW0!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 848w, https://substackcdn.com/image/fetch/$s_!ebW0!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 1272w, https://substackcdn.com/image/fetch/$s_!ebW0!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ebW0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png" width="1289" height="407" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:407,&quot;width&quot;:1289,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ebW0!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 424w, https://substackcdn.com/image/fetch/$s_!ebW0!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 848w, https://substackcdn.com/image/fetch/$s_!ebW0!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 1272w, https://substackcdn.com/image/fetch/$s_!ebW0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption"><em>Figure 9: A plot showing the training stability for
                                Pre-Norm (like in GPT-2, Llama 3, and many others) versus OLMo 2's flavor of Post-Norm.
                                This is an annotated figure from the OLMo 2 paper, https://arxiv.org/abs/2501.00656</em>
                        </figcaption>
                    </figure>
                </div>
                <p></p>
                <p>Unfortunately this figure shows the results of the reordering together with QK-Norm, which is a
                    separate concept. So, it’s hard to tell how much the normalization layer reordering contributed by
                    itself.</p>
                <h2 class="header-anchor-post"><strong>2.2 QK-Norm</strong>
                    <div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§qk-norm" class="pencraft pc-reset header-anchor offset-top"></div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/qk-norm" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h2>
                <p>Since the previous section already mentioned the QK-norm, and other LLMs we discuss later, such as
                    Gemma 2 and Gemma 3, also use QK-norm, let's briefly discuss what this is.</p>
                <p><span>QK-Norm is essentially yet another RMSNorm layer. It's placed inside the Multi-Head Attention
                        (MHA) module and applied to the queries (q) and keys (k) before applying RoPE. To illustrate
                        this, below is an excerpt of a Grouped-Query Attention (GQA) layer I wrote for my </span><a href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/11_qwen3" rel="">Qwen3
                        from-scratch implementation</a><span> (the QK-norm application in GQA is similar to MHA in
                        OLMo):</span></p>
                <pre><code>class GroupedQueryAttention(nn.Module):
    def __init__(
        self, d_in, num_heads, num_kv_groups,
        head_dim=None, qk_norm=False, dtype=None
    ):
        # ...

        if qk_norm:
            self.q_norm = RMSNorm(head_dim, eps=1e-6)
            self.k_norm = RMSNorm(head_dim, eps=1e-6)
        else:
            self.q_norm = self.k_norm = None

    def forward(self, x, mask, cos, sin):
        b, num_tokens, _ = x.shape

        # Apply projections
        queries = self.W_query(x) 
        keys = self.W_key(x)
        values = self.W_value(x) 

        # ...

        # Optional normalization
        if self.q_norm:
            queries = self.q_norm(queries)
        if self.k_norm:
            keys = self.k_norm(keys)

        # Apply RoPE
        queries = apply_rope(queries, cos, sin)
        keys = apply_rope(keys, cos, sin)

        # Expand K and V to match number of heads
        keys = keys.repeat_interleave(self.group_size, dim=1)
        values = values.repeat_interleave(self.group_size, dim=1)

        # Attention
        attn_scores = queries @ keys.transpose(2, 3)
        # ...
</code></pre>
                <p><span>As mentioned earlier, together with Post-Norm, QK-Norm stabilizes the training. Note that
                        QK-Norm was not invented by OLMo 2 but goes back to the </span><a href="https://arxiv.org/abs/2302.05442" rel="">2023 Scaling Vision Transformers
                        paper</a><span>.</span></p>
                <h2 class="header-anchor-post"><strong>2.3 OLMo 2 Summary</strong>
                    <div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§olmo-summary" class="pencraft pc-reset header-anchor offset-top"></div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/olmo-summary" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h2>
                <p>In short, the noteworthy OLMo 2 architecture design decisions are primarily the RMSNorm placements:
                    RMSNorm after instead of before the attention and FeedForward modules (a flavor of Post-Norm), as
                    well as the addition of RMSNorm for the queries and keys inside the attention mechanism (QK-Norm),
                    which both, together, help stabilize the training loss.</p>
                <p><span>Below is a figure that further compares OLMo 2 to Llama 3 side by side; as one can see, the
                        architectures are otherwise relatively similar except for the fact that OLMo 2 still uses the
                        traditional MHA instead of GQA. (However, the </span><a href="https://huggingface.co/allenai/OLMo-2-0325-32B-Instruct" rel="">OLMo 2 team released a 32B
                        variant</a><span> 3 months later that uses GQA.)</span></p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!S6Y9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!S6Y9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 424w, https://substackcdn.com/image/fetch/$s_!S6Y9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 848w, https://substackcdn.com/image/fetch/$s_!S6Y9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 1272w, https://substackcdn.com/image/fetch/$s_!S6Y9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!S6Y9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png" width="1329" height="737" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:737,&quot;width&quot;:1329,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:153520,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!S6Y9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 424w, https://substackcdn.com/image/fetch/$s_!S6Y9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 848w, https://substackcdn.com/image/fetch/$s_!S6Y9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 1272w, https://substackcdn.com/image/fetch/$s_!S6Y9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption">Figure 10: An architecture comparison between Llama 3 and OLMo
                            2.</figcaption>
                    </figure>
                </div>
                <p></p>
                <p></p>
                <p></p>
                <h1 class="header-anchor-post">3. Gemma 3<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§gemma" class="pencraft pc-reset header-anchor offset-top"></div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/gemma" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h1>
                <p>Google's Gemma models have always been really good, and I think they have always been a bit
                    underhyped compared to other popular models, like the Llama series. </p>
                <p>One of the distinguishing aspects of Gemma is the rather large vocabulary size (to support multiple
                    languages better), and the stronger focus on the 27B size (versus 8B or 70B). But note that Gemma 2
                    also comes in smaller sizes: 1B, 4B, and 12B.</p>
                <p>The 27B size hits a really nice sweet spot: it's much more capable than an 8B model but not as
                    resource-intensive as a 70B model, and it runs just fine locally on my Mac Mini.</p>
                <p><span>So, what else is interesting in </span><a href="https://arxiv.org/abs/2503.19786" rel="">Gemma
                        3</a><span>? As discussed earlier, other models like Deepseek-V3/R1 use a Mixture-of-Experts
                        (MoE) architecture to reduce memory requirements at inference, given a fixed model size. (The
                        MoE approach is also used by several other models we will discuss later.)</span></p>
                <p>Gemma 3 uses a different "trick" to reduce computational costs, namely sliding window attention.</p>
                <h2 class="header-anchor-post"><strong>3.1 Sliding Window Attention</strong>
                    <div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§sliding-window-attention" class="pencraft pc-reset header-anchor offset-top">
                            </div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/sliding-window-attention" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h2>
                <p><span>With sliding window attention (originally introduced in the </span><a href="https://arxiv.org/abs/2004.05150" rel="">LongFormer paper in 2020</a><span> and also
                        already used by </span><a href="http://arxiv.org/abs/2408.00118" rel="">Gemma 2</a><span>), the
                        Gemma 3 team was able to reduce the memory requirements in the KV cache by a substantial amount,
                        as shown in the figure below.</span></p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!LQA4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!LQA4!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 424w, https://substackcdn.com/image/fetch/$s_!LQA4!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 848w, https://substackcdn.com/image/fetch/$s_!LQA4!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 1272w, https://substackcdn.com/image/fetch/$s_!LQA4!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!LQA4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png" width="555" height="252.04511278195488" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:302,&quot;width&quot;:665,&quot;resizeWidth&quot;:555,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!LQA4!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 424w, https://substackcdn.com/image/fetch/$s_!LQA4!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 848w, https://substackcdn.com/image/fetch/$s_!LQA4!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 1272w, https://substackcdn.com/image/fetch/$s_!LQA4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption"><em>Figure 11: An annotated figure from Gemma 3 paper
                                (https://arxiv.org/abs/2503.19786) showing the KV cache memory savings via sliding
                                window attention.</em></figcaption>
                    </figure>
                </div>
                <p></p>
                <p><span>So, what is sliding window attention? If we think of regular self-attention as a
                    </span><em>global</em><span> attention mechanism, since each sequence element can access every other
                        sequence element, then we can think of sliding window attention as </span><em>local</em><span>
                        attention, because here we restrict the context size around the current query position. This is
                        illustrated in the figure below.</span></p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!tTJ5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!tTJ5!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 424w, https://substackcdn.com/image/fetch/$s_!tTJ5!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 848w, https://substackcdn.com/image/fetch/$s_!tTJ5!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 1272w, https://substackcdn.com/image/fetch/$s_!tTJ5!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!tTJ5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png" width="1456" height="721" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:721,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!tTJ5!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 424w, https://substackcdn.com/image/fetch/$s_!tTJ5!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 848w, https://substackcdn.com/image/fetch/$s_!tTJ5!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 1272w, https://substackcdn.com/image/fetch/$s_!tTJ5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption"><em>Figure 12: A comparison between regular attention (left)
                                and sliding window attention (right).</em></figcaption>
                    </figure>
                </div>
                <p></p>
                <p>Please note that sliding window attention can be used with both Multi-Head Attention and
                    Grouped-Query Attention; Gemma 3 uses grouped-query attention.</p>
                <p><span>As mentioned above, sliding window attention is also referred to as </span><em>local</em><span>
                        attention because the local window surrounds and moves with the current query position. In
                        contrast, regular attention is </span><em>global</em><span> as each token can access all other
                        tokens.</span></p>
                <p>Now, as briefly mentioned above, the Gemma 2 predecessor architecture also used sliding window
                    attention before. The difference in Gemma 3 is that they adjusted the ratio between global (regular)
                    and local (sliding) attention.</p>
                <p>For instance, Gemma 2 uses a hybrid attention mechanism that combines sliding window (local) and
                    global attention in a 1:1 ratio. Each token can attend to a 4k-token window of nearby context.</p>
                <p>Where Gemma 2 used sliding window attention in every other layer, Gemma 3 now has a 5:1 ratio,
                    meaning there's only 1 full attention layer for every 5 sliding windows (local) attention layers;
                    moreover, the sliding window size was reduced from 4096 (Gemma 2) to just 1024 (Gemma 3). This
                    shifts the model's focus towards more efficient, localized computations.</p>
                <p>According to their ablation study, the use of sliding window attention has minimal impact on modeling
                    performance, as shown in the figure below.</p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!YSZb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!YSZb!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 424w, https://substackcdn.com/image/fetch/$s_!YSZb!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 848w, https://substackcdn.com/image/fetch/$s_!YSZb!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 1272w, https://substackcdn.com/image/fetch/$s_!YSZb!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!YSZb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png" width="1456" height="434" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:434,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!YSZb!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 424w, https://substackcdn.com/image/fetch/$s_!YSZb!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 848w, https://substackcdn.com/image/fetch/$s_!YSZb!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 1272w, https://substackcdn.com/image/fetch/$s_!YSZb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption">Figure 13: An annotated figure from Gemma 3 paper
                            (https://arxiv.org/abs/2503.19786) showing that sliding window attention has little to no
                            impact on the LLM-generated output perplexity.</figcaption>
                    </figure>
                </div>
                <p></p>
                <p>While sliding window attention is the most notable architecture aspect of Gemma 3, I want to also
                    briefly go over the placement of the normalization layers as a follow-up to the previous OLMo 2
                    section.</p>
                <h2 class="header-anchor-post"><strong>3.2 Normalization Layer Placement in Gemma 3</strong>
                    <div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§normalization-layer-placement-in-gemma" class="pencraft pc-reset header-anchor offset-top"></div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/normalization-layer-placement-in-gemma" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h2>
                <p>A small but interesting tidbit to highlight is that Gemma 3 uses RMSNorm in both a Pre-Norm and
                    Post-Norm setting around its grouped-query attention module.</p>
                <p>This is similar to Gemma 2 but still worth highlighting, as it differs from (1) the Post-Norm used in
                    the original transformer (“Attention is all you need”), (2) the Pre-Norm, which was popularized by
                    GPT-2 and used in many other architectures afterwards, and (3) the Post-Norm flavor in OLMo 2 that
                    we saw earlier.</p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!A1BM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!A1BM!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 424w, https://substackcdn.com/image/fetch/$s_!A1BM!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 848w, https://substackcdn.com/image/fetch/$s_!A1BM!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 1272w, https://substackcdn.com/image/fetch/$s_!A1BM!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!A1BM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png" width="1068" height="855" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:855,&quot;width&quot;:1068,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!A1BM!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 424w, https://substackcdn.com/image/fetch/$s_!A1BM!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 848w, https://substackcdn.com/image/fetch/$s_!A1BM!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 1272w, https://substackcdn.com/image/fetch/$s_!A1BM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption"><em>Figure 14: An architecture comparison between OLMo2 and
                                Gemma 3; note the additional normalization layers in Gemma 3.</em></figcaption>
                    </figure>
                </div>
                <p></p>
                <p>I think this normalization layer placement is a relatively intuitive approach as it gets the best of
                    both worlds: Pre-Norm and Post-Norm. In my opinion, a bit of extra normalization can't hurt. In the
                    worst case, if the extra normalization is redundant, this adds a bit of inefficiency through
                    redundancy. In practice, since RMSNorm is relatively cheap in the grand scheme of things, this
                    shouldn't have any noticeable impact, though.</p>
                <h2 class="header-anchor-post"><strong>3.3 Gemma 3 Summary</strong>
                    <div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§gemma-summary" class="pencraft pc-reset header-anchor offset-top"></div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/gemma-summary" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h2>
                <p>Gemma 3 is a well-performing open-weight LLM that, in my opinion, is a bit underappreciated in the
                    open-source circles. The most interesting part is the use of sliding window attention to improve
                    efficiency (it will be interesting to combine it with MoE in the future).</p>
                <p>Also, Gemma 3 has a unique normalization layer placement, placing RMSNorm layers both before and
                    after the attention and FeedForward modules.</p>
                <h2 class="header-anchor-post"><strong>3.4 Bonus: Gemma 3n</strong>
                    <div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§bonus-gemma-n" class="pencraft pc-reset header-anchor offset-top"></div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/bonus-gemma-n" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h2>
                <p><span>A few months after the Gemma 3 release, Google shared </span><a href="https://developers.googleblog.com/en/introducing-gemma-3n/" rel="">Gemma 3n</a><span>,
                        which is a Gemma 3 model that has been optimized for small-device efficiency with the goal of
                        running on phones.</span></p>
                <p>One of the changes in Gemma 3n to achieve better efficiency is the so-called Per-Layer Embedding
                    (PLE) parameters layer. The key idea here is to keep only a subset of the model's parameters in GPU
                    memory. Token-layer specific embeddings, such as those for text, audio, and vision modalities, are
                    then streamed from the CPU or SSD on demand.</p>
                <p>The figure below illustrates the PLE memory savings, listing 5.44 billion parameters for a standard
                    Gemma 3 model. This likely refers to the Gemma 3 4-billion variant.</p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Su7d!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Su7d!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 424w, https://substackcdn.com/image/fetch/$s_!Su7d!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 848w, https://substackcdn.com/image/fetch/$s_!Su7d!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 1272w, https://substackcdn.com/image/fetch/$s_!Su7d!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Su7d!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png" width="606" height="442.1419939577039" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b05999d6-88ca-4739-8b0b-266b48da288b_662x483.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:483,&quot;width&quot;:662,&quot;resizeWidth&quot;:606,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Su7d!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 424w, https://substackcdn.com/image/fetch/$s_!Su7d!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 848w, https://substackcdn.com/image/fetch/$s_!Su7d!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 1272w, https://substackcdn.com/image/fetch/$s_!Su7d!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption">Figure 15: An annotated figure from Google's Gemma 3n blog
                            (https://developers.googleblog.com/en/introducing-gemma-3n/) illustrating the PLE memory
                            savings.</figcaption>
                    </figure>
                </div>
                <p>The 5.44 vs. 4 billion parameter discrepancy is because Google has an interesting way of reporting
                    parameter counts in LLMs. They often exclude embedding parameters to make the model appear smaller,
                    except in cases like this, where it is convenient to include them to make the model appear larger.
                    This is not unique to Google, as this approach has become a common practice across the field.</p>
                <p><span>Another interesting trick is the</span><a href="https://arxiv.org/abs/2310.07707" rel="">
                        MatFormer</a><span> concept (short for Matryoshka Transformer). For instance, Gemma 3n uses a
                        single shared LLM (transformer) architecture that can be sliced into smaller, independently
                        usable models. Each slice is trained to function on its own, so at inference time, we can run
                        just the part you need (instead of the large model).</span></p>
                <h1 class="header-anchor-post">4. Mistral Small 3.1<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§mistral-small" class="pencraft pc-reset header-anchor offset-top"></div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/mistral-small" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h1>
                <p><a href="https://mistral.ai/news/mistral-small-3-1" rel="">Mistral Small 3.1 24B</a><span>, which was
                        released in March shortly after Gemma 3, is noteworthy for outperforming Gemma 3 27B on several
                        benchmarks (except for math) while being faster.</span></p>
                <p>The reasons for the lower inference latency of Mistral Small 3.1 over Gemma 3 are likely due to their
                    custom tokenizer, as well as shrinking the KV cache and layer count. Otherwise, it's a standard
                    architecture as shown in the figure below.</p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ZZnO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ZZnO!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png 424w, https://substackcdn.com/image/fetch/$s_!ZZnO!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png 848w, https://substackcdn.com/image/fetch/$s_!ZZnO!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png 1272w, https://substackcdn.com/image/fetch/$s_!ZZnO!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ZZnO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png" width="1456" height="791" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:791,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ZZnO!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png 424w, https://substackcdn.com/image/fetch/$s_!ZZnO!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png 848w, https://substackcdn.com/image/fetch/$s_!ZZnO!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png 1272w, https://substackcdn.com/image/fetch/$s_!ZZnO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption"><em>Figure 16: An architecture comparison between Gemma 3 27B
                                and Mistral 3.1 Small 24B.</em></figcaption>
                    </figure>
                </div>
                <p></p>
                <p>Interestingly, earlier Mistral models had utilized sliding window attention, but they appear to have
                    abandoned it in Mistral Small 3.1. So, since Mistral uses regular Grouped-Query Attention instead of
                    Grouped-Query Attention with a sliding window as in Gemma 3, maybe there are additional inference
                    compute savings due to being able to use more optimized code (i.e., FlashAttention). For instance, I
                    speculate that while sliding window attention reduces memory usage, it doesn't necessarily reduce
                    inference latency, which is what Mistral Small 3.1 is focused on.</p>
                <h1 class="header-anchor-post">5. Llama 4<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§llama" class="pencraft pc-reset header-anchor offset-top"></div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/llama" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h1>
                <p><span>The extensive introductory discussion on Mixture-of-Experts (MoE) earlier in this article pays
                        off again. </span><a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" rel="">Llama 4</a><span> has also adopted an MoE approach and otherwise follows a relatively
                        standard architecture that is very similar to DeepSeek-V3, as shown in the figure below. (Llama
                        4 includes native multimodal support, similar to models like Gemma and Mistral. However, since
                        this article focuses on language modeling, we only focus on the text model.)</span></p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ShdO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ShdO!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 424w, https://substackcdn.com/image/fetch/$s_!ShdO!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 848w, https://substackcdn.com/image/fetch/$s_!ShdO!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 1272w, https://substackcdn.com/image/fetch/$s_!ShdO!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ShdO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png" width="1456" height="749" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:749,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ShdO!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 424w, https://substackcdn.com/image/fetch/$s_!ShdO!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 848w, https://substackcdn.com/image/fetch/$s_!ShdO!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 1272w, https://substackcdn.com/image/fetch/$s_!ShdO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption">Figure 17: An architecture comparison between DeepSeek V3
                            (671-billion parameters) and Llama 4 Maverick (400-billion parameters).</figcaption>
                    </figure>
                </div>
                <p></p>
                <p>While the Llama 4 Maverick architecture looks very similar to DeepSeek-V3 overall, there are some
                    interesting differences worth highlighting.</p>
                <p>First, Llama 4 uses Grouped-Query Attention similar to its predecessors, whereas DeepSeek-V3 uses
                    Multi-Head Latent Attention, which we discussed at the beginning of this article. Now, both
                    DeepSeek-V3 and Llama 4 Maverick are very large architectures, with DeepSeek-V3 being approximately
                    68% larger in its total parameter count. However, with 37 billion active parameters, DeepSeek-V3 has
                    more than twice as many active parameters as Llama 4 Maverick (17B).</p>
                <p>Llama 4 Maverick uses a more classic MoE setup with fewer but larger experts (2 active experts with
                    8,192 hidden size each) compared to DeepSeek-V3 (9 active experts with 2,048 hidden size each).
                    Also, DeepSeek uses MoE layers in each transformer block (except the first 3), whereas Llama 4
                    alternates MoE and dense modules in every other transformer block.</p>
                <p>Given the many small differences between architectures, it is difficult to determine their exact
                    impact on final model performance. The main takeaway, however, is that MoE architectures have seen a
                    significant rise in popularity in 2025.</p>
                <h1 class="header-anchor-post">6. Qwen3<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§qwen" class="pencraft pc-reset header-anchor offset-top"></div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/qwen" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h1>
                <p>The Qwen team consistently delivers high-quality open-weight LLMs. When I helped co-advising the LLM
                    efficiency challenge at NeurIPS 2023, I remember that the top winning solutions were all
                    Qwen2-based.</p>
                <p>Now, Qwen3 is another hit model series at the top of the leaderboards for their size classes. There
                    are 7 dense models: 0.6B, 1.7B, 4B, 8B, 14B, and 32B. And there are 2 MoE models: 30B-A3B, and
                    235B-A22B.</p>
                <p>(By the way, note that the missing whitespace in "Qwen3" is not a typo; I simply try to preserve the
                    original spelling the Qwen developers chose.)</p>
                <h2 class="header-anchor-post"><strong>6.1 Qwen3 (Dense)</strong>
                    <div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§qwen-dense" class="pencraft pc-reset header-anchor offset-top"></div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/qwen-dense" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h2>
                <p>Let's discuss the dense model architecture first. As of this writing, the 0.6B model may well be the
                    smallest current-generation open-weight model out there. And based on my personal experience, it
                    performs really well given its small size. It has great token/sec throughput and a low memory
                    footprint if you are planning to run it locally. But what's more, it's also easy to train locally
                    (for educational purposes) due to its small size.</p>
                <p>So, Qwen3 0.6B has replaced Llama 3 1B for me for most purposes. A comparison between these two
                    architectures is shown below.</p>
                <p></p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!e8cD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!e8cD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png 424w, https://substackcdn.com/image/fetch/$s_!e8cD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png 848w, https://substackcdn.com/image/fetch/$s_!e8cD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png 1272w, https://substackcdn.com/image/fetch/$s_!e8cD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!e8cD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png" width="1331" height="807" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:807,&quot;width&quot;:1331,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!e8cD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png 424w, https://substackcdn.com/image/fetch/$s_!e8cD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png 848w, https://substackcdn.com/image/fetch/$s_!e8cD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png 1272w, https://substackcdn.com/image/fetch/$s_!e8cD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption">Figure 18: An architecture comparison between Qwen3 0.6B and
                            Llama 3 1B; notice that Qwen3 is a deeper architecture with more layers, whereas Llama 3 is
                            a wider architecture with more attention heads.</figcaption>
                    </figure>
                </div>
                <p></p>
                <p><span>If you are interested in a human-readable Qwen3 implementation without external third-party LLM
                        library dependencies, I recently implemented </span><a href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/11_qwen3" rel="">Qwen3 from
                        scratch (in pure PyTorch)</a><span>.</span></p>
                <p>The computational performance numbers in the figure above are based on my from-scratch PyTorch
                    implementations when run on an A100 GPU. As one can see, Qwen3 has a smaller memory footprint as it
                    is a smaller architecture overall, but also uses smaller hidden layers and fewer attention heads.
                    However, it uses more transformer blocks than Llama 3, which leads to a slower runtime (lower
                    tokens/sec generation speed).</p>
                <h2 class="header-anchor-post"><strong>6.2 Qwen3 (MoE)</strong>
                    <div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§qwen-moe" class="pencraft pc-reset header-anchor offset-top"></div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/qwen-moe" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h2>
                <p>As mentioned earlier, Qwen3 also comes in two MoE flavors: 30B-A3B and 235B-A22B. Why do some
                    architectures, like Qwen3, come as regular (dense) and MoE (sparse) variants?</p>
                <p>As mentioned at the beginning of this article, MoE variants help reduce inference costs for large
                    base models. Offering both dense and MoE versions gives users flexibility depending on their goals
                    and constraints.</p>
                <p>Dense models are typically more straightforward to fine-tune, deploy, and optimize across various
                    hardware.</p>
                <p>On the other hand, MoE models are optimized for scaling inference. For instance, at a fixed inference
                    budget, they can achieve a higher overall model capacity (i.e., knowledge uptake during training due
                    to being larger) without proportionally increasing inference costs.</p>
                <p>By releasing both types, the Qwen3 series can support a broader range of use cases: dense models for
                    robustness, simplicity, and fine-tuning, and MoE models for efficient serving at scale.</p>
                <p>To round up this section, let's look at Qwen3 235B-A22B (note that the A22B stands for "22B active
                    parameters) to DeepSeek-V3, which has almost twice as many active parameters (37B).</p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!cVH6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!cVH6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png 424w, https://substackcdn.com/image/fetch/$s_!cVH6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png 848w, https://substackcdn.com/image/fetch/$s_!cVH6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png 1272w, https://substackcdn.com/image/fetch/$s_!cVH6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!cVH6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png" width="1456" height="744" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:744,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:231906,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!cVH6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png 424w, https://substackcdn.com/image/fetch/$s_!cVH6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png 848w, https://substackcdn.com/image/fetch/$s_!cVH6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png 1272w, https://substackcdn.com/image/fetch/$s_!cVH6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption">Figure 19: An architecture comparison between DeepSeek-V3 and
                            Qwen3 235B-A22B.</figcaption>
                    </figure>
                </div>
                <p></p>
                <p></p>
                <p><span>As shown in the figure above, the DeepSeek-V3 and Qwen3 235B-A22B architectures are remarkably
                        similar. What's noteworthy, though, is that the Qwen3 model moved away from using a shared
                        expert (earlier Qwen models, such as </span><a href="https://qwenlm.github.io/blog/qwen2.5-max/" rel="">Qwen2.5-MoE</a><span> did use a shared expert).</span></p>
                <p>Unfortunately, the Qwen3 team did not disclose any reason as to why they moved away from shared
                    experts. If I had to guess, it was perhaps simply not necessary for training stability for their
                    setup when they increased the experts from 2 (in Qwen2.5-MoE) to 8 (in Qwen3). And then they were
                    able to save the extra compute/memory cost by using only 8 instead of 8+1 experts. (However, this
                    doesn't explain why DeepSeek-V3 is still keeping their shared expert.)</p>
                <p><strong>Update.</strong><span> </span><a href="https://x.com/JustinLin610/status/1947364862184853626" rel="">Junyang Lin</a><span>, one of the developers of Qwen3, responded as follows:</span></p>
                <blockquote>
                    <p>At that moment we did not find significant enough improvement on shared expert and we were
                        worrying about the optimization for inference caused by shared expert. No straight answer to
                        this question honestly.</p>
                </blockquote>
                <h1 class="header-anchor-post">7. SmolLM3<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§smollm" class="pencraft pc-reset header-anchor offset-top"></div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/smollm" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h1>
                <p><a href="https://huggingface.co/blog/smollm3" rel="">SmolLM3</a><span> is perhaps not as nearly as
                        popular as the other LLMs covered in this article, but I thought it is still an interesting
                        model to include as it offers really good modeling performance at a relatively small and
                        convenient 3-billion parameter model size that sits between the 1.7B and 4B Qwen3 model, as
                        shown in the figure below.</span></p>
                <p>Moreover, it also shared a lot of the training details, similar to OLMo, which is rare and always
                    appreciated!</p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!vPTQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!vPTQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 424w, https://substackcdn.com/image/fetch/$s_!vPTQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 848w, https://substackcdn.com/image/fetch/$s_!vPTQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 1272w, https://substackcdn.com/image/fetch/$s_!vPTQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!vPTQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png" width="592" height="413.5235531628533" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ebfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:519,&quot;width&quot;:743,&quot;resizeWidth&quot;:592,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!vPTQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 424w, https://substackcdn.com/image/fetch/$s_!vPTQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 848w, https://substackcdn.com/image/fetch/$s_!vPTQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 1272w, https://substackcdn.com/image/fetch/$s_!vPTQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption"><em>Figure 20: An annotated figure from the SmolLM3
                                announcement post, https://huggingface.co/blog/smollm3, comparing the SmolLM3 win rate
                                to Qwen3 1.7B and 4B as well as Llama 3 3B and Gemma 3 4B.</em></figcaption>
                    </figure>
                </div>
                <p></p>
                <p>As shown in the architecture comparison figure below, the SmolLM3 architecture looks fairly standard.
                    The perhaps most interesting aspect is its use of NoPE (No Positional Embeddings), though.</p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!5Iki!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!5Iki!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png 424w, https://substackcdn.com/image/fetch/$s_!5Iki!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png 848w, https://substackcdn.com/image/fetch/$s_!5Iki!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png 1272w, https://substackcdn.com/image/fetch/$s_!5Iki!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!5Iki!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png" width="1431" height="777" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:777,&quot;width&quot;:1431,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!5Iki!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png 424w, https://substackcdn.com/image/fetch/$s_!5Iki!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png 848w, https://substackcdn.com/image/fetch/$s_!5Iki!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png 1272w, https://substackcdn.com/image/fetch/$s_!5Iki!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption">Figure 21: A side-by-side architecture comparison between
                            Qwen3 4B and SmolLM3 3B.</figcaption>
                    </figure>
                </div>
                <p></p>
                <h2 class="header-anchor-post"><strong>7.1 No Positional Embeddings (NoPE)</strong>
                    <div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§no-positional-embeddings-nope" class="pencraft pc-reset header-anchor offset-top">
                            </div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/no-positional-embeddings-nope" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h2>
                <p><span>NoPE is, in LLM contexts, an older idea that goes back to a 2023 paper (</span><a href="https://arxiv.org/abs/2305.19466" rel="">The Impact of Positional Encoding on Length
                        Generalization in Transformers</a><span>) to remove explicit positional information injection
                        (like through classic absolute positional embedding layers in early GPT architectures or
                        nowadays RoPE).</span></p>
                <p>In transformer-based LLMs, positional encoding is typically necessary because self-attention treats
                    tokens independently of order. Absolute position embeddings solve this by adding an additional
                    embedding layer that adds information to the token embeddings.</p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!lLgK!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!lLgK!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 424w, https://substackcdn.com/image/fetch/$s_!lLgK!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 848w, https://substackcdn.com/image/fetch/$s_!lLgK!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 1272w, https://substackcdn.com/image/fetch/$s_!lLgK!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!lLgK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png" width="1190" height="548" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:548,&quot;width&quot;:1190,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!lLgK!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 424w, https://substackcdn.com/image/fetch/$s_!lLgK!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 848w, https://substackcdn.com/image/fetch/$s_!lLgK!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 1272w, https://substackcdn.com/image/fetch/$s_!lLgK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption">Figure 22: A modified figure from my Build A Large Language
                            Model (From Scratch) book
                            (https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167) illustrating
                            absolute positional embeddings.</figcaption>
                    </figure>
                </div>
                <p>RoPE, on the other hand, solves this by rotating the query and key vectors relative to their token
                    position.</p>
                <p>In NoPE layers, however, no such positional signal is added at all: not fixed, not learned, not
                    relative. Nothing.</p>
                <p><span>Even though there is no positional embedding, the model still knows which tokens come before,
                        thanks to the causal attention mask. This mask prevents each token from attending to future
                        ones. As a result, a token at position </span><em>t</em><span> can only see tokens at positions
                    </span><em>≤ t</em><span>, which preserves the autoregressive ordering.</span></p>
                <p>So while there is no positional information that is explicitly added, there is still an implicit
                    sense of direction baked into the model's structure, and the LLM, in the regular
                    gradient-descent-based training, can learn to exploit it if it finds it beneficial for the
                    optimization objective. (Check out the NoPE paper's theorems for more information.)</p>
                <p><span>So, overall, the </span><a href="https://arxiv.org/abs/2305.19466" rel="">NoPE paper</a><span>
                        not only found that no positional information injection is necessary, but it also found that
                        NoPE has better length generalization, which means that LLM answering performance deteriorates
                        less with increased sequence length, as shown in the figure below.</span></p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!I9j6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!I9j6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 424w, https://substackcdn.com/image/fetch/$s_!I9j6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 848w, https://substackcdn.com/image/fetch/$s_!I9j6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 1272w, https://substackcdn.com/image/fetch/$s_!I9j6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!I9j6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png" width="1364" height="800" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:800,&quot;width&quot;:1364,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!I9j6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 424w, https://substackcdn.com/image/fetch/$s_!I9j6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 848w, https://substackcdn.com/image/fetch/$s_!I9j6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 1272w, https://substackcdn.com/image/fetch/$s_!I9j6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption">Figure 23: An annotated figure from the NoPE paper
                            (https://arxiv.org/abs/2305.19466) showing better length generalization with NoPE.
                        </figcaption>
                    </figure>
                </div>
                <p></p>
                <p>Note that the experiments shown above were conducted with a relatively small GPT-style model of
                    approximately 100 million parameters and relatively small context sizes. It is unclear how well
                    these findings generalize to larger, contemporary LLMs.</p>
                <p>For this reason, the SmolLM3 team likely only "applied" NoPE (or rather omitted RoPE) in every 4th
                    layer.</p>
                <h1 class="header-anchor-post">8. Kimi 2<div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent">
                        <div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA">
                            <div id="§kimi" class="pencraft pc-reset header-anchor offset-top"></div><button type="button" aria-label="Link" data-href="https://magazine.sebastianraschka.com/i/168650848/kimi" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_secondary-S63h9o" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link">
                                    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                    <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                                </svg></button>
                        </div>
                    </div>
                </h1>
                <p><a href="https://moonshotai.github.io/Kimi-K2/" rel="">Kimi 2</a><span> recently made big waves in
                        the AI community due to being an open-weight model with an incredibly good performance.
                        According to benchmarks, it's on par with the best proprietary models like Google's Gemini,
                        Anthropic's Claude, and OpenAI's ChatGPT models.</span></p>
                <p><span>A notable aspect is its use of a variant of the relatively new</span><a href="https://github.com/KellerJordan/Muon" rel=""> Muon</a><span> optimizer over AdamW. As far
                        as I know, this is the first time Muon was used over AdamW for any production model of this size
                        (</span><a href="https://arxiv.org/abs/2502.16982" rel="">previously</a><span>, it has only been
                        shown to scale up to 16B). This resulted in very nice training loss curves, which probably
                        helped catapult this model to the top of the aforementioned benchmarks.</span></p>
                <p>While people commented that the loss was exceptionally smooth (due to the lack of spikes), I think
                    it's not exceptionally smooth (e.g., see the OLMo 2 loss curve in the figure below; also, the L2
                    norm of the gradient would probably be a better metric to track training stability). However, what's
                    remarkable is how well the loss curve decays.</p>
                <p>However, as mentioned in the introduction of this article, training methodologies are a topic for
                    another time.</p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!_Zh8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!_Zh8!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 424w, https://substackcdn.com/image/fetch/$s_!_Zh8!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 848w, https://substackcdn.com/image/fetch/$s_!_Zh8!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 1272w, https://substackcdn.com/image/fetch/$s_!_Zh8!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!_Zh8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png" width="612" height="551.5256916996047" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:684,&quot;width&quot;:759,&quot;resizeWidth&quot;:612,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!_Zh8!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 424w, https://substackcdn.com/image/fetch/$s_!_Zh8!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 848w, https://substackcdn.com/image/fetch/$s_!_Zh8!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 1272w, https://substackcdn.com/image/fetch/$s_!_Zh8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption">Figure 24: Annotated figures from the Kimi K2 announcement
                            blog article (https://moonshotai.github.io/Kimi-K2/) and the OLMo 2 paper
                            (https://arxiv.org/abs/2305.19466).</figcaption>
                    </figure>
                </div>
                <p>The model itself is 1 trillion parameters large, which is truly impressive.</p>
                <p><span>It may be the biggest LLM of this generation as of this writing (given the constraints that
                        Llama 4 Behemoth is not released, proprietary LLMs don't count, and Google's 1.6 trillion
                    </span><a href="https://arxiv.org/abs/2101.03961" rel="">Switch Transformer</a><span> is an
                        encoder-decoder architecture from a different generation).</span></p>
                <p>It's also coming full circle as Kimi 2 uses the DeepSeek-V3 architecture we covered at the beginning
                    of this article except they made it larger, as shown in the figure below.</p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!B3em!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!B3em!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 424w, https://substackcdn.com/image/fetch/$s_!B3em!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 848w, https://substackcdn.com/image/fetch/$s_!B3em!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 1272w, https://substackcdn.com/image/fetch/$s_!B3em!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!B3em!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png" width="1456" height="743" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b721c5ef-057b-405b-9293-f11e161d9230_1599x816.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:743,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!B3em!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 424w, https://substackcdn.com/image/fetch/$s_!B3em!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 848w, https://substackcdn.com/image/fetch/$s_!B3em!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 1272w, https://substackcdn.com/image/fetch/$s_!B3em!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a>
                        <figcaption class="image-caption">Figure 25: An architecture comparison between DeepSeek V3 and
                            Kimi K2.</figcaption>
                    </figure>
                </div>
                <p></p>
                <p>As shown in the figure above, Kimi 2.5 is basically the same as DeepSeek V3, except that it uses more
                    experts in the MoE modules and fewer heads in the Multi-head Latent Attention (MLA) module.</p>
                <p><span>Kimi 2 is not coming out of nowhere. The earlier Kimi 1.5 model discussed in the </span><a href="https://arxiv.org/abs/2501.12599" rel="">Kimi k1.5: Scaling Reinforcement Learning with
                        LLMs paper</a><span>, was impressive as well. However, it had the bad luck that the DeepSeek R1
                        model paper was published on exactly the same date on January 22nd. Moreover, as far as I know,
                        the Kimi 1.5 weights were never publicly shared.</span></p>
                <p>So, most likely the Kimi K2 team took these lessons to heart and shared Kimi K2 as an open-weight
                    model, before DeepSeek R2 was released. As of this writing, Kimi K2 is the most impressive
                    open-weight model.</p>
                <p><strong>After all these years, LLM releases remain exciting, and I am curious to see what's
                        next!</strong></p>
                <p></p>
                <div>
                    <hr>
                </div>
                <p><em>This magazine is a personal passion project, and your support helps keep it alive. If you would
                        like to contribute, there are a few great ways:</em></p>
                <ul>
                    <li>
                        <p><em><strong><a href="https://amzn.to/4fqvn0D" rel="">Grab a copy of my
                                        book</a></strong><span>. Build a Large Language Model (From Scratch) walks you
                                    through building an LLM step by step, from tokenizer to training.</span></em></p>
                    </li>
                </ul>
                <ul>
                    <li>
                        <p><em><strong><a href="https://www.manning.com/livevideo/master-and-build-large-language-models" rel="">Check out the video course</a></strong><span>. There’s now a 17-hour
                                    video course based on the book, available from Manning. It follows the book closely,
                                    section by section, and works well both as a standalone or as a code-along resource.
                                    The video course is ad-free (unlike the YouTube version) and has a cleaner, more
                                    structured format. It also contains 5 additional hours of pre-requisite video
                                    material created by Abhinav Kimothi.</span></em></p>
                    </li>
                </ul>
                <ul>
                    <li>
                        <p><em><strong><a href="https://magazine.sebastianraschka.com/subscribe" rel="">Subscribe</a></strong><span>. A paid subscription helps to make my
                                    writing sustainable and gives you access to additional contents.</span></em></p>
                    </li>
                </ul>
                <p><em>Thanks for reading, and for helping support independent research!</em></p>
                <div class="captioned-image-container">
                    <figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!wVLk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg" data-component-name="Image2ToDOM" rel="" class="image-link image2 is-viewable-img">
                            <div class="image2-inset">
                                <picture>
                                    <source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!wVLk!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg" width="1456" height="878" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:878,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!wVLk!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1456w" sizes="100vw" loading="lazy" class="sizing-normal">
                                </picture>
                                <div class="image-link-expand">
                                    <div class="pencraft pc-display-flex pc-gap-8 pc-reset">
                                        <div class="pencraft pc-reset icon-container restack-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw">
                                                <path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path>
                                                <path d="M21 3v5h-5"></path>
                                                <path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path>
                                                <path d="M8 16H3v5"></path>
                                            </svg></div>
                                        <div class="pencraft pc-reset icon-container view-image"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2">
                                                <polyline points="15 3 21 3 21 9"></polyline>
                                                <polyline points="9 21 3 21 3 15"></polyline>
                                                <line x1="21" x2="14" y1="3" y2="10"></line>
                                                <line x1="3" x2="10" y1="21" y2="14"></line>
                                            </svg></div>
                                    </div>
                                </div>
                            </div>
                        </a></figure>
                </div>
            </div>
        </div>