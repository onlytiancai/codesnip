https://github.com/onlytiancai/codesnip/commit/a7483143afc5dd203dace73f3cd70609ffc206df 

改了3行代码，让神经网络的预测准确率从 50% 提升到了100%，前向传播是把sigmiod改成relu，反向传播把sigmid的导数改成relu的导数，只改隐藏层。 

如果不修改隐藏层的激活函数，无论如何修改learning_rate和epochs，以及 hidden_size 都不能提高模型效果，Loss 始终无法降低，始终是 0.25，修改后Epoch 1000, Loss: 0.1489，Epoch 2000, Loss: 0.0037，训练速度大大提高。

这是一个非常典型的梯度消失与学习率不匹配导致的训练停滞问题。对于隐藏层，ReLU  是一个更好的选择，因为它在正区间导数恒为 1，即使学习率保持在 0.01，也可能有更好的表现，因为它缓解了梯度消失问题。

再分析下原来用sigmoid的版本为什么损失始终停留在 0.25。
- 初始权重很小，接近 0，但 sigmoid(0) = 0.5, xor 的真实值是 0 和 1，这样初始误差 output_error 就相对很大，离 1 和 0 都很远。
- 输出层梯度 output_delta 是误差乘以sigmoid 导数，sigmoid 导数最大值是 0.25，这俩乘起来是个很小的数。
- 隐藏层梯度hidden_delta 是输出层梯度乘以隐藏层权重W2.T，再乘以 sigmoid导数（又是0.25），这值更小了，相当于梯度丢失了。
- 这时候学习率又不高，只有0.01，再乘以隐藏层梯度来更新权重，这是一个很小很小的值，每次迭代权重几乎没更新。
- 要么提高学习率，要么把隐藏层的激活函数缓存不容易造成梯度丢失的，总之这俩要配套。
