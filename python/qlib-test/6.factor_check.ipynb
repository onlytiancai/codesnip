{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c35ad0f-0e80-4b72-b47f-4a504d387711",
   "metadata": {},
   "source": [
    "思路是：先用 Alpha158 handler 取出特征矩阵，然后基于 statsmodels 做 ADF/KPSS 平稳性检验，和基于相关系数矩阵 + VIF 做共线性诊断。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af1779e-8d67-4a19-aab0-2d6cf477a36b",
   "metadata": {},
   "source": [
    "## 1) 取出 Alpha158 因子数据（Panel → MultiIndex DataFrame）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "474bb769-5438-4047-a5df-399df3c4fc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[74066:MainThread](2025-09-02 15:59:45,992) INFO - qlib.Initialization - [config.py:452] - default_conf: client.\n",
      "[74066:MainThread](2025-09-02 15:59:45,993) INFO - qlib.Initialization - [__init__.py:79] - qlib successfully initialized based on client settings.\n",
      "[74066:MainThread](2025-09-02 15:59:45,993) INFO - qlib.Initialization - [__init__.py:81] - data_path={'__DEFAULT_FREQ': PosixPath('/Users/huhao/.qlib/qlib_data/cn_data')}\n",
      "[74066:MainThread](2025-09-02 16:00:03,027) INFO - qlib.timer - [log.py:127] - Time cost: 17.032s | Loading data Done\n",
      "[74066:MainThread](2025-09-02 16:00:03,157) INFO - qlib.timer - [log.py:127] - Time cost: 0.053s | DropnaLabel Done\n",
      "[74066:MainThread](2025-09-02 16:00:03,573) INFO - qlib.timer - [log.py:127] - Time cost: 0.415s | CSZScoreNorm Done\n",
      "[74066:MainThread](2025-09-02 16:00:03,576) INFO - qlib.timer - [log.py:127] - Time cost: 0.549s | fit & process data Done\n",
      "[74066:MainThread](2025-09-02 16:00:03,576) INFO - qlib.timer - [log.py:127] - Time cost: 17.582s | Init data Done\n"
     ]
    }
   ],
   "source": [
    "import qlib\n",
    "from qlib.data import D\n",
    "from qlib.contrib.data.handler import Alpha158\n",
    "from qlib.workflow import R\n",
    "import pandas as pd\n",
    "qlib.init(provider_uri=\"~/.qlib/qlib_data/cn_data\")  # 根据你的路径调整\n",
    "\n",
    "# 定义股票池与时间窗\n",
    "universe = D.instruments(market=\"csi300\")     # 例：沪深300\n",
    "start, end = \"2016-01-01\", \"2022-12-31\"\n",
    "\n",
    "# 使用 Alpha158 handler 计算特征\n",
    "handler = Alpha158(instruments=universe, start_time=start, end_time=end, fit_start_time=start, fit_end_time=end)\n",
    "df_feat = handler.fetch(col_set=\"feature\") \n",
    "\n",
    "# 可选：与label对齐或去极值/标准化，按你后续建模流程来\n",
    "# 这里先简单清洗\n",
    "df_feat = df_feat.sort_index().dropna(how=\"all\")  # 行全空去掉\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f186293-63fd-4484-bdfe-833615752112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>KMID</th>\n",
       "      <th>KLEN</th>\n",
       "      <th>KMID2</th>\n",
       "      <th>KUP</th>\n",
       "      <th>KUP2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th>instrument</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2016-01-04</th>\n",
       "      <th>SH600000</th>\n",
       "      <td>-0.026258</td>\n",
       "      <td>0.039934</td>\n",
       "      <td>-0.657535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SH600008</th>\n",
       "      <td>-0.099804</td>\n",
       "      <td>0.105675</td>\n",
       "      <td>-0.944445</td>\n",
       "      <td>0.003914</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SH600009</th>\n",
       "      <td>-0.042668</td>\n",
       "      <td>0.046393</td>\n",
       "      <td>-0.919709</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.007299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SH600010</th>\n",
       "      <td>-0.099448</td>\n",
       "      <td>0.104972</td>\n",
       "      <td>-0.947369</td>\n",
       "      <td>0.002762</td>\n",
       "      <td>0.026316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SH600011</th>\n",
       "      <td>-0.055046</td>\n",
       "      <td>0.060780</td>\n",
       "      <td>-0.905662</td>\n",
       "      <td>0.003440</td>\n",
       "      <td>0.056603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2022-12-30</th>\n",
       "      <th>SZ300896</th>\n",
       "      <td>-0.003063</td>\n",
       "      <td>0.023940</td>\n",
       "      <td>-0.127942</td>\n",
       "      <td>0.004418</td>\n",
       "      <td>0.184562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SZ300979</th>\n",
       "      <td>-0.007301</td>\n",
       "      <td>0.023814</td>\n",
       "      <td>-0.306569</td>\n",
       "      <td>0.008170</td>\n",
       "      <td>0.343066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SZ300999</th>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.017004</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.006664</td>\n",
       "      <td>0.391889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SZ301236</th>\n",
       "      <td>-0.011798</td>\n",
       "      <td>0.033146</td>\n",
       "      <td>-0.355932</td>\n",
       "      <td>0.016292</td>\n",
       "      <td>0.491525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SZ301269</th>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.013667</td>\n",
       "      <td>0.056908</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.487806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>513680 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           KMID      KLEN     KMID2       KUP      KUP2\n",
       "datetime   instrument                                                  \n",
       "2016-01-04 SH600000   -0.026258  0.039934 -0.657535  0.000000  0.000000\n",
       "           SH600008   -0.099804  0.105675 -0.944445  0.003914  0.037037\n",
       "           SH600009   -0.042668  0.046393 -0.919709  0.000339  0.007299\n",
       "           SH600010   -0.099448  0.104972 -0.947369  0.002762  0.026316\n",
       "           SH600011   -0.055046  0.060780 -0.905662  0.003440  0.056603\n",
       "...                         ...       ...       ...       ...       ...\n",
       "2022-12-30 SZ300896   -0.003063  0.023940 -0.127942  0.004418  0.184562\n",
       "           SZ300979   -0.007301  0.023814 -0.306569  0.008170  0.343066\n",
       "           SZ300999    0.000919  0.017004  0.054054  0.006664  0.391889\n",
       "           SZ301236   -0.011798  0.033146 -0.355932  0.016292  0.491525\n",
       "           SZ301269    0.000778  0.013667  0.056908  0.006667  0.487806\n",
       "\n",
       "[513680 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat[['KMID','KLEN','KMID2','KUP','KUP2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e06a65-7269-4fe2-92e9-c8cc47d455b3",
   "metadata": {},
   "source": [
    "## 2) 平稳性检测（ADF + KPSS）\n",
    "\n",
    "要点\n",
    "\n",
    "- 因子是时间序列，做 时间向 的单位根检验。\n",
    "- 按（股票、因子）分别做 ADF；结果在横截面上做聚合（如通过率、p 值分布）。\n",
    "- ADF 原假设“非平稳”；KPSS 原假设“平稳”。两者结合更稳健。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65290e76-9034-4a95-b5eb-f6b6dd4a400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 安全的 ADF/KPSS wrapper\n",
    "def safe_adf(series, min_n=50, regression='c', autolag='AIC'):\n",
    "    out = {'adf_stat': np.nan, 'adf_p': np.nan, 'adf_note': None, 'adf_error': None}\n",
    "    ts = series.dropna()\n",
    "    if len(ts) < min_n or ts.nunique() <= 1 or ts.std() < 1e-12:\n",
    "        out['adf_error'] = 'short_or_constant'\n",
    "        return out\n",
    "    try:\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.simplefilter(\"always\")\n",
    "            stat, p, *_ = adfuller(ts.values, autolag=autolag, regression=regression)\n",
    "            if any(isinstance(x.message, InterpolationWarning) for x in w):\n",
    "                out['adf_note'] = 'interp_warning'\n",
    "                p = np.nan\n",
    "            out.update({'adf_stat': stat, 'adf_p': p})\n",
    "            return out\n",
    "    except Exception as e:\n",
    "        out['adf_error'] = str(e)\n",
    "        return out\n",
    "\n",
    "def safe_kpss(series, min_n=50, regression='c', nlags='auto'):\n",
    "    out = {'kpss_stat': np.nan, 'kpss_p': np.nan, 'kpss_note': None, 'kpss_error': None}\n",
    "    ts = series.dropna()\n",
    "    if len(ts) < min_n or ts.nunique() <= 1 or ts.std() < 1e-12:\n",
    "        out['kpss_error'] = 'short_or_constant'\n",
    "        return out\n",
    "    try:\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.simplefilter(\"always\")\n",
    "            stat, p, *_ = kpss(ts.values, regression=regression, nlags=nlags)\n",
    "            if any(isinstance(x.message, InterpolationWarning) for x in w):\n",
    "                out['kpss_note'] = 'interp_warning'\n",
    "                p = np.nan\n",
    "            out.update({'kpss_stat': stat, 'kpss_p': p})\n",
    "            return out\n",
    "    except Exception as e:\n",
    "        out['kpss_error'] = str(e)\n",
    "        return out\n",
    "\n",
    "# 3. Panel 检测逻辑\n",
    "def panel_adf_kpss(df_feat, min_n=80):\n",
    "    rows = []\n",
    "    for fac in df_feat.columns:\n",
    "        mat = df_feat[fac].unstack(level=1)\n",
    "        for inst in mat.columns:\n",
    "            s = mat[inst]\n",
    "            adf_r = safe_adf(s, min_n=min_n)\n",
    "            kpss_r = safe_kpss(s, min_n=min_n)\n",
    "            row = {'factor': fac, 'instrument': inst, 'n_obs': s.dropna().shape[0]}\n",
    "            row.update(adf_r)\n",
    "            row.update(kpss_r)\n",
    "            rows.append(row)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2e9bca-8a50-42c3-ab65-ad8d4bceff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = panel_adf_kpss(df_feat, min_n=80)\n",
    "\n",
    "# 4. 聚合报表（修复后的写法）\n",
    "summary = res_df.groupby(\"factor\").agg({\n",
    "    \"n_obs\": \"count\",\n",
    "    \"adf_p\": lambda x: np.mean(x < 0.05),\n",
    "    \"kpss_p\": lambda x: np.mean(x > 0.05),\n",
    "})\n",
    "\n",
    "both_pass = res_df.groupby(\"factor\").apply(\n",
    "    lambda g: np.mean((g[\"adf_p\"] < 0.05) & (g[\"kpss_p\"] > 0.05))\n",
    ")\n",
    "summary[\"both_pass_rate\"] = both_pass\n",
    "\n",
    "print(\"==== Top 10 最稳定因子 ====\")\n",
    "print(summary.sort_values(\"both_pass_rate\", ascending=False).head(10))\n",
    "print(\"\\n==== Bottom 10 最不稳定因子 ====\")\n",
    "print(summary.sort_values(\"both_pass_rate\", ascending=True).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329396c3-e2f1-4ff2-b7fa-b5cf2b3fde95",
   "metadata": {},
   "source": [
    "\n",
    "## Top 10 最稳定因子\n",
    "\n",
    "这些因子（如 `WVMA30`, `CORR10`, `STD5` 等）在 **大部分股票上都能通过 ADF 和 KPSS 的平稳性检验**：\n",
    "\n",
    "* **`adf_p` 列**：表示 ADF p 值 < 0.05 的比例\n",
    "\n",
    "  * 比如 `CORR10` 的 0.956790 ≈ 95.7% 股票在 ADF 下判定为平稳。\n",
    "\n",
    "* **`kpss_p` 列**：表示 KPSS p 值 > 0.05 的比例\n",
    "\n",
    "  * `CORR10` 的 0.148148 ≈ 14.8% 股票在 KPSS 下判定为平稳。\n",
    "\n",
    "* **`both_pass_rate` 列**：两个检验同时通过的比例\n",
    "\n",
    "  * `WVMA30` 大约 14.6%，在所有因子里算是比较高的。\n",
    "\n",
    "👉 结论：这些因子总体上 **比较平稳**，但也不是完美，大部分因子真正双检验都过的比例只有 10–15%，说明 **股票层面的因子时间序列大多还是弱平稳或存在单位根**。\n",
    "\n",
    "---\n",
    "\n",
    "## Bottom 10 最不稳定因子\n",
    "\n",
    "典型代表：`RESI20`, `RESI10`, `RESI30`, `RESI60`，以及一系列 `VSUM*` 系列。\n",
    "\n",
    "* **RESI 系列**：\n",
    "\n",
    "  * `adf_p` 非常高（>0.9），说明几乎所有股票在 ADF 下都被判为「非平稳」。\n",
    "  * `kpss_p` 接近 0，说明在 KPSS 下也被判为「非平稳」。\n",
    "  * `both_pass_rate` = 0，彻底不平稳。\n",
    "\n",
    "* **VWAP0**：ADF 和 KPSS 两个检验都挂零，说明它的序列几乎完全没有平稳性。\n",
    "\n",
    "* **VSUMP/VSUMN/VSUMD 系列**：`kpss_p` 在 0.8–1.8% 之间，也基本不平稳。\n",
    "\n",
    "👉 结论：\n",
    "这些因子序列要么趋势很强、要么波动模式不稳定，**对时间序列模型（如 IC 回归、时序预测）非常不友好**。在建模时需要：\n",
    "\n",
    "* 考虑 **差分（diff）或标准化** 来处理。\n",
    "* 或者直接 **剔除掉这些因子**，避免引入噪声。\n",
    "\n",
    "---\n",
    "\n",
    "⚖️ **整体情况**\n",
    "\n",
    "* 在 Alpha158 里，确实只有一小部分因子在股票横截面上表现出较好的平稳性。\n",
    "* 像 `RESI*` 这种残差相关的指标，大概率是 **非平稳的“价格驱动型因子”**。\n",
    "* 反而一些基于波动率（`STD*`）、移动平均（`WVMA*`）、相关性（`CORR*`）的因子，平稳性更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d59ae-eb04-4b41-857c-5c399ab7430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 可视化某个因子的分布\n",
    "fac = summary.sort_values(\"both_pass_rate\").index[-1]\n",
    "sub = res_df[res_df['factor'] == fac]\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.scatter(sub['adf_p'], sub['kpss_p'], alpha=0.6)\n",
    "plt.axvline(0.05, color='red', ls='--')\n",
    "plt.axhline(0.05, color='red', ls='--')\n",
    "plt.xlabel(\"ADF p-value (<0.05=stationary)\")\n",
    "plt.ylabel(\"KPSS p-value (>0.05=stationary)\")\n",
    "plt.title(f\"{fac} 平稳性检测 (股票横截面)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cea3cb-74e9-4422-ade7-61f774e58691",
   "metadata": {},
   "outputs": [],
   "source": [
    "这是针对 **因子 WVMA30 ** 做的 **平稳性检测结果（横截面股票层面）**：\n",
    "\n",
    "---\n",
    "\n",
    "### 图表内容\n",
    "\n",
    "* **横轴：ADF p-value**\n",
    "\n",
    "  * 判别标准：`< 0.05` → 认为序列平稳。\n",
    "  * 图中红色虚线在 `0.05` 处。\n",
    "\n",
    "* **纵轴：KPSS p-value**\n",
    "\n",
    "  * 判别标准：`> 0.05` → 认为序列平稳。\n",
    "  * 图中红色虚线在 `0.05` 处。\n",
    "\n",
    "* **散点：每个点代表某只股票在该因子上的序列检验结果**\n",
    "\n",
    "  * 越靠左（ADF p 越小）说明平稳性越强。\n",
    "  * 越靠上（KPSS p 越大）说明平稳性越强。\n",
    "  * **理想的因子**应该分布在左上象限（ADF < 0.05 且 KPSS > 0.05）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517d98e1-0d15-42f7-9f8a-81f1a110f6ad",
   "metadata": {},
   "source": [
    "常见处理\n",
    "\n",
    "- 对“明显非平稳”的因子：做差分 / 对数差分 / 滚动去趋势（如 s - s.rolling(60).mean()）后重检。\n",
    "- 也可只在训练集时间窗内做检验，避免数据泄露。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df189a38-9716-4a58-9333-5c7b9d7175a6",
   "metadata": {},
   "source": [
    "### 3) 多重共线性检测（相关矩阵 + VIF）\n",
    "\n",
    "要点\n",
    "\n",
    "- 共线性是横截面问题：同一交易日、不同因子之间的线性相关。\n",
    "- 做法：选取一个代表性日期（或在多日上分别统计），对因子矩阵 X_t 计算相关系数矩阵&VIF。\n",
    "- 阈值经验：|ρ|>0.9 视为高度相关；VIF>10 视为严重共线（可调为 5）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b82a1-9248-4003-9dda-ed154b8f9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# 取一个日期的横截面（也可对多日循环后求均值/分位数）\n",
    "a_date = df_feat.index.get_level_values(0).unique()[-1]   # 取样例的最后一天\n",
    "X = df_feat.xs(a_date, level=0)  # index=instrument, columns=factors\n",
    "X = X.dropna(axis=1, thresh=int(0.8*len(X)))  # 丢弃缺失过多的因子\n",
    "X = X.dropna(axis=0)                           # 丢掉有缺失的股票\n",
    "# 简单标准化（便于数值稳定）\n",
    "X = (X - X.mean())/X.std(ddof=0)\n",
    "\n",
    "# 1) 相关性矩阵\n",
    "corr = X.corr().abs()\n",
    "\n",
    "# 2) VIF（逐列回归的 1/(1-R^2)）\n",
    "vif = pd.Series(\n",
    "    [variance_inflation_factor(X.values, i) for i in range(X.shape[1])],\n",
    "    index=X.columns, name=\"VIF\"\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "# 3) 基于阈值的自动剔除（示例：先剔高度相关，再按VIF去冗余）\n",
    "#   先相关性去冗余：保留每对高度相关(|ρ|>0.95)中的重要性更高者（这里用全样本方差占位，你也可以换成IC/重要性等）\n",
    "to_drop = set()\n",
    "corr_thr = 0.95\n",
    "fac_var = df_feat.var()  # 全样本上的因子方差，示意用途\n",
    "for i in corr.columns:\n",
    "    if i in to_drop: \n",
    "        continue\n",
    "    for j in corr.index:\n",
    "        if i == j or j in to_drop: \n",
    "            continue\n",
    "        if corr.loc[j, i] > corr_thr:\n",
    "            # 保留方差更大的（更“有信息量”的）因子\n",
    "            drop = j if fac_var[j] < fac_var[i] else i\n",
    "            to_drop.add(drop)\n",
    "\n",
    "X_red = X.drop(columns=list(to_drop))\n",
    "\n",
    "# 再按 VIF 阈值迭代剔除\n",
    "def drop_by_vif(X, thr=10):\n",
    "    while True:\n",
    "        vif_s = pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)\n",
    "        worst = vif_s.max()\n",
    "        if worst <= thr:\n",
    "            return X, vif_s.sort_values(ascending=False)\n",
    "        X = X.drop(columns=[vif_s.idxmax()])\n",
    "\n",
    "X_final, vif_final = drop_by_vif(X_red, thr=10)\n",
    "\n",
    "# 结果：\n",
    "# - corr：因子间相关性矩阵\n",
    "# - vif：初始VIF排序\n",
    "# - X_final/vif_final：去冗余后的因子子集与其VIF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c8ef8-1d71-41ae-b5e6-67f9703aef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab293b-1598-430e-83d7-2d5d9343cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70516115-05f3-4d36-a94d-dcaf3a99a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e9f28b-796e-4eae-8a94-174865a3cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef80b87-cfce-4482-ac6b-81bcec47ef65",
   "metadata": {},
   "source": [
    "实务建议\n",
    "\n",
    "- 不要只看某一天：可在多日上重复计算，将“高相关对出现频次”与“VIF 均值/分位数”作为依据，挑出长期共线的因子对再做剔除。\n",
    "- 如果你后续用树模型（如 LGBM），对共线性不敏感；但在线性/稀疏模型里（如 Lasso/线回归）就很重要。\n",
    "- 也可用 PCA/正交化（如对相关簇做 PCA 取前主成分）作为“去共线”的替代方案。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b64b7-e2dd-4224-abfe-3119958695e7",
   "metadata": {},
   "source": [
    "## 4) 把两件事串成流程（示例）\n",
    "\n",
    "训练集期间做 ADF+KPSS，过滤掉“通过率过低”的因子（例如 both_pass_rate < 0.6）。\n",
    "\n",
    "对剩余因子，在多日横截面上统计相关矩阵和 VIF，按阈值与业务偏好（可结合 IC、特征重要性）剔除冗余。\n",
    "\n",
    "记录保留名单，固定到后续训练/回测配置中，避免数据泄露。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qlib)",
   "language": "python",
   "name": "qlib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
