{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca05a754-59e1-4217-ba5d-1229243802c5",
   "metadata": {},
   "source": [
    "# 1. ä»‹ç»peftçš„LoraConfig å’Œç”¨äºåº”ç”¨ p-tuning çš„ PromptEncoderConfigï¼Œå„ç”¨äºä»€ä¹ˆåœºæ™¯\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e0163f-57ea-4aec-b6f2-6972e9b2a0fb",
   "metadata": {},
   "source": [
    "éå¸¸å¥½çš„é—®é¢˜ï¼æˆ‘ä»¬æ¥è¯¦ç»†è®²è®² **PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰** ä¸­ä¸¤ä¸ªæ ¸å¿ƒé…ç½®ç±»ï¼š\n",
    "ğŸ‘‰ `LoraConfig` å’Œ ğŸ‘‰ `PromptEncoderConfig`ã€‚\n",
    "\n",
    "è¿™ä¸¤ä¸ªç±»ä»£è¡¨ä¸¤ç§**ä¸åŒçš„å‚æ•°é«˜æ•ˆå¾®è°ƒç­–ç•¥**ï¼Œåˆ†åˆ«é€‚ç”¨äºä¸åŒçš„æ¨¡å‹å’Œä»»åŠ¡åœºæ™¯ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ä¸€ã€`LoraConfig` â€”â€” LoRAï¼ˆLow-Rank Adaptationï¼‰\n",
    "\n",
    "### ğŸŒŸ æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "LoRA çš„æ ¸å¿ƒæ˜¯ï¼š\n",
    "**åœ¨å†»ç»“åŸæ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹ï¼Œåªä¸ºç‰¹å®šå±‚ï¼ˆå¦‚ Attention çš„æƒé‡çŸ©é˜µï¼‰æ·»åŠ ä½ç§©å¯è®­ç»ƒçŸ©é˜µã€‚**\n",
    "\n",
    "åŸå§‹æƒé‡çŸ©é˜µ `W`ï¼ˆç»´åº¦ dÃ—kï¼‰åœ¨è®­ç»ƒä¸­ä¸æ›´æ–°ï¼Œè€Œæ˜¯å¢åŠ ä¸¤ä¸ªå°çŸ©é˜µï¼š\n",
    "\n",
    "> ( W' = W + BA )ï¼Œå…¶ä¸­ A âˆˆ â„^{rÃ—k}, B âˆˆ â„^{dÃ—r}ï¼Œr â‰ª d, k\n",
    "\n",
    "è¿™ä½¿å¾—è®­ç»ƒå‚æ•°é‡å¤§å¤§å‡å°‘ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ `LoraConfig` ä¸»è¦å‚æ•°\n",
    "\n",
    "| å‚æ•°               | å«ä¹‰                         | ç¤ºä¾‹                                       |\n",
    "| ---------------- | -------------------------- | ---------------------------------------- |\n",
    "| `r`              | ä½ç§©çŸ©é˜µçš„ç§©ï¼ˆrankï¼‰ï¼Œæ§åˆ¶å¯è®­ç»ƒå‚æ•°è§„æ¨¡     | é€šå¸¸ 4ã€8ã€16                                |\n",
    "| `lora_alpha`     | ç¼©æ”¾å› å­ï¼Œæ§åˆ¶ LoRA çš„æ›´æ–°å¼ºåº¦         | é€šå¸¸ 16 æˆ– 32                               |\n",
    "| `target_modules` | éœ€è¦æ’å…¥ LoRA çš„å±‚å              | å¦‚ `[\"q_proj\", \"v_proj\"]`ï¼ˆé€‚ç”¨äºTransformerï¼‰ |\n",
    "| `lora_dropout`   | åœ¨ LoRA å±‚å‰ä½¿ç”¨ dropoutï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰ | 0.05                                     |\n",
    "| `bias`           | æ˜¯å¦è®­ç»ƒ bias                  | `\"none\"` æˆ– `\"all\"`                       |\n",
    "| `task_type`      | æŒ‡å®šä»»åŠ¡ç±»å‹ï¼ˆç”¨äº PEFT æ¡†æ¶è‡ªåŠ¨æ³¨å†Œï¼‰     | `\"CAUSAL_LM\"`, `\"SEQ_CLS\"`, ç­‰            |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  é€‚ç”¨åœºæ™¯\n",
    "\n",
    "* å¤§å‹ Transformer æ¨¡å‹ï¼ˆBERTã€GPTã€LLaMAã€T5ã€Falcon ç­‰ï¼‰\n",
    "* ä»»åŠ¡ï¼šæ–‡æœ¬ç”Ÿæˆã€é—®ç­”ã€åˆ†ç±»ã€æŒ‡ä»¤å¾®è°ƒç­‰\n",
    "* èµ„æºæœ‰é™åœºæ™¯ä¸‹çš„é«˜æ•ˆå¾®è°ƒ\n",
    "\n",
    "ğŸ“ **å…¸å‹ç”¨é€”ï¼š**\n",
    "å¯¹ **å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰** è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰ï¼Œè€Œä¸ç ´ååŸæ¨¡å‹æ€§èƒ½ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## äºŒã€`PromptEncoderConfig` â€”â€” P-Tuning v2 / Prompt Tuning\n",
    "\n",
    "### ğŸŒŸ æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "P-Tuning v2 é€šè¿‡åœ¨æ¨¡å‹è¾“å…¥ä¸­æ·»åŠ **å¯è®­ç»ƒçš„è™šæ‹Ÿæç¤ºå‘é‡ï¼ˆprompt embeddingsï¼‰**ï¼Œè®©æ¨¡å‹å­¦ä¹ å¦‚ä½•æ ¹æ®ä»»åŠ¡è°ƒæ•´ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚\n",
    "ä¸åŒäº LoRA æ”¹åŠ¨æƒé‡ï¼ŒPrompt Tuning åªåœ¨è¾“å…¥ä¾§åŠ å…¥å‚æ•°ã€‚\n",
    "\n",
    "Prompt å‘é‡é€šå¸¸é€šè¿‡ä¸€ä¸ªå°å‹ **Prompt Encoderï¼ˆå¦‚ LSTM æˆ– MLPï¼‰** ç”Ÿæˆã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ `PromptEncoderConfig` ä¸»è¦å‚æ•°\n",
    "\n",
    "| å‚æ•°                    | å«ä¹‰                           | ç¤ºä¾‹                            |\n",
    "| --------------------- | ---------------------------- | ----------------------------- |\n",
    "| `num_virtual_tokens`  | è™šæ‹Ÿ prompt token æ•°é‡           | 20, 50, 100                   |\n",
    "| `encoder_hidden_size` | Prompt ç¼–ç å™¨éšè—å±‚ç»´åº¦              | 128, 256                      |\n",
    "| `token_dim`           | è¯å‘é‡ç»´åº¦ï¼ˆä¸€èˆ¬ä¸æ¨¡å‹ embedding å¤§å°ä¸€è‡´ï¼‰  | è‡ªåŠ¨æ£€æµ‹                          |\n",
    "| `encoder_type`        | Prompt ç¼–ç å™¨ç±»å‹ï¼ˆ`mlp` æˆ– `lstm`ï¼‰ | `\"mlp\"`                       |\n",
    "| `task_type`           | æŒ‡å®šä»»åŠ¡ç±»å‹ï¼ˆç”¨äº PEFT æ¡†æ¶ï¼‰           | `\"CAUSAL_LM\"`, `\"SEQ_CLS\"`, ç­‰ |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  é€‚ç”¨åœºæ™¯\n",
    "\n",
    "* æ¨¡å‹è¾“å…¥ç«¯å¯æ’å…¥ Prompt embedding çš„ä»»åŠ¡ï¼ˆå°¤å…¶é€‚åˆ encoder-decoder æˆ– decoder-only æ¨¡å‹ï¼‰\n",
    "* ä½èµ„æºä»»åŠ¡ï¼ˆä¾‹å¦‚åªæœ‰å‡ ç™¾æ¡æ ·æœ¬ï¼‰\n",
    "* ä¸æƒ³ä¿®æ”¹æ¨¡å‹ç»“æ„æˆ–å‚æ•°çš„æƒ…å†µ\n",
    "\n",
    "ğŸ“ **å…¸å‹ç”¨é€”ï¼š**\n",
    "\n",
    "* åˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æï¼‰\n",
    "* å°‘æ ·æœ¬å­¦ä¹ ï¼ˆfew-shotï¼‰\n",
    "* æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­åŠ å…¥ä»»åŠ¡æç¤ºï¼ˆprompt-based fine-tuningï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” å¯¹æ¯”æ€»ç»“\n",
    "\n",
    "| å¯¹æ¯”é¡¹        | LoRA (`LoraConfig`) | P-Tuning / Prompt Tuning (`PromptEncoderConfig`) |\n",
    "| ---------- | ------------------- | ------------------------------------------------ |\n",
    "| å‚æ•°æ’å…¥ä½ç½®     | æ¨¡å‹æƒé‡å±‚ï¼ˆå¦‚ attentionï¼‰  | è¾“å…¥åµŒå…¥å±‚ï¼ˆembeddingï¼‰                                 |\n",
    "| æ˜¯å¦ä¿®æ”¹æ¨¡å‹ç»“æ„   | æ˜¯ï¼ˆå¢åŠ  LoRA å±‚ï¼‰        | å¦ï¼ˆä»…åŠ è™šæ‹Ÿ tokenï¼‰                                    |\n",
    "| è®­ç»ƒå‚æ•°é‡      | å°‘ï¼ˆä½ç§©çŸ©é˜µï¼‰             | æå°‘ï¼ˆå‡ åä¸ªè™šæ‹Ÿ tokenï¼‰                                  |\n",
    "| æ˜¯å¦éœ€è¦æ¨¡å‹å†…éƒ¨è®¿é—® | æ˜¯ï¼ˆè¦çŸ¥é“ç›®æ ‡å±‚ï¼‰           | å¦ï¼ˆåªè¦ embedding å±‚å³å¯ï¼‰                              |\n",
    "| å…¸å‹ä»»åŠ¡       | æŒ‡ä»¤å¾®è°ƒã€å¤§æ¨¡å‹é€‚é…          | å°‘æ ·æœ¬åˆ†ç±»ã€å°æ ·æœ¬ç”Ÿæˆ                                      |\n",
    "| ä¼˜ç‚¹         | å…¼å®¹æ€§å¥½ï¼Œæ•ˆæœç¨³å®š           | æ›´è½»é‡ï¼Œè®­ç»ƒæ›´å¿«                                         |\n",
    "| ç¼ºç‚¹         | éœ€è¦çŸ¥é“æ¨¡å‹å†…éƒ¨ç»“æ„          | æ•ˆæœå¯èƒ½ä¸å¦‚ LoRA ç¨³å®š                                   |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… å°ç»“\n",
    "\n",
    "* **`LoraConfig`**ï¼šé€‚åˆå¤§æ¨¡å‹ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒï¼Œ**ä¿®æ”¹æƒé‡å±‚ç»“æ„ï¼ˆä¸­å±‚é€‚é…ï¼‰**ã€‚\n",
    "* **`PromptEncoderConfig`**ï¼šé€‚åˆè½»é‡ä»»åŠ¡æˆ–ä½èµ„æºåœºæ™¯ï¼Œ**åªåœ¨è¾“å…¥ç«¯å­¦ä¹ ä»»åŠ¡æç¤ºï¼ˆå‰å±‚é€‚é…ï¼‰**ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ca1b3e-36a3-40fc-88f7-73f01283300a",
   "metadata": {},
   "source": [
    "# 2. causal language modeling å› æœè¯­è¨€æ¨¡å‹\n",
    "\n",
    "https://huggingface.co/docs/transformers/tasks/language_modeling\n",
    "\n",
    "There are two types of language modeling, causal and masked. This guide illustrates causal language modeling. Causal language models are frequently used for text generation. You can use these models for creative applications like choosing your own text adventure or an intelligent coding assistant like Copilot or CodeParrot.\n",
    "\n",
    "è¯­è¨€å»ºæ¨¡åˆ†ä¸ºå› æœå‹å’Œæ©ç å‹ä¸¤ç§ã€‚æœ¬æŒ‡å—å°†é‡ç‚¹ä»‹ç»å› æœå‹è¯­è¨€å»ºæ¨¡ã€‚å› æœå‹è¯­è¨€æ¨¡å‹å¸¸ç”¨äºæ–‡æœ¬ç”Ÿæˆã€‚æ‚¨å¯ä»¥å°†è¿™äº›æ¨¡å‹ç”¨äºå„ç§åˆ›æ„åº”ç”¨ï¼Œä¾‹å¦‚äº’åŠ¨å¼æ–‡æœ¬å†’é™©æ¸¸æˆæˆ–åƒ Copilot æˆ– CodeParrot è¿™æ ·çš„æ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹ã€‚\n",
    "\n",
    "notebook list\n",
    "\n",
    "https://huggingface.co/spaces/PEFT/causal-language-modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8828a4d0-086c-4562-a16e-0ec476d3e7b9",
   "metadata": {},
   "source": [
    "# 3. ä»‹ç»transformerså’Œpeftè¿™ä¸¤ä¸ªpythonåº“\n",
    "\n",
    "å½“ç„¶å¯ä»¥ ğŸ‘\n",
    "ä¸‹é¢æˆ‘æ¥è¯¦ç»†ä»‹ç»ä¸€ä¸‹ **ğŸ¤— Transformers** å’Œ **PEFT** è¿™ä¸¤ä¸ªåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œå¤§æ¨¡å‹å¾®è°ƒä¸­éå¸¸å¸¸ç”¨çš„ Python åº“ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  ä¸€ã€Transformers åº“ï¼ˆæ¥è‡ª Hugging Faceï¼‰\n",
    "\n",
    "### ğŸ“¦ ç®€ä»‹\n",
    "\n",
    "`transformers` æ˜¯ç”± **Hugging Face** å¼€å‘çš„ä¸€ä¸ªå¼€æºåº“ï¼Œç”¨äºæ–¹ä¾¿åœ°ä½¿ç”¨ã€è®­ç»ƒå’Œéƒ¨ç½² **é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPretrained Language Modelsï¼‰**ã€‚\n",
    "å®ƒæ”¯æŒå¤§é‡çš„æ¨¡å‹æ¶æ„ï¼Œæ¯”å¦‚ï¼š\n",
    "\n",
    "* BERTã€RoBERTaã€DistilBERT\n",
    "* GPT ç³»åˆ—ï¼ˆGPT-2ã€GPT-Neoã€GPT-Jã€GPT-3.5-turbo API æ¥å£ï¼‰\n",
    "* T5ã€BARTã€MarianMT\n",
    "* Vision Transformer (ViT)ã€CLIPã€Whisper ç­‰å¤šæ¨¡æ€æ¨¡å‹\n",
    "\n",
    "### ğŸ”§ æ ¸å¿ƒåŠŸèƒ½\n",
    "\n",
    "1. **æ¨¡å‹åŠ è½½ä¸æ¨ç†**\n",
    "\n",
    "   ```python\n",
    "   from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "   tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "   model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "   inputs = tokenizer(\"Transformers are awesome!\", return_tensors=\"pt\")\n",
    "   outputs = model(**inputs)\n",
    "   print(outputs.logits)\n",
    "   ```\n",
    "\n",
    "2. **æ¨¡å‹å¾®è°ƒï¼ˆFine-tuningï¼‰**\n",
    "   æ”¯æŒ Trainer API å’Œ Accelerate ç­‰å·¥å…·å¿«é€Ÿè¿›è¡Œè®­ç»ƒï¼š\n",
    "\n",
    "   ```python\n",
    "   from transformers import Trainer, TrainingArguments\n",
    "   # å®šä¹‰æ•°æ®é›†ã€æ¨¡å‹ã€è®­ç»ƒå‚æ•°ï¼Œç„¶åç›´æ¥Trainer()è®­ç»ƒ\n",
    "   ```\n",
    "\n",
    "3. **å¤šä»»åŠ¡æ”¯æŒ**\n",
    "\n",
    "   * æ–‡æœ¬åˆ†ç±»ï¼ˆSentiment Analysisï¼‰\n",
    "   * é—®ç­”ï¼ˆQuestion Answeringï¼‰\n",
    "   * æœºå™¨ç¿»è¯‘ï¼ˆTranslationï¼‰\n",
    "   * æ–‡æœ¬ç”Ÿæˆï¼ˆText Generationï¼‰\n",
    "   * è¯­éŸ³è¯†åˆ«ï¼ˆSpeech Recognitionï¼‰\n",
    "   * å›¾åƒç†è§£ï¼ˆVision Modelsï¼‰\n",
    "\n",
    "4. **ç”Ÿæ€å®Œå¤‡**\n",
    "\n",
    "   * ğŸ¤— Datasetsã€Tokenizersã€Accelerateã€PEFTã€Diffusers\n",
    "   * Transformers Hub ä¸Šæœ‰æ•°åƒä¸ªå¼€æºæ¨¡å‹å¯ä»¥ä¸€é”®åŠ è½½\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸª¶ äºŒã€PEFT åº“ï¼ˆParameter-Efficient Fine-Tuningï¼‰\n",
    "\n",
    "### ğŸ“¦ ç®€ä»‹\n",
    "\n",
    "`peft`ï¼ˆParameter-Efficient Fine-Tuningï¼‰æ˜¯ Hugging Face çš„ä¸€ä¸ªç‹¬ç«‹åº“ï¼Œ\n",
    "ä¸“é—¨ç”¨äº **å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é«˜æ•ˆå¾®è°ƒ**ï¼Œå³åªæ›´æ–°æ¨¡å‹çš„ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œä»è€ŒèŠ‚çœè®¡ç®—ä¸æ˜¾å­˜ã€‚\n",
    "\n",
    "> å…¨ç§°ï¼š**PEFT â€” Parameter-Efficient Fine-Tuning**\n",
    "\n",
    "å®ƒå¯ä»¥ä¸ `transformers` åº“æ— ç¼é›†æˆã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "ä¼ ç»Ÿå¾®è°ƒï¼ˆFine-tuningï¼‰éœ€è¦æ›´æ–°æ¨¡å‹çš„ **å…¨éƒ¨å‚æ•°ï¼ˆå¯èƒ½æ˜¯æ•°åäº¿ä¸ªï¼‰**ï¼Œ\n",
    "è€Œ PEFT åªå¾®è°ƒä¸€å°éƒ¨åˆ†ï¼ˆä¾‹å¦‚ 0.1%â€“1%ï¼‰ï¼Œå¸¸è§æ–¹æ³•åŒ…æ‹¬ï¼š\n",
    "\n",
    "| æ–¹æ³•                             | æ€æƒ³              | ç‰¹ç‚¹       |\n",
    "| ------------------------------ | --------------- | -------- |\n",
    "| **LoRA** (Low-Rank Adaptation) | åœ¨æ¨¡å‹æƒé‡çŸ©é˜µä¸Šæ·»åŠ ä½ç§©é€‚é…å±‚ | æœ€å¸¸ç”¨ï¼ŒèŠ‚çœæ˜¾å­˜ |\n",
    "| **Prefix Tuning**              | åœ¨è¾“å…¥å‰åŠ å¯å­¦ä¹ å‰ç¼€å‘é‡    | ä¸æ”¹åŠ¨æ¨¡å‹ç»“æ„  |\n",
    "| **Prompt Tuning**              | å­¦ä¹ ä¸€ç»„â€œè™šæ‹Ÿæç¤ºâ€      | é€‚åˆç”Ÿæˆä»»åŠ¡   |\n",
    "| **Adapters**                   | åœ¨æ¯å±‚ä¸­æ’å…¥å°é€‚é…æ¨¡å—     | æ¨¡å—åŒ–ã€çµæ´»   |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹ï¼ˆä»¥ LoRA ä¸ºä¾‹ï¼‰\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# å®šä¹‰LoRAé…ç½®\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    r=8,  # ä½ç§©ç»´åº¦\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# å°†æ¨¡å‹è½¬æ¢ä¸ºPEFTæ¨¡å‹\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # æ˜¾ç¤ºå¯è®­ç»ƒå‚æ•°æ¯”ä¾‹\n",
    "```\n",
    "\n",
    "è®­ç»ƒååªéœ€ä¿å­˜ LoRA çš„æƒé‡ï¼š\n",
    "\n",
    "```python\n",
    "model.save_pretrained(\"./lora-llama2\")\n",
    "```\n",
    "\n",
    "åŠ è½½æ—¶å¯ä»¥åˆå¹¶ LoRA æ¨¡å—ï¼š\n",
    "\n",
    "```python\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, \"./lora-llama2\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ Transformers ä¸ PEFT çš„å…³ç³»\n",
    "\n",
    "| å¯¹æ¯”é¡¹    | Transformers       | PEFT                        |\n",
    "| ------ | ------------------ | --------------------------- |\n",
    "| å®šä½     | æ¨¡å‹åŠ è½½ã€æ¨ç†ã€è®­ç»ƒæ¡†æ¶       | å‚æ•°é«˜æ•ˆå¾®è°ƒæ¡†æ¶                    |\n",
    "| æ¥æº     | Hugging Face ä¸»åº“    | Hugging Face å­åº“             |\n",
    "| æ˜¯å¦ç‹¬ç«‹è¿è¡Œ | âœ… æ˜¯                | âš ï¸ éœ€é…åˆ Transformers ä½¿ç”¨      |\n",
    "| ç”¨é€”     | é€šç”¨é¢„è®­ç»ƒå’Œæ¨ç†           | å¤§æ¨¡å‹ä½æˆæœ¬å¾®è°ƒ                    |\n",
    "| ç¤ºä¾‹æ¨¡å‹   | BERTã€T5ã€GPTã€CLIP ç­‰ | LoRAã€Prefix Tuningã€Adapters |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ æ€»ç»“\n",
    "\n",
    "| åº“                | ä¸»è¦ç”¨é€”         | ä¼˜ç‚¹              |\n",
    "| ---------------- | ------------ | --------------- |\n",
    "| **Transformers** | ä½¿ç”¨å’Œè®­ç»ƒå„ç§é¢„è®­ç»ƒæ¨¡å‹ | ç”Ÿæ€å®Œå–„ã€ä¸Šæ‰‹ç®€å•ã€æ”¯æŒå¤šæ¨¡æ€ |\n",
    "| **PEFT**         | é«˜æ•ˆå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹    | æ˜¾å­˜å ç”¨å°ã€è®­ç»ƒå¿«ã€æ•ˆæœå¥½   |\n",
    "\n",
    "---\n",
    "\n",
    "æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ å†™ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ï¼ˆæ¯”å¦‚â€œä½¿ç”¨ Transformers + PEFT å¾®è°ƒä¸€ä¸ªæ–‡æœ¬åˆ†ç±»æ¨¡å‹â€ï¼‰ï¼Ÿ\n",
    "é‚£æ ·æˆ‘å¯ä»¥ç»™å‡ºå¯è¿è¡Œçš„ä»£ç æ¨¡æ¿ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46be7d2-3ae8-4ca4-af1a-f045a735947f",
   "metadata": {},
   "source": [
    "# 4. Hugging Face è¿˜æœ‰å“ªäº›å¸¸ç”¨çš„åº“\n",
    "\n",
    "éå¸¸å¥½çš„é—®é¢˜ ğŸ‘\n",
    "Hugging Face ä¸åªæ˜¯æœ‰ `transformers` å’Œ `peft` â€”â€” å®ƒå…¶å®æ˜¯ä¸€ä¸ªå®Œæ•´çš„ **AI æ¨¡å‹ç”Ÿæ€ç³»ç»Ÿ**ï¼Œ\n",
    "æ¶µç›–äº† **NLPã€CVã€è¯­éŸ³ã€ç”Ÿæˆæ¨¡å‹** ç­‰å¤šä¸ªé¢†åŸŸã€‚ä¸‹é¢æˆ‘å¸®ä½ ç³»ç»Ÿåœ°æ¢³ç†ä¸€ä¸‹ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© ä¸€ã€Hugging Face å¸¸ç”¨æ ¸å¿ƒåº“æ€»è§ˆ\n",
    "\n",
    "| ç±»åˆ«           | åº“å                                            | ä¸»è¦ç”¨é€”                              | ç®€ä»‹                               |\n",
    "| ------------ | --------------------------------------------- | --------------------------------- | -------------------------------- |\n",
    "| ğŸ§  æ¨¡å‹        | **transformers**                              | é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ã€æ¨ç†ã€å¾®è°ƒ                     | æ ¸å¿ƒåº“ï¼Œæ”¯æŒä¸Šåƒä¸ªæ¨¡å‹ï¼ˆBERTã€GPTã€T5 ç­‰ï¼‰       |\n",
    "| ğŸ§® æ•°æ®        | **datasets**                                  | æ•°æ®é›†åŠ è½½ä¸å¤„ç†                          | é«˜æ•ˆã€ç»Ÿä¸€çš„æ•°æ®ç®¡é“ï¼Œæ”¯æŒæµå¼åŠ è½½å’Œ Arrow å­˜å‚¨      |\n",
    "| âš¡ åŠ é€Ÿ         | **accelerate**                                | å¤š GPU / TPU / åˆ†å¸ƒå¼è®­ç»ƒ               | ç®€åŒ–åˆ†å¸ƒå¼è®­ç»ƒä»£ç ï¼Œä¸€è¡Œå³å¯å¯ç”¨å¤šå¡               |\n",
    "| ğŸª¶ å¾®è°ƒ        | **peft**                                      | å‚æ•°é«˜æ•ˆå¾®è°ƒ                            | æ”¯æŒ LoRAã€Prefix Tuningã€Adapters ç­‰ |\n",
    "| ğŸ§Š æ¨¡å‹é‡åŒ–      | **bitsandbytes**                              | 8-bit / 4-bit æ¨¡å‹åŠ è½½                | èŠ‚çœæ˜¾å­˜ã€å…¼å®¹ Transformers             |\n",
    "| ğŸ¨ æ–‡ç”Ÿå›¾       | **diffusers**                                 | æ‰©æ•£æ¨¡å‹ï¼ˆStable Diffusionã€ControlNetï¼‰ | æ–‡æœ¬ç”Ÿæˆå›¾åƒã€å›¾åƒç¼–è¾‘                      |\n",
    "| ğŸ™ï¸ è¯­éŸ³       | **speech** / **transformers**                 | è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³åˆæˆ                         | å†…å« Whisperã€Wav2Vec2ã€TTS æ¨¡å‹       |\n",
    "| ğŸ“¦ æ¨¡å‹éƒ¨ç½²      | **optimum**                                   | æ€§èƒ½ä¼˜åŒ–ä¸ç¡¬ä»¶åŠ é€Ÿ                         | æ”¯æŒ ONNXã€TensorRTã€OpenVINO ç­‰      |\n",
    "| ğŸ“Š è¯„ä¼°        | **evaluate**                                  | æ¨¡å‹è¯„æµ‹æŒ‡æ ‡                            | æä¾› BLEUã€ROUGEã€Accuracyã€F1 ç­‰å¸¸è§æŒ‡æ ‡  |\n",
    "| ğŸ§¬ Tokenizer | **tokenizers**                                | è¶…å¿«åˆ†è¯å™¨ï¼ˆRust å®ç°ï¼‰                    | æ¯”ä¼ ç»Ÿ Python åˆ†è¯å™¨å¿«æ•°åå€               |\n",
    "| ğŸ’¬ å¯¹è¯ä»£ç†      | **trl** (Transformers Reinforcement Learning) | RLHFã€PPOã€DPO ç­‰                    | ç”¨äºè®­ç»ƒå¯¹è¯æ¨¡å‹ï¼ˆChatGPT ç±»ï¼‰              |\n",
    "| ğŸŒ Hub æ¥å£    | **huggingface_hub**                           | ä¸ Hugging Face Hub äº¤äº’             | ä¸Šä¼ ã€ä¸‹è½½æ¨¡å‹ã€æ•°æ®é›†ã€ç©ºé—´åº”ç”¨                 |\n",
    "| ğŸ§° å·¥å…·        | **safetensors**                               | å®‰å…¨é«˜æ•ˆçš„æ¨¡å‹æƒé‡æ ¼å¼                       | æ›¿ä»£ `.bin`ï¼ŒåŠ è½½é€Ÿåº¦æ›´å¿«ã€æ›´å®‰å…¨             |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  äºŒã€NLP å¸¸ç”¨åº“ç»„åˆ\n",
    "\n",
    "| ä»»åŠ¡ç±»å‹         | å¸¸ç”¨åº“ç»„åˆ                                       | è¯´æ˜                      |\n",
    "| ------------ | ------------------------------------------- | ----------------------- |\n",
    "| æ–‡æœ¬åˆ†ç±»         | `transformers` + `datasets` + `evaluate`    | åŠ è½½æ•°æ®ã€è®­ç»ƒæ¨¡å‹ã€è®¡ç®—æŒ‡æ ‡          |\n",
    "| ç”Ÿæˆä»»åŠ¡ï¼ˆç¿»è¯‘/æ‘˜è¦ï¼‰  | `transformers` + `datasets`                 | æ”¯æŒ T5ã€BART ç­‰ Seq2Seq æ¨¡å‹ |\n",
    "| å¯¹è¯æ¨¡å‹ï¼ˆLLM å¾®è°ƒï¼‰ | `transformers` + `peft` + `trl`             | ç”¨äº LoRA + RLHF è®­ç»ƒ       |\n",
    "| æ¨¡å‹æ¨ç†åŠ é€Ÿ       | `transformers` + `optimum` + `bitsandbytes` | èŠ‚çœæ˜¾å­˜ã€æé«˜åå               |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¨ ä¸‰ã€CVï¼ˆè®¡ç®—æœºè§†è§‰ï¼‰ä¸å¤šæ¨¡æ€\n",
    "\n",
    "| åº“                | ç”¨é€”                                         | ç¤ºä¾‹æ¨¡å‹                         |\n",
    "| ---------------- | ------------------------------------------ | ---------------------------- |\n",
    "| **diffusers**    | æ–‡æœ¬ç”Ÿæˆå›¾åƒã€å›¾åƒä¿®å¤                                | Stable Diffusion, ControlNet |\n",
    "| **transformers** | æ”¯æŒ Vision Transformers (ViT), CLIP, DINOv2 | å›¾åƒåˆ†ç±»ã€å›¾æ–‡åŒ¹é…                    |\n",
    "| **datasets**     | å¤„ç†å›¾åƒæ•°æ®é›†                                    | COCOã€ImageNetã€CIFAR ç­‰        |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ™ï¸ å››ã€è¯­éŸ³ä¸éŸ³é¢‘\n",
    "\n",
    "| åº“                | ç”¨é€”               | ç¤ºä¾‹æ¨¡å‹                        |\n",
    "| ---------------- | ---------------- | --------------------------- |\n",
    "| **transformers** | è¯­éŸ³è¯†åˆ«ä¸åˆæˆ          | Whisper, Wav2Vec2, SpeechT5 |\n",
    "| **datasets**     | åŠ è½½éŸ³é¢‘æ•°æ®é›†          | Librispeech, CommonVoice    |\n",
    "| **evaluate**     | è®¡ç®— WER/CER ç­‰è¯­éŸ³æŒ‡æ ‡ | è¯„ä¼°æ¨¡å‹æ€§èƒ½                      |\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ äº”ã€æ¨¡å‹ä¼˜åŒ–ä¸éƒ¨ç½²\n",
    "\n",
    "| åº“                | ç”¨é€”        | è¯´æ˜                                       |\n",
    "| ---------------- | --------- | ---------------------------------------- |\n",
    "| **optimum**      | æ¨¡å‹åœ¨ç¡¬ä»¶ä¸Šçš„ä¼˜åŒ– | ä¸ ONNX / TensorRT / Habana / OpenVINO é›†æˆ |\n",
    "| **bitsandbytes** | ä½æ¯”ç‰¹é‡åŒ–åŠ è½½   | å¯ä¸ `transformers` é…åˆä½¿ç”¨ 4-bit/8-bit æ¨¡å‹    |\n",
    "| **accelerate**   | ç®€åŒ–åˆ†å¸ƒå¼è®­ç»ƒ   | è‡ªåŠ¨å¤„ç†å¤š GPUã€æ··åˆç²¾åº¦ã€DDP ç­‰å¤æ‚é…ç½®                 |\n",
    "| **safetensors**  | æ¨¡å‹å®‰å…¨å­˜å‚¨æ ¼å¼  | é€Ÿåº¦å¿«ä¸”é˜²æ­¢æ¶æ„ pickle ä»£ç æ³¨å…¥                     |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§° å…­ã€å¼€å‘ä¸éƒ¨ç½²è¾…åŠ©\n",
    "\n",
    "| åº“                      | ç”¨é€”                             |\n",
    "| ---------------------- | ------------------------------ |\n",
    "| **huggingface_hub**    | ä¸Šä¼ /ä¸‹è½½æ¨¡å‹ä¸æ•°æ®ï¼Œè®¿é—® Hugging Face Hub |\n",
    "| **gradio**             | å¿«é€Ÿæ„å»º Web Demoï¼ˆå¸¸å’Œ HF Space é…åˆï¼‰  |\n",
    "| **spaces**ï¼ˆé Python åº“ï¼‰ | Hugging Face çš„äº‘ç«¯åº”ç”¨æ‰˜ç®¡å¹³å°         |\n",
    "| **inference-api**      | å®˜æ–¹æ¨ç†æœåŠ¡ API                     |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ ä¸ƒã€æ¨èç»„åˆä½¿ç”¨åœºæ™¯\n",
    "\n",
    "| ç›®æ ‡         | æ¨èç»„åˆ                                        |\n",
    "| ---------- | ------------------------------------------- |\n",
    "| å¾®è°ƒä¸€ä¸ªæ–‡æœ¬åˆ†ç±»æ¨¡å‹ | `transformers` + `datasets` + `evaluate`    |\n",
    "| å¾®è°ƒå¤§æ¨¡å‹ï¼ˆLLMï¼‰ | `transformers` + `peft` + `accelerate`      |\n",
    "| åšå›¾åƒç”Ÿæˆ      | `diffusers` + `transformers`                |\n",
    "| åšè¯­éŸ³è¯†åˆ«      | `transformers` + `datasets` + `evaluate`    |\n",
    "| æ¨¡å‹é‡åŒ–/éƒ¨ç½²    | `transformers` + `optimum` + `bitsandbytes` |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å…«ã€å¿«é€Ÿå®‰è£…å‚è€ƒ\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets evaluate accelerate peft bitsandbytes diffusers optimum\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ ç”»ä¸€ä¸ª **â€œHugging Face ç”Ÿæ€ä½“ç³»å›¾â€**ï¼ˆå±•ç¤ºè¿™äº›åº“çš„å…³ç³»å’Œç”¨é€”ï¼‰ï¼Ÿ\n",
    "å¯ä»¥ç”¨å›¾ç¤ºæ–¹å¼è®©ä½ ä¸€çœ¼çœ‹æ‡‚æ•´ä¸ªç”Ÿæ€ç»“æ„ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df4623c-a806-4afe-821b-8d7815315844",
   "metadata": {},
   "source": [
    "# 5. Seq2SeqTrainerå’ŒSFTTrainerçš„åŒºåˆ«æ˜¯å•¥ï¼Ÿ\n",
    "\n",
    "è¿˜æœ‰å…¶å®ƒå“ªäº›ç±»ä¼¼çš„ç±»ï¼Œåˆ†åˆ«ç”¨åœ¨ä»€ä¹ˆåœºæ™¯ï¼Ÿ\n",
    "\n",
    "éå¸¸å¥½çš„é—®é¢˜ ğŸ’¡\n",
    "`Seq2SeqTrainer` å’Œ `SFTTrainer` éƒ½æ˜¯ Hugging Face ç”Ÿæ€ä¸­ç”¨äº **è®­ç»ƒæ¨¡å‹** çš„é«˜å±‚å°è£…ç±»ï¼Œ\n",
    "ä½†å®ƒä»¬çš„å®šä½ã€é€‚ç”¨ä»»åŠ¡å’Œåº•å±‚é€»è¾‘å·®å¼‚å¾ˆå¤§ã€‚\n",
    "æˆ‘æ¥ç»™ä½ è¯¦ç»†ã€ç³»ç»Ÿåœ°è®²æ¸…æ¥šè¿™ä¸¤ä¸ªç±»çš„åŒºåˆ«ï¼Œä»¥åŠå…¶ä»–å¸¸è§ Trainer ç±»çš„ç”¨é€”ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© ä¸€ã€æ€»ä½“èƒŒæ™¯\n",
    "\n",
    "åœ¨ Hugging Face çš„ç”Ÿæ€ä¸­ï¼Œè®­ç»ƒç±»ä¸»è¦åˆ†ä¸ºä¸¤æ¡è·¯çº¿ï¼š\n",
    "\n",
    "| è·¯çº¿               | æ ¸å¿ƒåº“                                        | å…¸å‹ Trainer                                 | ä¸»è¦ç”¨é€”                                  |\n",
    "| ---------------- | ------------------------------------------ | ------------------------------------------ | ------------------------------------- |\n",
    "| **é€šç”¨è®­ç»ƒè·¯çº¿**       | `transformers`                             | `Trainer`, `Seq2SeqTrainer`                | å¾®è°ƒä¼ ç»Ÿ Transformer æ¨¡å‹ï¼ˆBERTã€T5ã€BART ç­‰ï¼‰   |\n",
    "| **å¤§æ¨¡å‹ï¼ˆLLMï¼‰å¾®è°ƒè·¯çº¿** | `trl`ï¼ˆTransformers Reinforcement Learningï¼‰ | `SFTTrainer`, `PPOTrainer`, `DPOTrainer` ç­‰ | é’ˆå¯¹ ChatGPT/LLM çš„å¾®è°ƒã€å¯¹é½è®­ç»ƒï¼ˆSFTã€RLHFã€DPOï¼‰ |\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ äºŒã€`Seq2SeqTrainer` â€”â€” ä¼ ç»Ÿåºåˆ—åˆ°åºåˆ—ä»»åŠ¡ä¸“ç”¨\n",
    "\n",
    "### ğŸ“¦ æ‰€åœ¨åº“\n",
    "\n",
    "`transformers`\n",
    "\n",
    "### ğŸ“˜ ç”¨é€”\n",
    "\n",
    "ç”¨äºè®­ç»ƒ **Seq2Seqï¼ˆåºåˆ—åˆ°åºåˆ—ï¼‰æ¨¡å‹**ï¼Œå…¸å‹æ¨¡å‹æœ‰ï¼š\n",
    "\n",
    "* T5ã€BARTã€MarianMTï¼ˆç¿»è¯‘ï¼‰\n",
    "* Pegasusï¼ˆæ‘˜è¦ï¼‰\n",
    "* Encoder-Decoder ç±»å‹çš„æ¨¡å‹\n",
    "\n",
    "### âš™ï¸ åŠŸèƒ½ç‰¹ç‚¹\n",
    "\n",
    "* ç»§æ‰¿è‡ª `Trainer`\n",
    "* æ”¯æŒ **è‡ªåŠ¨è§£ç **ï¼ˆ`generate()`ï¼‰ç”¨äºéªŒè¯æ—¶ç”Ÿæˆè¾“å‡º\n",
    "* æ”¯æŒ **ROUGEã€BLEU** ç­‰ç”Ÿæˆç±»æŒ‡æ ‡\n",
    "* è‡ªåŠ¨å¤„ç† decoder çš„ label shiftï¼ˆ`labels[:, :-1]` vs `decoder_input_ids[:, 1:]`ï¼‰\n",
    "\n",
    "### ğŸ§© ç¤ºä¾‹\n",
    "\n",
    "```python\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, AutoModelForSeq2SeqLM\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5-finetune\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    predict_with_generate=True,  # å¯ç”¨è‡ªåŠ¨ç”Ÿæˆ\n",
    "    per_device_train_batch_size=8,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\"),\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "âœ… **é€‚åˆåœºæ™¯ï¼š**\n",
    "\n",
    "* ç¿»è¯‘ï¼ˆTranslateï¼‰\n",
    "* æ‘˜è¦ç”Ÿæˆï¼ˆSummarizationï¼‰\n",
    "* é—®ç­”ï¼ˆQuestion Generationï¼‰\n",
    "* ä»»ä½• encoder-decoder ç±»å‹ä»»åŠ¡\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  ä¸‰ã€`SFTTrainer` â€”â€” æŒ‡ä»¤å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰\n",
    "\n",
    "### ğŸ“¦ æ‰€åœ¨åº“\n",
    "\n",
    "`trl`ï¼ˆTransformers Reinforcement Learningï¼‰\n",
    "\n",
    "```bash\n",
    "pip install trl\n",
    "```\n",
    "\n",
    "### ğŸ“˜ ç”¨é€”\n",
    "\n",
    "`SFTTrainer` ç”¨äº **å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰**ï¼Œ\n",
    "ä¹Ÿå°±æ˜¯ ChatGPT çš„ç¬¬ä¸€é˜¶æ®µï¼š**ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuning, SFTï¼‰**ã€‚\n",
    "\n",
    "å®ƒé€šå¸¸ç”¨äºï¼š\n",
    "\n",
    "* Alpacaã€Baizeã€Vicunaã€LLaMAã€Mistral ç­‰æ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒ\n",
    "* åœ¨æŒ‡ä»¤+å›å¤ï¼ˆinstructionâ€“responseï¼‰æ ¼å¼çš„æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹\n",
    "\n",
    "### âš™ï¸ åŠŸèƒ½ç‰¹ç‚¹\n",
    "\n",
    "* æ”¯æŒè‡ªå®šä¹‰ **prompt æ ¼å¼æ¨¡æ¿**\n",
    "* æ”¯æŒä¸ **PEFT (LoRA)** ç»“åˆ\n",
    "* æ”¯æŒ 4-bit/8-bit é‡åŒ–æ¨¡å‹ (`bitsandbytes`)\n",
    "* å†…ç½®å…¼å®¹ LLM å¸¸ç”¨æ ¼å¼ï¼ˆå¦‚ ShareGPT/Alpaca JSONï¼‰\n",
    "\n",
    "### ğŸ§© ç¤ºä¾‹\n",
    "\n",
    "```python\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",  # æŒ‡æ˜æ•°æ®å­—æ®µ\n",
    "    max_seq_length=1024,\n",
    "    packing=True,  # è‡ªåŠ¨æ‹¼æ¥æ ·æœ¬ï¼Œæé«˜æ•ˆç‡\n",
    ")\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "âœ… **é€‚åˆåœºæ™¯ï¼š**\n",
    "\n",
    "* æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰\n",
    "* èŠå¤©å¾®è°ƒï¼ˆChat Fine-tuningï¼‰\n",
    "* Alpaca / Vicuna / LLaMA SFT\n",
    "* ChatGPT å¼è®­ç»ƒçš„ç¬¬ä¸€é˜¶æ®µ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ å››ã€æ ¸å¿ƒåŒºåˆ«å¯¹æ¯”\n",
    "\n",
    "| ç‰¹æ€§        | **Seq2SeqTrainer**             | **SFTTrainer**                  |\n",
    "| --------- | ------------------------------ | ------------------------------- |\n",
    "| æ‰€å±åº“       | `transformers`                 | `trl`                           |\n",
    "| æ¨¡å‹ç±»å‹      | Encoderâ€“Decoder                | Decoder-onlyï¼ˆLLMï¼‰               |\n",
    "| è¾“å…¥æ ¼å¼      | `(input_text, target_text)`    | å•å­—æ®µï¼ˆå¤šè½®æŒ‡ä»¤+å›å¤æ–‡æœ¬ï¼‰                  |\n",
    "| ä¸»è¦ä»»åŠ¡      | ç¿»è¯‘ã€æ‘˜è¦ã€é—®ç­”                       | æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰        |\n",
    "| å…¸å‹æ¨¡å‹      | T5, BART, MarianMT             | LLaMA, Mistral, Falcon, GPT-Neo |\n",
    "| æ˜¯å¦è‡ªåŠ¨ç”Ÿæˆ    | âœ… `predict_with_generate=True` | âŒ ä¸€èˆ¬ä»…è®¡ç®— loss                    |\n",
    "| æ˜¯å¦æ”¯æŒ LoRA | âš™ï¸ éœ€æ‰‹åŠ¨æ•´åˆ PEFT                  | âœ… å†…ç½®æ”¯æŒ                          |\n",
    "| æ˜¯å¦æ”¯æŒé‡åŒ–    | âš™ï¸ æ‰‹åŠ¨é›†æˆ bitsandbytes           | âœ… è‡ªåŠ¨å…¼å®¹                          |\n",
    "| è¾“å‡ºç›®æ ‡      | æ–‡æœ¬åºåˆ—                           | ä¸‹ä¸€ä¸ª token æ¦‚ç‡ï¼ˆè¯­è¨€å»ºæ¨¡ï¼‰              |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© äº”ã€å…¶å®ƒå¸¸è§ Trainer ç±»ï¼ˆæŒ‰ä»»åŠ¡ç±»å‹åˆ†ï¼‰\n",
    "\n",
    "| ç±»å                 | åº“              | ç”¨é€”                                                     |\n",
    "| ------------------ | -------------- | ------------------------------------------------------ |\n",
    "| **Trainer**        | `transformers` | é€šç”¨è®­ç»ƒå™¨ï¼Œæ”¯æŒåˆ†ç±»ã€å›å½’ç­‰ä»»åŠ¡                                       |\n",
    "| **Seq2SeqTrainer** | `transformers` | Encoder-Decoder ç»“æ„ä»»åŠ¡ï¼ˆç¿»è¯‘/æ‘˜è¦ï¼‰                            |\n",
    "| **SFTTrainer**     | `trl`          | æŒ‡ä»¤å¾®è°ƒï¼ˆSFTï¼‰                                              |\n",
    "| **PPOTrainer**     | `trl`          | RLHFï¼ˆå¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼ŒChatGPT ç¬¬äºŒé˜¶æ®µï¼‰                              |\n",
    "| **DPOTrainer**     | `trl`          | ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDirect Preference Optimizationï¼‰ï¼ŒRLHF çš„ç®€åŒ–æ›¿ä»£      |\n",
    "| **ORPOTrainer**    | `trl`          | æ–°å‹å¯¹é½ç®—æ³•ï¼ˆOff-policy Regularized Preference Optimizationï¼‰ |\n",
    "| **RewardTrainer**  | `trl`          | è®­ç»ƒ Reward Modelï¼ˆç”¨äº RLHFï¼‰                               |\n",
    "| **Accelerator**    | `accelerate`   | åº•å±‚é€šç”¨åŠ é€Ÿå™¨ï¼Œç”¨äºè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯                                      |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  å…­ã€æ€»ç»“å¯¹ç…§è¡¨\n",
    "\n",
    "| ä»»åŠ¡ç±»å‹         | æ¨è Trainer                   | è¯´æ˜                 |\n",
    "| ------------ | ---------------------------- | ------------------ |\n",
    "| åˆ†ç±»/å›å½’        | `Trainer`                    | é€šç”¨æ¨¡å‹ï¼ˆBERTç­‰ï¼‰        |\n",
    "| ç¿»è¯‘/æ‘˜è¦        | `Seq2SeqTrainer`             | Encoder-Decoder ç»“æ„ |\n",
    "| æŒ‡ä»¤å¾®è°ƒ         | `SFTTrainer`                 | Chat/LLM å¾®è°ƒ        |\n",
    "| RLHF ç¬¬äºŒé˜¶æ®µ    | `PPOTrainer`                 | å¼ºåŒ–å­¦ä¹ å¾®è°ƒ             |\n",
    "| åå¥½ä¼˜åŒ–         | `DPOTrainer` / `ORPOTrainer` | æ›¿ä»£ RLHFï¼Œè®­ç»ƒæ›´ç®€å•      |\n",
    "| è®­ç»ƒ Reward æ¨¡å‹ | `RewardTrainer`              | ç”¨äº RLHF æµç¨‹ä¸­çš„å¥–åŠ±æ¨¡å‹   |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ é¢å¤–å»ºè®®\n",
    "\n",
    "* å¦‚æœä½ åœ¨åš **T5ã€BARTã€ç¿»è¯‘ã€æ‘˜è¦** â†’ ç”¨ `Seq2SeqTrainer`\n",
    "* å¦‚æœä½ åœ¨åš **LLaMAã€Mistralã€ChatGLM çš„æŒ‡ä»¤å¾®è°ƒ** â†’ ç”¨ `SFTTrainer`\n",
    "* å¦‚æœä½ åœ¨åš **ChatGPT å¼å¯¹é½è®­ç»ƒ** â†’ ç”¨ `SFTTrainer` + `PPOTrainer` / `DPOTrainer`\n",
    "\n",
    "---\n",
    "\n",
    "æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ ç”»ä¸€å¼ ã€ŒHugging Face Trainer ç±»ç”Ÿæ€å…³ç³»å›¾ã€ï¼ˆå±•ç¤º Trainer / Seq2SeqTrainer / SFTTrainer / PPOTrainer / DPOTrainer ä¹‹é—´çš„å±‚çº§å’Œæµç¨‹ï¼‰ï¼Ÿ\n",
    "è¿™å¼ å›¾èƒ½éå¸¸ç›´è§‚åœ°çœ‹å‡ºæ¯ä¸ªç±»åœ¨æ•´ä¸ªå¾®è°ƒæµç¨‹ä¸­çš„ä½ç½®ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a12ff-e691-4d74-bf78-213f789321bb",
   "metadata": {},
   "source": [
    "# 6. ä»‹ç» trl åº“\n",
    "\n",
    "å½“ç„¶å¯ä»¥ ğŸ‘\n",
    "ä¸‹é¢ç»™ä½ ä¸€ä¸ªç³»ç»Ÿè€Œå…¨é¢çš„ä»‹ç»ï¼Œå¸®åŠ©ä½ å½»åº•ç†è§£ **ğŸ¤— `trl`ï¼ˆTransformers Reinforcement Learningï¼‰** è¿™ä¸ªåº“ã€‚\n",
    "å®ƒæ˜¯ Hugging Face ç”¨äº **å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½å’Œå¾®è°ƒ** çš„å…³é”®ç»„ä»¶ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  ä¸€ã€ä»€ä¹ˆæ˜¯ `trl`\n",
    "\n",
    "> **`trl` = Transformers Reinforcement Learning**\n",
    "\n",
    "`trl` æ˜¯ Hugging Face æ¨å‡ºçš„ä¸€ä¸ªä¸“é—¨ç”¨äº **è®­ç»ƒã€å¾®è°ƒå’Œå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰** çš„åº“ã€‚\n",
    "å®ƒå°† **ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**ã€**å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰**ã€**åå¥½ä¼˜åŒ–ï¼ˆDPO/ORPOï¼‰** ç­‰å¤šç§å¯¹é½ç®—æ³•ç»Ÿä¸€åœ¨ä¸€ä¸ªæ¡†æ¶ä¸‹ã€‚\n",
    "\n",
    "ğŸ“¦ å®˜æ–¹ä»“åº“ï¼š[https://github.com/huggingface/trl](https://github.com/huggingface/trl)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© äºŒã€`trl` çš„å®šä½ï¼ˆä¸ Transformers çš„å…³ç³»ï¼‰\n",
    "\n",
    "| é¡¹ç›®              | ç”¨é€”                    | æ˜¯å¦ä¾èµ– Transformers |\n",
    "| --------------- | --------------------- | ----------------- |\n",
    "| ğŸ¤— Transformers | é€šç”¨æ¨¡å‹åŠ è½½ã€æ¨ç†ã€å¾®è°ƒæ¡†æ¶        | âœ… æ˜¯               |\n",
    "| ğŸ¤— TRL          | å¤§æ¨¡å‹çš„ **å¯¹é½ä¸å¼ºåŒ–å­¦ä¹ è®­ç»ƒ** æ¡†æ¶ | âœ… åŸºäº Transformers |\n",
    "| ğŸ¤— PEFT         | å‚æ•°é«˜æ•ˆå¾®è°ƒ (LoRA/Adapter) | âœ… å¯ä¸ TRL è”ç”¨       |\n",
    "| ğŸ¤— Accelerate   | åˆ†å¸ƒå¼è®­ç»ƒåŠ é€Ÿ               | âœ… è¢« TRL å†…éƒ¨è°ƒç”¨      |\n",
    "\n",
    "> ğŸ”— ç®€å•è¯´ï¼š\n",
    ">\n",
    "> * `transformers` è´Ÿè´£â€œæ¨¡å‹ç»“æ„å’Œè®­ç»ƒåŸºç¡€â€\n",
    "> * `trl` è´Ÿè´£â€œLLM å¯¹é½å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒâ€\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ ä¸‰ã€æ ¸å¿ƒåŠŸèƒ½æ¨¡å—\n",
    "\n",
    "`trl` çš„è®¾è®¡ç›®æ ‡æ˜¯æ”¯æŒ LLM ä» **SFT â†’ RLHF â†’ DPO/ORPO** çš„å…¨æµç¨‹ã€‚\n",
    "\n",
    "### 1ï¸âƒ£ **SFTTrainer â€” ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰**\n",
    "\n",
    "* ç›¸å½“äº ChatGPT è®­ç»ƒçš„ **ç¬¬ä¸€é˜¶æ®µ**\n",
    "* è®©æ¨¡å‹å­¦ä¼šâ€œæ ¹æ®æŒ‡ä»¤ç”Ÿæˆå›ç­”â€\n",
    "\n",
    "ğŸ§© ç¤ºä¾‹ï¼š\n",
    "\n",
    "```python\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    packing=True,\n",
    ")\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "âœ… ç‰¹ç‚¹ï¼š\n",
    "\n",
    "* æ”¯æŒ LoRA / QLoRA / é‡åŒ–è®­ç»ƒ\n",
    "* è‡ªåŠ¨æ‹¼æ¥æ ·æœ¬ï¼ˆpackingï¼‰æé«˜æ•ˆç‡\n",
    "* å¯ä¸ `datasets` æ— ç¼ç»“åˆ\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ **PPOTrainer â€” RLHFï¼ˆReinforcement Learning from Human Feedbackï¼‰**\n",
    "\n",
    "* ChatGPT è®­ç»ƒçš„ **ç¬¬äºŒé˜¶æ®µ**\n",
    "* ä½¿ç”¨å¥–åŠ±æ¨¡å‹ï¼ˆReward Modelï¼‰è¿›è¡Œå¼ºåŒ–å­¦ä¹ \n",
    "* ç®—æ³•åŸºäº **PPOï¼ˆProximal Policy Optimizationï¼‰**\n",
    "\n",
    "ğŸ§© ç¤ºä¾‹ï¼š\n",
    "\n",
    "```python\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "\n",
    "config = PPOConfig(model_name=\"gpt2\", batch_size=8)\n",
    "ppo_trainer = PPOTrainer(config=config, model=model, tokenizer=tokenizer)\n",
    "ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "```\n",
    "\n",
    "âœ… ç‰¹ç‚¹ï¼š\n",
    "\n",
    "* å®ç°äº† PPO ç®—æ³•\n",
    "* å¯ä½¿ç”¨è‡ªå®šä¹‰å¥–åŠ±æ¨¡å‹\n",
    "* æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒå’Œæ¢¯åº¦ç´¯ç§¯\n",
    "* å…¸å‹ç”¨äº RLHFï¼ˆChatGPT / InstructGPT ç¬¬äºŒé˜¶æ®µï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ **RewardTrainer â€” è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼ˆReward Modelï¼‰**\n",
    "\n",
    "* ChatGPT çš„ **å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰è®­ç»ƒé˜¶æ®µ**\n",
    "* é€šè¿‡â€œå¥½å›ç­” vs å·®å›ç­”â€æ ·æœ¬å¯¹æ¥å­¦ä¹ åå¥½åˆ†æ•°\n",
    "\n",
    "ğŸ§© ç¤ºä¾‹ï¼š\n",
    "\n",
    "```python\n",
    "from trl import RewardTrainer\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=reward_model,\n",
    "    args=training_args,\n",
    "    train_dataset=pairwise_dataset,\n",
    ")\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "âœ… ç‰¹ç‚¹ï¼š\n",
    "\n",
    "* è¾“å…¥æ˜¯ `(chosen, rejected)` å¯¹\n",
    "* è¾“å‡º reward åˆ†æ•°å·®å¼‚\n",
    "* ç”¨äº PPOTrainer çš„å¥–åŠ±æ¥æº\n",
    "\n",
    "---\n",
    "\n",
    "### 4ï¸âƒ£ **DPOTrainer â€” ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDirect Preference Optimizationï¼‰**\n",
    "\n",
    "* ä¸€ç§ **æ— éœ€å¼ºåŒ–å­¦ä¹ ** çš„ RLHF æ›¿ä»£ç®—æ³•\n",
    "* ç›´æ¥æœ€å°åŒ–å¥–åŠ±æ¨¡å‹éšå¼ç›®æ ‡ï¼Œè®­ç»ƒæ›´ç¨³å®šã€æ›´å¿«\n",
    "\n",
    "ğŸ§© ç¤ºä¾‹ï¼š\n",
    "\n",
    "```python\n",
    "from trl import DPOTrainer\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    train_dataset=pairwise_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "âœ… ç‰¹ç‚¹ï¼š\n",
    "\n",
    "* æ— éœ€æ˜¾å¼å¥–åŠ±æ¨¡å‹\n",
    "* è®¡ç®—ç®€å•ï¼Œç¨³å®šé«˜æ•ˆ\n",
    "* é€‚åˆå¿«é€Ÿå¯¹é½ LLM\n",
    "\n",
    "---\n",
    "\n",
    "### 5ï¸âƒ£ **ORPOTrainer â€” æ–°å‹å¯¹é½ç®—æ³•**\n",
    "\n",
    "* ORPO = **Off-policy Regularized Preference Optimization**\n",
    "* ä¸€ç§æ¯” DPO æ›´ç¨³çš„ RLHF æ›¿ä»£ç®—æ³•ï¼ˆHugging Face 2024 å¹´æ–°å¼•å…¥ï¼‰\n",
    "\n",
    "âœ… ç‰¹ç‚¹ï¼š\n",
    "\n",
    "* æ›´å¥½åœ°å¹³è¡¡å‚è€ƒæ¨¡å‹ä¸è®­ç»ƒæ¨¡å‹\n",
    "* æ— éœ€å¼ºåŒ–å­¦ä¹ å¾ªç¯\n",
    "* è¢«è®¤ä¸ºæ˜¯ DPO çš„æ”¹è¿›ç‰ˆæœ¬\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  å››ã€TRL è®­ç»ƒæµç¨‹æ€»è§ˆï¼ˆChatGPT å¼ï¼‰\n",
    "\n",
    "```\n",
    "æ•°æ®å‡†å¤‡\n",
    "   â†“\n",
    "SFTTrainerï¼ˆç›‘ç£å¾®è°ƒï¼‰\n",
    "   â†“\n",
    "RewardTrainerï¼ˆè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼‰\n",
    "   â†“\n",
    "PPOTrainer æˆ– DPOTrainerï¼ˆåå¥½ä¼˜åŒ– / RLHFï¼‰\n",
    "   â†“\n",
    "å¾—åˆ°å¯¹é½æ¨¡å‹ï¼ˆAligned Modelï¼‰\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ äº”ã€TRL çš„å…³é”®ä¼˜åŠ¿\n",
    "\n",
    "| ä¼˜åŠ¿                     | è¯´æ˜                                      |\n",
    "| ---------------------- | --------------------------------------- |\n",
    "| âœ… ä¸ Transformers å®Œå…¨å…¼å®¹  | å¯ç›´æ¥ä½¿ç”¨ Transformers æ¨¡å‹ä¸ Tokenizer        |\n",
    "| âš™ï¸ æ”¯æŒ PEFT / LoRA / é‡åŒ– | èŠ‚çœæ˜¾å­˜ã€æ”¯æŒå¤§æ¨¡å‹å¾®è°ƒ                            |\n",
    "| ğŸ§© æ¨¡å—åŒ–è®¾è®¡               | SFTã€Rewardã€PPOã€DPOã€ORPO å¯ç‹¬ç«‹ä½¿ç”¨           |\n",
    "| ğŸš€ é«˜åº¦è‡ªåŠ¨åŒ–               | å†…ç½®æ—¥å¿—ã€è¯„ä¼°ã€prompt æ ¼å¼åŒ–                      |\n",
    "| ğŸ§  ç¤¾åŒºä¸°å¯Œ                | æ”¯æŒ LLaMAã€Mistralã€Falconã€ChatGLM ç­‰ä¸»æµ LLM |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§° å…­ã€å®‰è£…ä¸ä¾èµ–\n",
    "\n",
    "```bash\n",
    "pip install trl\n",
    "# æ¨èä¸ transformers, datasets, peft, accelerate ä¸€èµ·ä½¿ç”¨\n",
    "pip install transformers datasets peft accelerate bitsandbytes\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© ä¸ƒã€TRL ä¸­ä¸»è¦ç±»ä¸ä½¿ç”¨åœºæ™¯å¯¹ç…§è¡¨\n",
    "\n",
    "| ç±»å              | åŠŸèƒ½      | å…¸å‹ä»»åŠ¡                      | å¯¹åº” ChatGPT é˜¶æ®µ |\n",
    "| --------------- | ------- | ------------------------- | ------------- |\n",
    "| `SFTTrainer`    | ç›‘ç£å¾®è°ƒ    | æŒ‡ä»¤å¾®è°ƒ (Instruction Tuning) | é˜¶æ®µ 1          |\n",
    "| `RewardTrainer` | å¥–åŠ±æ¨¡å‹è®­ç»ƒ  | æ¯”è¾ƒæ ·æœ¬åå¥½                    | é˜¶æ®µ 2          |\n",
    "| `PPOTrainer`    | å¼ºåŒ–å­¦ä¹ è®­ç»ƒ  | RLHF                      | é˜¶æ®µ 3          |\n",
    "| `DPOTrainer`    | ç›´æ¥åå¥½ä¼˜åŒ–  | æ— éœ€ RL çš„å¯¹é½                 | é˜¶æ®µ 3ï¼ˆæ›¿ä»£ PPOï¼‰  |\n",
    "| `ORPOTrainer`   | ç¨³å®šçš„åå¥½ä¼˜åŒ– | æ”¹è¿› DPO                    | é˜¶æ®µ 3ï¼ˆæ›¿ä»£ PPOï¼‰  |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© å…«ã€ä¸€ä¸ªå®Œæ•´çš„ RLHF æµç¨‹ç¤ºæ„å›¾\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      æ•°æ®é›†æ„å»º       â”‚\n",
    "â”‚  (æŒ‡ä»¤ + å›å¤æ ·æœ¬)   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   SFTTrainer å¾®è°ƒæ¨¡å‹ â”‚\n",
    "â”‚   (ç›‘ç£å­¦ä¹ )         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  RewardTrainer è®­ç»ƒå¥–åŠ±æ¨¡å‹ â”‚\n",
    "â”‚  (å­¦ä¹ åå¥½è¯„åˆ†)      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ PPOTrainer / DPOTrainer â”‚\n",
    "â”‚  (å¼ºåŒ–å­¦ä¹  / åå¥½ä¼˜åŒ–)  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â†“\n",
    "ğŸ† å¯¹é½åçš„è¯­è¨€æ¨¡å‹ï¼ˆAligned LLMï¼‰\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”® ä¹ã€æ€»ç»“\n",
    "\n",
    "| ç‰¹æ€§      | æè¿°                                                                       |\n",
    "| ------- | ------------------------------------------------------------------------ |\n",
    "| ğŸ“¦ åº“å   | `trl`                                                                    |\n",
    "| ğŸ§© åŠŸèƒ½   | ç›‘ç£å¾®è°ƒã€å¥–åŠ±å»ºæ¨¡ã€å¼ºåŒ–å­¦ä¹ ã€åå¥½ä¼˜åŒ–                                                      |\n",
    "| ğŸ’¬ ä¸»è¦ç±»  | `SFTTrainer`, `RewardTrainer`, `PPOTrainer`, `DPOTrainer`, `ORPOTrainer` |\n",
    "| ğŸ”— ä¾èµ–   | `transformers`, `peft`, `accelerate`, `datasets`                         |\n",
    "| ğŸ’¡ åº”ç”¨åœºæ™¯ | ChatGPT / LLaMA / Mistral ç­‰ LLM å¯¹é½è®­ç»ƒ                                     |\n",
    "\n",
    "---\n",
    "\n",
    "æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ ç”»ä¸€å¼ ã€ŒTRL ç”Ÿæ€ç»“æ„å›¾ã€ï¼Œå±•ç¤ºå„ä¸ª Trainerï¼ˆSFT/PPO/DPO/Rewardï¼‰ä¹‹é—´çš„ä¾èµ–å…³ç³»å’Œè®­ç»ƒæµç¨‹ï¼Ÿ\n",
    "é‚£å¼ å›¾èƒ½ä¸€ç›®äº†ç„¶åœ°çœ‹æ¸…æ•´ä¸ª ChatGPT å¼è®­ç»ƒç®¡çº¿ã€‚\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
