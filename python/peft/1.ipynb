{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24cb5476-819d-4e72-a41b-7ef8233eb15b",
   "metadata": {},
   "source": [
    "# 使用 peft 微调大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c682d06-bd5d-4a28-8946-d6d861708bb6",
   "metadata": {},
   "source": [
    "环境\n",
    "- Apple M4 Mac mini\n",
    "- Python 3.11.9\n",
    "- torch 2.9.0\n",
    "- transformers 4.57.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c0887-9ee1-4948-ad37-6f94a5194f5c",
   "metadata": {},
   "source": [
    "参考链接：\n",
    "- https://huggingface.co/docs/peft/quicktour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fecbc2-34e6-4a13-b31c-c156ee1b0d53",
   "metadata": {},
   "source": [
    "## 一、微调技术介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f640e0-5157-4742-ba70-6b6f8f3cd76a",
   "metadata": {},
   "source": [
    "**PEFT 是什么**\n",
    "\n",
    "PEFT = Parameter-Efficient Fine-Tuning，中文叫 **参数高效微调**。\n",
    "\n",
    "它是由 **Hugging Face** 推出的一个 Python 库，用来在不修改大模型全部参数的前提下，对模型进行轻量级微调。\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**为什么要用 PEFT？**\n",
    "\n",
    "以一个 70 亿参数的模型（7B）为例：\n",
    "\n",
    "* 全参数微调需要几十 GB 显存；\n",
    "* 训练时间长；\n",
    "* 模型保存也占很多空间。\n",
    "\n",
    "而 PEFT 通过“只训练部分参数”的方式（比如 LoRA 层），可以：\n",
    "\n",
    "* 让微调在 **消费级 GPU（如 RTX 4090 或 A100 40G）** 上完成；\n",
    "* 大幅节省内存和显存；\n",
    "* 保留模型的原始能力；\n",
    "* 输出一个非常小的“适配器权重”（几百 MB 甚至更少）。\n",
    "\n",
    "---\n",
    "\n",
    "**PEFT 的核心思想**\n",
    "\n",
    "PEFT 提供了几种 **参数高效微调方法** 的统一接口：\n",
    "\n",
    "| 方法                             | 原理                    | 适用场景         |\n",
    "| ------------------------------ | --------------------- | ------------ |\n",
    "| **LoRA** (Low-Rank Adaptation) | 在模型的线性层插入可训练的低秩矩阵     | 最常用；大模型任务通用  |\n",
    "| **Prefix Tuning**              | 给模型输入加上可训练的“提示向量”     | 少样本学习、NLP    |\n",
    "| **Prompt Tuning**              | 学习一小段 token embedding | Prompt 工程自动化 |\n",
    "| **P-Tuning v2**                | 比 Prompt Tuning 更强的版本 | 复杂任务         |\n",
    "| **AdaLoRA**                    | 动态调整 LoRA rank 大小     | 提高效率与性能平衡    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d069d2-e4ec-4a43-8c10-9d0d41abe387",
   "metadata": {},
   "source": [
    "## 二、准备工作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3621af92-cc3c-4f05-a9b4-5e4f1ce46ea7",
   "metadata": {},
   "source": [
    "**确认依赖库版本**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f7d11b9-56ac-4b81-839c-e03cfadea30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.3.2', '2.9.0', '4.57.1')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, torch, transformers\n",
    "np.__version__, torch.__version__, transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9062bc1-17a3-4ce9-ab2c-8851e2c9ea27",
   "metadata": {},
   "source": [
    "**关闭多线程分词，避免多进程冲突警告**。\n",
    "\n",
    "Hugging Face 的 tokenizers 库在后台使用 Rust 多线程加速（并行分词）。\n",
    "但是，当 Python 的 multiprocessing 在已经启用了并行的情况下调用 fork() 时，会触发这个警告，以避免潜在的死锁。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "600a81ac-50c6-4367-ad12-7f5ef781e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc29b4e4-79c9-4302-b3c2-694c52aca835",
   "metadata": {},
   "source": [
    "**允许 MPS 不支持的算子自动 fallback 到 CPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe4ab8b-b356-400e-8f7d-161241ed808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12258958-027b-48fe-99b5-511419465752",
   "metadata": {},
   "source": [
    "**定义 device，在 MAC 上使用 mps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58a1b6ad-fd33-4862-a63c-728f2295e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41b79e5-7b13-482b-86ae-f6a7d4792ac4",
   "metadata": {},
   "source": [
    "**开启训练详细日志，以便查看训练细节**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb5b9c62-e358-42c6-b915-5471d2d81dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers.trainer\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af083a78-ca1e-474b-bb04-ecdb6c6a7e51",
   "metadata": {},
   "source": [
    "## 三、基础模型测试\n",
    "\n",
    "用默认的 bigscience/mt0 模型测试一下英语翻译成中文的效果\n",
    "\n",
    "`bigscience/mt0-large` 介绍\n",
    "\n",
    "| 项目       | 内容                                                                               |\n",
    "| -------- | -------------------------------------------------------------------------------- |\n",
    "| **模型名称** | `bigscience/mt0-large`                                                           |\n",
    "| **来源**   | [BigScience Project (Hugging Face)](https://huggingface.co/bigscience/mt0-large) |\n",
    "| **模型类型** | **T5 结构（Encoder-Decoder）多语言序列到序列模型**                                             |\n",
    "| **参数量**  | 1.2B 参数（中型）                                                                      |\n",
    "| **基础模型** | MT5（Multilingual T5）                                                             |\n",
    "| **支持语言** | 100 多种语言，包括英语、中文、法语、西班牙语、阿拉伯语、俄语等                                                |\n",
    "| **任务类型** | Text-to-text（可用于翻译、分类、摘要、问答、指令生成等）                                               |\n",
    "| **训练方式** | 指令微调（Instruction-tuned），在多语言指令数据上训练                                              |\n",
    "| **特点**   | - 强多语言理解能力                                                                       |\n",
    "|          | - 已经进行过指令调优（比原始 mT5 更易用）                                                         |\n",
    "|          | - 支持通用文本生成任务                                                                     |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "`mt0-large` 太大，建议在 Mac 上从小版本开始微调：\n",
    "\n",
    "| 模型名称                   | 参数量   | 是否多语言 | 适合 Mac?  |\n",
    "| ---------------------- | ----- | ----- | -------- |\n",
    "| `bigscience/mt0-small` | ~300M | ✅     | ✅ 强烈推荐   |\n",
    "| `bigscience/mt0-base`  | ~580M | ✅     | ✅ 可行     |\n",
    "| `bigscience/mt0-large` | 1.2B  | ✅     | ⚠️ 可能超内存 |\n",
    "| `bigscience/mt0-xl`    | 3.7B  | ✅     | ❌ 不建议    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "683a7fec-785b-4679-8b92-1bd303950d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对于那些被逐出的老人来说,这些老城的人口稀少,尤其是老人、老人和孩子,通常会面临严重的社会问题。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"bigscience/mt0-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "\n",
    "text = '''Translate English to Chinese: \n",
    "The difficult access to these older sites for displaced persons gives rise to \n",
    "concerns about the living conditions of the people who are residents there, \n",
    "in particular widows, the elderly and children, \n",
    "who often live in intolerable hardship.'''\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=1024)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1039b3bd-537e-4cb1-bda8-db4a246b0ccf",
   "metadata": {},
   "source": [
    "## 四、微调训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba10a25-cbf2-4f41-b108-57b780405907",
   "metadata": {},
   "source": [
    "### 1、创建 LoRa 配置\n",
    "\n",
    "每个 PEFT 方法都由一个 PeftConfig 类定义，该类存储构建 PeftModel 所需的所有重要参数。\n",
    "\n",
    "例如，要使用 LoRa 进行训练，请加载并创建一个 LoraConfig 类。\n",
    "- task_type ：要训练的任务，要与模型类型匹配（本例中为序列到序列语言建模）\n",
    "  - `SEQ_CLS` → 序列分类（BERT 类）\n",
    "  - `CAUSAL_LM` → 自回归语言模型（GPT 类）\n",
    "  - `SEQ_2_SEQ_LM` → 序列到序列（T5、BART 类）\n",
    "- inference_mode ：是否使用该模型进行推理\n",
    "- r ：低秩矩阵的维数，正常值 8， 小内存设置成 4\n",
    "- lora_alpha ：低秩矩阵的缩放因子，正常值 32， 小内存设置成 8\n",
    "- lora_dropout ：LoRa 层的 dropout 概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8d297e5-e2f3-428d-8eda-fc9ee375f0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM,  inference_mode=False, r=4, lora_alpha=8, lora_dropout=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee869fa-4979-4651-92a7-6d582e909a79",
   "metadata": {},
   "source": [
    "### 2、创建 PEFT 模型\n",
    "LoraConfig 设置完成后，使用 get_peft_model() 函数创建一个 PeftModel 。\n",
    "\n",
    "该函数接受一个基础模型（可以从 Transformers 库加载）和一个 LoraConfig， 其中包含用于配置模型以使用 LoRA 进行训练的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9292bf7-8b89-4a5b-98e1-45196a9ed5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 442,368 || all params: 582,843,648 || trainable%: 0.0759\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e1d0e-fc08-4c33-aa23-7aaf33f187b3",
   "metadata": {},
   "source": [
    "可以看到总共 582M 参数，可训练的参数有 7%。\n",
    "\n",
    "现在你可以使用 Transformers Trainer 、Accelerate 或任何自定义的 PyTorch 训练循环来训练模型了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2656a75f-f21b-4783-ac23-d258643aa16c",
   "metadata": {},
   "source": [
    "### 3、准备训练数据集\n",
    "\n",
    "opus100 是一个支持 100 种语言对翻译的数据集，我们选择英文翻译成中文的子集，并且为了降低内存使用量，我们只使用 1% 的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "565dd5f6-96c1-4b31-8e7a-ff8a4579626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"opus100\", \"en-zh\", split=\"train[:1%]\").train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d08983-d57a-4341-b22b-2dac1cf27614",
   "metadata": {},
   "source": [
    "查看数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "483c2ab3-cba1-4023-8d66-c052730cb6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03293237-0dc2-484a-8986-570ced432883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': [{'en': 'I order you!', 'zh': '我命令你回来！'},\n",
       "  {'en': 'Look, if she comes here just let me know or, or get her to ring me or, or something',\n",
       "   'zh': '就通知我,或請她打給我'},\n",
       "  {'en': 'Consequently, it occurs to the Nigerian delegation that it might be wise for us not to proceed to take action on the present draft amendment, and I would therefore urge my Canadian friends and the other sponsors of that draft to tarry a little and let us, outside the premises of this grand Hall — perhaps in one of our other smaller rooms, under one of the Vice-Presidents of the General Assembly — meet and further reflect on this draft instead of forcing it to a vote.',\n",
       "   'zh': '当然，在尼日利亚代表团看来，我们或许以不就当前的修订草案开始采取行动为宜，为此，我敦促加拿大的朋友和该草案其他提案国再等一等，让我们在此大会议厅外、或许在我们其他较小的会议室里，在大会副主席的主持下聚在一起，对草案再作考量，而不是硬着头皮进行表决。'}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab65d0d-1242-4f64-82bc-12e3e33edec5",
   "metadata": {},
   "source": [
    "**其它数据集介绍**\n",
    "\n",
    "| 任务类型 | 推荐数据集                                        | 语言支持 |\n",
    "| ---- | -------------------------------------------- | ---- |\n",
    "| 分类   | XNLI, PAWS-X, Amazon Reviews                 | 多语言  |\n",
    "| 翻译   | OPUS100, Flores200, WMT                      | 多语言  |\n",
    "| 摘要   | MLSUM, WikiLingua                            | 多语言  |\n",
    "| 问答   | TyDi QA, XQuAD, SQuAD                        | 多语言  |\n",
    "| 指令   | FLAN, SuperNI, OpenOrca, Alpaca-Multilingual | 多语言  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90fecfc-a90a-4c87-a6e3-e2101d5020ce",
   "metadata": {},
   "source": [
    "### 4、对数据进行预处理\n",
    "\n",
    "定义预处理函数：把原始文本构造成给模型的 prompt，并进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68d7b276-60d7-4d26-880e-71728b3199e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    # 构造输入 prompt（mt0 是 instruction-style，因此加上任务说明）   \n",
    "    inputs = [f\"translate English to Chinese: {ex['en']}\" for ex in examples[\"translation\"]] # 把每个英文句子包装成翻译指令\n",
    "    targets = [ex[\"zh\"] for ex in examples[\"translation\"]]\n",
    "\n",
    "    # 使用 tokenizer 对 inputs 做编码（截断 + padding 到 max_length）\n",
    "    model_inputs = tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=128)  # 编码输入，统一长度\n",
    "\n",
    "    # 对目标文本进行编码，得到 labels\n",
    "    labels = tokenizer(targets, truncation=True, padding=\"max_length\", max_length=128)  # 编码目标文本\n",
    "\n",
    "    # 将 labels 的 pad token 替换为 -100，这是 transformers 损失函数忽略的标记\n",
    "    # 这样在计算交叉熵时，对 pad 部分不会贡献损失\n",
    "    labels_input_ids = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in label] for label in labels[\"input_ids\"]\n",
    "    ]  # 将 labels 中的 pad id 转为 -100\n",
    "\n",
    "    # 把处理好的 labels 放回 model_inputs 字典，Trainer 会读取 \"labels\" 字段用于计算 loss\n",
    "    model_inputs[\"labels\"] = labels_input_ids  # 为返回的数据添加 labels\n",
    "\n",
    "    # 返回最终的编码后的输入字典（包含 input_ids, attention_mask, labels）\n",
    "    return model_inputs  # 返回每个样本的字典"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47223466-9198-400a-997b-0c82b9c94195",
   "metadata": {},
   "source": [
    "把预处理函数映射到整个数据集（batched=True 表示按批次处理以加速）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15884554-b053-4f7b-9d4e-36bedb451550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768d6d57d74a45c090d74567d33e85d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4982ae500074e8b86d75cce77303312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(preprocess, batched=True)  # 对 dataset 的每个 split 应用 preprocess 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0012d8b1-31b3-4605-8a33-364479fdda47",
   "metadata": {},
   "source": [
    "查看预处理后的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f5c3d2d-948c-41f6-a0e0-a467040c3e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e075faa-e9ec-4a36-8cfb-6c5ddf1bca55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: translate English to Chinese: I order you!\n",
      "Label: 我命令你回来!\n"
     ]
    }
   ],
   "source": [
    "example = tokenized_datasets[\"train\"][0]\n",
    "print(\"Input:\", tokenizer.decode(example[\"input_ids\"], skip_special_tokens=True))\n",
    "print(\"Label:\", tokenizer.decode([id for id in example[\"labels\"] if id != -100], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e4840d-0d9c-4d58-bb16-9993c0f01fe9",
   "metadata": {},
   "source": [
    "### 5、定义评测指标\n",
    "\n",
    "`SacreBLEU` 是一个标准化的 **BLEU（Bilingual Evaluation Understudy）** 实现，由 **Matt Post (2018)** 提出。\n",
    "它的目标是让 BLEU 评测更 **可复现（reproducible）**、更 **一致（consistent）**。\n",
    "\n",
    "传统 BLEU 实现存在很多不一致的问题（例如 tokenization、大小写处理等），导致论文之间的 BLEU 分数无法直接比较。\n",
    "`sacrebleu` 通过**内置标准化的分词器、预处理规则、版本控制**等手段解决了这一痛点。\n",
    "\n",
    "`evaluate` 是 Hugging Face evaluate 库，用于加载评测指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6b4f038-b7c0-4978-a2cf-8efa73abc25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"sacrebleu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ceb06d-c761-49a7-b68d-7caf13f572ab",
   "metadata": {},
   "source": [
    "定义 compute_metrics，用于 Trainer 在 eval 时计算指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7294095-f047-4d84-9de1-b1cb2667bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    # eval_pred 的典型形式是 (predictions, labels)；这里 predictions 是生成器输出的 token id 矩阵\n",
    "    preds, labels = eval_pred  # 解包预测结果和标签\n",
    "\n",
    "    # 如果 predictions 是 logits（非生成模式），则需要取 argmax；但 predict_with_generate=True 时 preds 已是 token id\n",
    "    if isinstance(preds, tuple):  # 万一 preds 是 tuple（某些版本返回 logits 等），我们取第一个\n",
    "        preds = preds[0]\n",
    "\n",
    "    # 将预测中的 -100（如果有的话）替换为 tokenizer.pad_token_id，便于 decode\n",
    "    # 但正常生成结果里不应该有 -100，这里是稳健处理\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)  # 将 -100 换为 pad id（以便 decode）\n",
    "\n",
    "    # 把预测和真实标签解码成字符串（跳过 special tokens）\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)  # 解码预测文本\n",
    "    # labels 里仍可能有 -100，需要把它们换成 pad_token_id 再解码\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)  # 将 labels 中的 -100 替回 pad id\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)  # 解码真实文本\n",
    "\n",
    "    # sacrebleu 的 compute 需要 references 是 list of list 的形式：[[ref1], [ref2], ...]\n",
    "    references = [[l] for l in decoded_labels]  # 为每个样本创建单一引用列表\n",
    "\n",
    "    # 调用 sacrebleu 指标进行计算并返回结果字典（sacrebleu 返回 'score' 键表示 BLEU）\n",
    "    result = metric.compute(predictions=decoded_preds, references=references)  # 计算 sacrebleu 分数\n",
    "    # 将结果包装成 dict 返回，Trainer 会把它显示在 eval 输出里\n",
    "    return {\"bleu\": result[\"score\"]}  # 返回一个包含 BLEU 分数的字典"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca7f790-7ca2-45d0-884d-db0cf6e51ed7",
   "metadata": {},
   "source": [
    "**其他常见机器翻译 / 文本生成评测指标**\n",
    "\n",
    "| 指标                              | 类型               | 核心思想                | 适用场景         | 优点         | 缺点           |\n",
    "| ------------------------------- | ---------------- | ------------------- | ------------ | ---------- | ------------ |\n",
    "| **BLEU / SacreBLEU**            | 基于词汇匹配           | n-gram 精度 + 长度惩罚    | 机器翻译、文本生成    | 标准、稳定、可复现  | 忽略语义，不敏感于同义词 |\n",
    "| **CHRF / CHRF++**               | 基于字符匹配           | 计算字符 n-gram F-score | 低资源语言、形态丰富语言 | 对拼写变化更鲁棒   | 难以解释、不直观     |\n",
    "| **METEOR**                      | 基于词匹配 + 词形 + 同义词 | 考虑词形还原与同义词匹配        | 翻译评估         | 对语义变化更敏感   | 实现复杂、较慢      |\n",
    "| **TER (Translation Edit Rate)** | 基于编辑距离           | 最少编辑操作数 / 参考长度      | 机器翻译         | 易解释（与人工一致） | 不考虑意义，只看编辑操作 |\n",
    "| **ROUGE**                       | 基于召回率的 n-gram 匹配 | 比较生成文本与参考摘要的重叠      | 摘要生成         | 标准指标，通用性高  | 对词序、语义不敏感    |\n",
    "| **BERTScore**                   | 语义级指标            | 基于 BERT 词向量相似度      | 文本生成、翻译、对话   | 考虑语义相似     | 计算成本高，需要模型   |\n",
    "| **COMET / BLEURT**              | 学习型指标            | 使用神经网络学习人工评估分数      | 高级 MT 评测     | 与人类一致性高    | 需 GPU，训练依赖大  |\n",
    "\n",
    "---\n",
    "\n",
    "**选型建议（适用场景）**\n",
    "\n",
    "| 场景               | 推荐指标                             | 理由           |\n",
    "| ---------------- | -------------------------------- | ------------ |\n",
    "| **机器翻译标准基线**     | `sacrebleu`                      | 标准、易对比、被广泛使用 |\n",
    "| **形态复杂或低资源语言**   | `chrf` / `chrf++`                | 字符级鲁棒        |\n",
    "| **生成摘要/对话**      | `rouge` + `bertscore`            | 兼顾召回与语义      |\n",
    "| **高质量语义评测**      | `bertscore` / `comet` / `bleurt` | 与人类评判最一致     |\n",
    "| **人工评测替代（神经模型）** | `comet`                          | 最新一代、语义敏感    |\n",
    "\n",
    "---\n",
    "\n",
    "**总结**\n",
    "\n",
    "| 类别         | 示例指标                            | 关键特点            |\n",
    "| ---------- | ------------------------------- | --------------- |\n",
    "| **基于表面匹配** | BLEU / SacreBLEU / ROUGE / CHRF | 快速、解释性强，但不懂语义   |\n",
    "| **基于语义嵌入** | BERTScore / COMET / BLEURT      | 能理解同义、语境，接近人工判断 |\n",
    "| **基于编辑距离** | TER                             | 可解释性强，但不智能      |\n",
    "| **综合指标**   | COMETKiwi / GEMBA               | 结合语言模型和上下文语义    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4082f25-a56d-4316-8bc6-5f0215c22452",
   "metadata": {},
   "source": [
    "### 6、定义训练参数\n",
    "\n",
    "为了能在单机上进行微调，有一些参数要特殊设置\n",
    "\n",
    "| 优化项        | 原因                         | 建议值          |\n",
    "| ---------- | -------------------------- | ------------ |\n",
    "| 模型大小       | 减少显存占用                     | `mt0-base`   |\n",
    "| Batch size | 避免内存溢出                     | 1            |\n",
    "| LoRA 参数    | 进一步轻量化                     | r=4, alpha=8 |\n",
    "| 精度         | MPS 不支持 fp16               | 使用默认 fp32    |\n",
    "| 数据量        | 控制训练规模                     | 1~5% 数据即可测试  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47f788e1-f559-4c53-bff0-bed2f9be80e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mt0-lora\",                  # 模型输出和检查点保存目录\n",
    "    per_device_train_batch_size=1,            # 每个 GPU/设备上的训练 batch size（根据显存调整）, Mac上必须非常小，否则内存爆\n",
    "    per_device_eval_batch_size=1,             # 验证时的 batch size\n",
    "    gradient_accumulation_steps=8,            # 把 batch size 设为 1，用梯度累积模拟更大 batch 模拟 batch size=8\n",
    "    learning_rate=5e-5,                       # 学习率，默认5e-5，内存小调低学习率\n",
    "    num_train_epochs=1,                       # 训练轮次，#先跑1个epoch测试稳定性\n",
    "    eval_strategy=\"epoch\",                    # 评估频率（这里设置为每个 epoch 评估一次）\n",
    "    predict_with_generate=True,               # 在评估时使用 generate() 来产生文本（必需用于语言生成任务）\n",
    "    save_strategy=\"epoch\",                    # epoch 表示每个 epoch 保存一次checkpoint），不能设置为 no 来降低内存，必须和 eval_strategy 一致\n",
    "    logging_strategy=\"steps\",                 # 日志策略（按 steps）\n",
    "    logging_steps=10,                         # 每多少 step 打印一次日志\n",
    "    fp16=False,                               # 如果支持则开启半精度训练以节省显存（需要 CUDA + 支持）#  MPS 不支持 fp16\n",
    "    load_best_model_at_end=True,              # 在训练结束加载最佳验证结果对应的模型\n",
    "    dataloader_pin_memory=False,              # 在 CUDA 上能让 DataLoader 更快地把数据拷贝到 GPU。MPS 不支持\n",
    "    metric_for_best_model=\"bleu\",             # 根据哪个指标判断最佳模型\n",
    "    report_to=\"none\",                         # 不向wandb等发送日志\n",
    "    log_level=\"info\",                         # 输出日志到控制台\n",
    "    log_level_replica=\"info\",                 # 对多进程安全\n",
    "    disable_tqdm=False,                       # 禁用进度条，防止日志被覆盖\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a59d0a-051f-4412-98c0-251563ca53f8",
   "metadata": {},
   "source": [
    "### 7、创建训练器\n",
    "\n",
    "这里 eval_dataset 尝试使用 validation，若数据集没有 validation split，会使用 test。在真实训练里建议准备独立的 validation。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb94df67-e709-4a65-83b3-baecaac8720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                              # 注入（已包装 LoRA 的）模型\n",
    "    args=training_args,                       # 训练参数\n",
    "    train_dataset=tokenized_datasets[\"train\"],# 训练集\n",
    "    eval_dataset=tokenized_datasets[\"validation\"] if \"validation\" in tokenized_datasets else tokenized_datasets[\"test\"], \n",
    "    processing_class=tokenizer,               # tokenizer（用于 decode/generation 时）\n",
    "    compute_metrics=compute_metrics,          # 评估时要计算的指标函数\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85d1db-a116-4d94-a58b-1b56292ca916",
   "metadata": {},
   "source": [
    "添加打印日志的回调，一般来说已经添加了，这里为了显示训练过程中的细节，显式的添加一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7766c131-9729-490f-9f97-9c781e034ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_callback import PrinterCallback\n",
    "trainer.add_callback(PrinterCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d1f0a-77b3-4c87-9502-0f210c17894a",
   "metadata": {},
   "source": [
    "### 8、启动训练 \n",
    "\n",
    "运行训练过程；训练期间会按 training_args 指定的策略保存 checkpoint 并在 eval 时计算指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ae02949-d0c6-46a5-95f4-f31f953758e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Training set don't have a corresponding argument in `PeftModelForSeq2SeqLM.forward` and have been ignored: translation. If translation are not expected by `PeftModelForSeq2SeqLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 9,000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 1,125\n",
      "  Number of trainable parameters = 442,368\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1125/1125 31:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.537800</td>\n",
       "      <td>3.218850</td>\n",
       "      <td>10.728505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0011, 'grad_norm': 0.2660370469093323, 'learning_rate': 4.96e-05, 'epoch': 0.008888888888888889}\n",
      "{'loss': 4.116, 'grad_norm': 0.34418511390686035, 'learning_rate': 4.915555555555556e-05, 'epoch': 0.017777777777777778}\n",
      "{'loss': 4.2667, 'grad_norm': 0.35320037603378296, 'learning_rate': 4.871111111111111e-05, 'epoch': 0.02666666666666667}\n",
      "{'loss': 3.8212, 'grad_norm': 0.5087679028511047, 'learning_rate': 4.826666666666667e-05, 'epoch': 0.035555555555555556}\n",
      "{'loss': 4.1358, 'grad_norm': 1.3656669855117798, 'learning_rate': 4.782222222222222e-05, 'epoch': 0.044444444444444446}\n",
      "{'loss': 4.0941, 'grad_norm': 0.6689131855964661, 'learning_rate': 4.737777777777778e-05, 'epoch': 0.05333333333333334}\n",
      "{'loss': 4.158, 'grad_norm': 0.5645339488983154, 'learning_rate': 4.6933333333333333e-05, 'epoch': 0.06222222222222222}\n",
      "{'loss': 3.681, 'grad_norm': 0.4290207326412201, 'learning_rate': 4.648888888888889e-05, 'epoch': 0.07111111111111111}\n",
      "{'loss': 3.9938, 'grad_norm': 0.39746496081352234, 'learning_rate': 4.6044444444444445e-05, 'epoch': 0.08}\n",
      "{'loss': 4.1354, 'grad_norm': 0.5381544828414917, 'learning_rate': 4.5600000000000004e-05, 'epoch': 0.08888888888888889}\n",
      "{'loss': 4.0008, 'grad_norm': 0.5252560973167419, 'learning_rate': 4.5155555555555556e-05, 'epoch': 0.09777777777777778}\n",
      "{'loss': 4.3176, 'grad_norm': 0.8325200080871582, 'learning_rate': 4.4711111111111115e-05, 'epoch': 0.10666666666666667}\n",
      "{'loss': 4.0663, 'grad_norm': 0.6570723056793213, 'learning_rate': 4.426666666666667e-05, 'epoch': 0.11555555555555555}\n",
      "{'loss': 4.1322, 'grad_norm': 0.6117323637008667, 'learning_rate': 4.3822222222222227e-05, 'epoch': 0.12444444444444444}\n",
      "{'loss': 3.9441, 'grad_norm': 0.7769255638122559, 'learning_rate': 4.337777777777778e-05, 'epoch': 0.13333333333333333}\n",
      "{'loss': 3.7743, 'grad_norm': 0.7211622595787048, 'learning_rate': 4.293333333333334e-05, 'epoch': 0.14222222222222222}\n",
      "{'loss': 3.8168, 'grad_norm': 0.609197199344635, 'learning_rate': 4.248888888888889e-05, 'epoch': 0.1511111111111111}\n",
      "{'loss': 4.1088, 'grad_norm': 0.7244412899017334, 'learning_rate': 4.204444444444445e-05, 'epoch': 0.16}\n",
      "{'loss': 3.9274, 'grad_norm': 0.7003903985023499, 'learning_rate': 4.16e-05, 'epoch': 0.1688888888888889}\n",
      "{'loss': 3.9925, 'grad_norm': 1.0535649061203003, 'learning_rate': 4.115555555555556e-05, 'epoch': 0.17777777777777778}\n",
      "{'loss': 4.023, 'grad_norm': 0.7679335474967957, 'learning_rate': 4.071111111111111e-05, 'epoch': 0.18666666666666668}\n",
      "{'loss': 3.943, 'grad_norm': 0.6612690687179565, 'learning_rate': 4.026666666666667e-05, 'epoch': 0.19555555555555557}\n",
      "{'loss': 4.0191, 'grad_norm': 0.9727287888526917, 'learning_rate': 3.9822222222222224e-05, 'epoch': 0.20444444444444446}\n",
      "{'loss': 3.7357, 'grad_norm': 0.7918069362640381, 'learning_rate': 3.937777777777778e-05, 'epoch': 0.21333333333333335}\n",
      "{'loss': 4.008, 'grad_norm': 0.9532528519630432, 'learning_rate': 3.8933333333333336e-05, 'epoch': 0.2222222222222222}\n",
      "{'loss': 3.9514, 'grad_norm': 0.6290379762649536, 'learning_rate': 3.848888888888889e-05, 'epoch': 0.2311111111111111}\n",
      "{'loss': 3.9451, 'grad_norm': 1.0980257987976074, 'learning_rate': 3.804444444444445e-05, 'epoch': 0.24}\n",
      "{'loss': 3.607, 'grad_norm': 0.7965654134750366, 'learning_rate': 3.76e-05, 'epoch': 0.24888888888888888}\n",
      "{'loss': 3.8912, 'grad_norm': 0.7548227310180664, 'learning_rate': 3.715555555555555e-05, 'epoch': 0.2577777777777778}\n",
      "{'loss': 3.7047, 'grad_norm': 0.7746040225028992, 'learning_rate': 3.671111111111111e-05, 'epoch': 0.26666666666666666}\n",
      "{'loss': 4.0965, 'grad_norm': 0.9763217568397522, 'learning_rate': 3.626666666666667e-05, 'epoch': 0.27555555555555555}\n",
      "{'loss': 3.764, 'grad_norm': 0.9020406007766724, 'learning_rate': 3.582222222222222e-05, 'epoch': 0.28444444444444444}\n",
      "{'loss': 3.6233, 'grad_norm': 0.8398830890655518, 'learning_rate': 3.537777777777778e-05, 'epoch': 0.29333333333333333}\n",
      "{'loss': 3.7407, 'grad_norm': 1.1059136390686035, 'learning_rate': 3.493333333333333e-05, 'epoch': 0.3022222222222222}\n",
      "{'loss': 3.5565, 'grad_norm': 1.1085898876190186, 'learning_rate': 3.448888888888889e-05, 'epoch': 0.3111111111111111}\n",
      "{'loss': 3.494, 'grad_norm': 0.7901690602302551, 'learning_rate': 3.4044444444444445e-05, 'epoch': 0.32}\n",
      "{'loss': 4.0272, 'grad_norm': 0.7892969846725464, 'learning_rate': 3.3600000000000004e-05, 'epoch': 0.3288888888888889}\n",
      "{'loss': 3.5412, 'grad_norm': 0.8494188785552979, 'learning_rate': 3.3155555555555556e-05, 'epoch': 0.3377777777777778}\n",
      "{'loss': 3.8824, 'grad_norm': 0.6788021922111511, 'learning_rate': 3.2711111111111115e-05, 'epoch': 0.3466666666666667}\n",
      "{'loss': 3.6897, 'grad_norm': 1.6140516996383667, 'learning_rate': 3.226666666666667e-05, 'epoch': 0.35555555555555557}\n",
      "{'loss': 3.6532, 'grad_norm': 2.0105478763580322, 'learning_rate': 3.1822222222222226e-05, 'epoch': 0.36444444444444446}\n",
      "{'loss': 3.4881, 'grad_norm': 0.7135293483734131, 'learning_rate': 3.137777777777778e-05, 'epoch': 0.37333333333333335}\n",
      "{'loss': 3.9315, 'grad_norm': 1.1216226816177368, 'learning_rate': 3.093333333333334e-05, 'epoch': 0.38222222222222224}\n",
      "{'loss': 3.6543, 'grad_norm': 0.827518880367279, 'learning_rate': 3.048888888888889e-05, 'epoch': 0.39111111111111113}\n",
      "{'loss': 3.6785, 'grad_norm': 0.9024472236633301, 'learning_rate': 3.004444444444445e-05, 'epoch': 0.4}\n",
      "{'loss': 3.6299, 'grad_norm': 0.8396804928779602, 'learning_rate': 2.96e-05, 'epoch': 0.4088888888888889}\n",
      "{'loss': 3.7687, 'grad_norm': 1.1383917331695557, 'learning_rate': 2.9155555555555557e-05, 'epoch': 0.4177777777777778}\n",
      "{'loss': 3.5719, 'grad_norm': 0.8629504442214966, 'learning_rate': 2.8711111111111113e-05, 'epoch': 0.4266666666666667}\n",
      "{'loss': 3.6443, 'grad_norm': 0.9419986009597778, 'learning_rate': 2.8266666666666668e-05, 'epoch': 0.43555555555555553}\n",
      "{'loss': 3.8004, 'grad_norm': 0.8388625979423523, 'learning_rate': 2.782222222222222e-05, 'epoch': 0.4444444444444444}\n",
      "{'loss': 3.3734, 'grad_norm': 1.0022343397140503, 'learning_rate': 2.737777777777778e-05, 'epoch': 0.4533333333333333}\n",
      "{'loss': 3.6198, 'grad_norm': 1.1884604692459106, 'learning_rate': 2.6933333333333332e-05, 'epoch': 0.4622222222222222}\n",
      "{'loss': 3.7431, 'grad_norm': 0.9221943020820618, 'learning_rate': 2.648888888888889e-05, 'epoch': 0.4711111111111111}\n",
      "{'loss': 3.4846, 'grad_norm': 0.8202416300773621, 'learning_rate': 2.6044444444444443e-05, 'epoch': 0.48}\n",
      "{'loss': 3.8045, 'grad_norm': 0.9099463820457458, 'learning_rate': 2.5600000000000002e-05, 'epoch': 0.4888888888888889}\n",
      "{'loss': 3.7075, 'grad_norm': 0.9225313663482666, 'learning_rate': 2.5155555555555555e-05, 'epoch': 0.49777777777777776}\n",
      "{'loss': 4.0631, 'grad_norm': 1.0993441343307495, 'learning_rate': 2.4711111111111114e-05, 'epoch': 0.5066666666666667}\n",
      "{'loss': 3.7731, 'grad_norm': 0.9938179850578308, 'learning_rate': 2.426666666666667e-05, 'epoch': 0.5155555555555555}\n",
      "{'loss': 3.5476, 'grad_norm': 0.8456200361251831, 'learning_rate': 2.3822222222222225e-05, 'epoch': 0.5244444444444445}\n",
      "{'loss': 3.8906, 'grad_norm': 1.1552603244781494, 'learning_rate': 2.337777777777778e-05, 'epoch': 0.5333333333333333}\n",
      "{'loss': 3.8428, 'grad_norm': 1.0858603715896606, 'learning_rate': 2.2933333333333333e-05, 'epoch': 0.5422222222222223}\n",
      "{'loss': 3.4585, 'grad_norm': 1.0949122905731201, 'learning_rate': 2.248888888888889e-05, 'epoch': 0.5511111111111111}\n",
      "{'loss': 3.5054, 'grad_norm': 1.0201783180236816, 'learning_rate': 2.2044444444444444e-05, 'epoch': 0.56}\n",
      "{'loss': 3.5601, 'grad_norm': 0.9060865044593811, 'learning_rate': 2.16e-05, 'epoch': 0.5688888888888889}\n",
      "{'loss': 3.4654, 'grad_norm': 0.8158338665962219, 'learning_rate': 2.1155555555555556e-05, 'epoch': 0.5777777777777777}\n",
      "{'loss': 3.5713, 'grad_norm': 0.8063846826553345, 'learning_rate': 2.071111111111111e-05, 'epoch': 0.5866666666666667}\n",
      "{'loss': 4.0098, 'grad_norm': 1.0431650876998901, 'learning_rate': 2.0266666666666667e-05, 'epoch': 0.5955555555555555}\n",
      "{'loss': 4.0166, 'grad_norm': 0.9654749631881714, 'learning_rate': 1.9822222222222223e-05, 'epoch': 0.6044444444444445}\n",
      "{'loss': 3.6992, 'grad_norm': 1.3022429943084717, 'learning_rate': 1.9377777777777778e-05, 'epoch': 0.6133333333333333}\n",
      "{'loss': 3.6501, 'grad_norm': 0.9339846968650818, 'learning_rate': 1.8933333333333334e-05, 'epoch': 0.6222222222222222}\n",
      "{'loss': 3.7637, 'grad_norm': 1.8519632816314697, 'learning_rate': 1.848888888888889e-05, 'epoch': 0.6311111111111111}\n",
      "{'loss': 3.4946, 'grad_norm': 1.0746086835861206, 'learning_rate': 1.8044444444444445e-05, 'epoch': 0.64}\n",
      "{'loss': 3.6394, 'grad_norm': 0.8447054028511047, 'learning_rate': 1.76e-05, 'epoch': 0.6488888888888888}\n",
      "{'loss': 3.7938, 'grad_norm': 1.039047122001648, 'learning_rate': 1.7155555555555557e-05, 'epoch': 0.6577777777777778}\n",
      "{'loss': 3.5529, 'grad_norm': 1.0755187273025513, 'learning_rate': 1.6711111111111112e-05, 'epoch': 0.6666666666666666}\n",
      "{'loss': 3.6283, 'grad_norm': 0.9097932577133179, 'learning_rate': 1.6266666666666665e-05, 'epoch': 0.6755555555555556}\n",
      "{'loss': 3.4799, 'grad_norm': 1.2923827171325684, 'learning_rate': 1.582222222222222e-05, 'epoch': 0.6844444444444444}\n",
      "{'loss': 3.7078, 'grad_norm': 0.9024430513381958, 'learning_rate': 1.537777777777778e-05, 'epoch': 0.6933333333333334}\n",
      "{'loss': 3.5307, 'grad_norm': 0.8763388991355896, 'learning_rate': 1.4933333333333335e-05, 'epoch': 0.7022222222222222}\n",
      "{'loss': 3.5796, 'grad_norm': 0.9468176364898682, 'learning_rate': 1.448888888888889e-05, 'epoch': 0.7111111111111111}\n",
      "{'loss': 3.4457, 'grad_norm': 2.3853840827941895, 'learning_rate': 1.4044444444444446e-05, 'epoch': 0.72}\n",
      "{'loss': 3.5779, 'grad_norm': 1.004813313484192, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.7288888888888889}\n",
      "{'loss': 3.6657, 'grad_norm': 1.2323321104049683, 'learning_rate': 1.3155555555555558e-05, 'epoch': 0.7377777777777778}\n",
      "{'loss': 3.4411, 'grad_norm': 0.9684973955154419, 'learning_rate': 1.2711111111111113e-05, 'epoch': 0.7466666666666667}\n",
      "{'loss': 3.459, 'grad_norm': 1.1893324851989746, 'learning_rate': 1.2266666666666667e-05, 'epoch': 0.7555555555555555}\n",
      "{'loss': 3.7277, 'grad_norm': 0.7243606448173523, 'learning_rate': 1.1822222222222223e-05, 'epoch': 0.7644444444444445}\n",
      "{'loss': 3.6362, 'grad_norm': 0.9027371406555176, 'learning_rate': 1.1377777777777779e-05, 'epoch': 0.7733333333333333}\n",
      "{'loss': 3.8051, 'grad_norm': 1.2323426008224487, 'learning_rate': 1.0933333333333334e-05, 'epoch': 0.7822222222222223}\n",
      "{'loss': 3.5836, 'grad_norm': 1.0884231328964233, 'learning_rate': 1.048888888888889e-05, 'epoch': 0.7911111111111111}\n",
      "{'loss': 3.7217, 'grad_norm': 0.8331623673439026, 'learning_rate': 1.0044444444444446e-05, 'epoch': 0.8}\n",
      "{'loss': 3.5928, 'grad_norm': 1.049676775932312, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.8088888888888889}\n",
      "{'loss': 3.6798, 'grad_norm': 0.9588401317596436, 'learning_rate': 9.155555555555557e-06, 'epoch': 0.8177777777777778}\n",
      "{'loss': 3.5683, 'grad_norm': 0.925174355506897, 'learning_rate': 8.711111111111111e-06, 'epoch': 0.8266666666666667}\n",
      "{'loss': 3.4175, 'grad_norm': 1.2703282833099365, 'learning_rate': 8.266666666666667e-06, 'epoch': 0.8355555555555556}\n",
      "{'loss': 3.9266, 'grad_norm': 0.968224048614502, 'learning_rate': 7.822222222222222e-06, 'epoch': 0.8444444444444444}\n",
      "{'loss': 3.6649, 'grad_norm': 1.063761830329895, 'learning_rate': 7.377777777777778e-06, 'epoch': 0.8533333333333334}\n",
      "{'loss': 3.6938, 'grad_norm': 0.9497489929199219, 'learning_rate': 6.933333333333334e-06, 'epoch': 0.8622222222222222}\n",
      "{'loss': 3.5026, 'grad_norm': 1.2617253065109253, 'learning_rate': 6.488888888888888e-06, 'epoch': 0.8711111111111111}\n",
      "{'loss': 3.7538, 'grad_norm': 1.1890140771865845, 'learning_rate': 6.044444444444445e-06, 'epoch': 0.88}\n",
      "{'loss': 3.7059, 'grad_norm': 0.9978010654449463, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.8888888888888888}\n",
      "{'loss': 3.7603, 'grad_norm': 4.978487014770508, 'learning_rate': 5.155555555555555e-06, 'epoch': 0.8977777777777778}\n",
      "{'loss': 3.6768, 'grad_norm': 1.0401909351348877, 'learning_rate': 4.711111111111111e-06, 'epoch': 0.9066666666666666}\n",
      "{'loss': 3.465, 'grad_norm': 1.5187568664550781, 'learning_rate': 4.266666666666667e-06, 'epoch': 0.9155555555555556}\n",
      "{'loss': 3.5731, 'grad_norm': 1.001815915107727, 'learning_rate': 3.8222222222222224e-06, 'epoch': 0.9244444444444444}\n",
      "{'loss': 3.6404, 'grad_norm': 0.9632301926612854, 'learning_rate': 3.3777777777777777e-06, 'epoch': 0.9333333333333333}\n",
      "{'loss': 3.4731, 'grad_norm': 1.0924047231674194, 'learning_rate': 2.9333333333333333e-06, 'epoch': 0.9422222222222222}\n",
      "{'loss': 3.9184, 'grad_norm': 1.0039764642715454, 'learning_rate': 2.488888888888889e-06, 'epoch': 0.9511111111111111}\n",
      "{'loss': 3.544, 'grad_norm': 1.2065404653549194, 'learning_rate': 2.0444444444444447e-06, 'epoch': 0.96}\n",
      "{'loss': 3.6693, 'grad_norm': 1.1730626821517944, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.9688888888888889}\n",
      "{'loss': 3.7313, 'grad_norm': 0.8702567219734192, 'learning_rate': 1.1555555555555556e-06, 'epoch': 0.9777777777777777}\n",
      "{'loss': 3.602, 'grad_norm': 1.0124660730361938, 'learning_rate': 7.111111111111112e-07, 'epoch': 0.9866666666666667}\n",
      "{'loss': 3.5378, 'grad_norm': 0.9199131727218628, 'learning_rate': 2.6666666666666667e-07, 'epoch': 0.9955555555555555}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSeq2SeqLM.forward` and have been ignored: translation. If translation are not expected by `PeftModelForSeq2SeqLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2188501358032227, 'eval_bleu': 10.728504525135813, 'eval_runtime': 370.3786, 'eval_samples_per_second': 2.7, 'eval_steps_per_second': 2.7, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./mt0-lora/checkpoint-1125\n",
      "loading configuration file config.json from cache at /Users/huhao/.cache/huggingface/hub/models--bigscience--mt0-base/snapshots/0c500042c1f80b6b6ec208cf73d9ee43f1dac245/config.json\n",
      "Model config MT5Config {\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./mt0-lora/checkpoint-1125/tokenizer_config.json\n",
      "Special tokens file saved in ./mt0-lora/checkpoint-1125/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./mt0-lora/checkpoint-1125 (score: 10.728504525135813).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1865.4904, 'train_samples_per_second': 4.824, 'train_steps_per_second': 0.603, 'train_loss': 3.748817679511176, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1125, training_loss=3.748817679511176, metrics={'train_runtime': 1865.4904, 'train_samples_per_second': 4.824, 'train_steps_per_second': 0.603, 'train_loss': 3.748817679511176, 'epoch': 1.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bf2658-a394-4d09-9bf9-32ec40b4fc66",
   "metadata": {},
   "source": [
    "### 9、保存 LoRA 权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53a19f3e-ec16-4b7c-b80a-1c25393425d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./mt0-lora-mac-final\n",
      "loading configuration file config.json from cache at /Users/huhao/.cache/huggingface/hub/models--bigscience--mt0-base/snapshots/0c500042c1f80b6b6ec208cf73d9ee43f1dac245/config.json\n",
      "Model config MT5Config {\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./mt0-lora-mac-final/tokenizer_config.json\n",
      "Special tokens file saved in ./mt0-lora-mac-final/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./mt0-lora-mac-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e222e4-898c-44f9-ae75-87c40bab177d",
   "metadata": {},
   "source": [
    "生成的文件很小\n",
    "\n",
    "```\n",
    "% du -sh *\n",
    "2.6M\tmlruns\n",
    " 22M\tmt0-lora\n",
    " 17M\tmt0-lora-mac-final\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb862b-63ea-46a5-ad21-9d930efda290",
   "metadata": {},
   "source": [
    "## 五、测试生成的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb3eb57-7de9-43e5-9cb0-cf1f89679d24",
   "metadata": {},
   "source": [
    "加载微调后模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c324185-ed88-41e9-94d7-59893a4a90c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at /Users/huhao/.cache/huggingface/hub/models--bigscience--mt0-base/snapshots/0c500042c1f80b6b6ec208cf73d9ee43f1dac245/spiece.model\n",
      "loading file tokenizer.json from cache at /Users/huhao/.cache/huggingface/hub/models--bigscience--mt0-base/snapshots/0c500042c1f80b6b6ec208cf73d9ee43f1dac245/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/huhao/.cache/huggingface/hub/models--bigscience--mt0-base/snapshots/0c500042c1f80b6b6ec208cf73d9ee43f1dac245/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/huhao/.cache/huggingface/hub/models--bigscience--mt0-base/snapshots/0c500042c1f80b6b6ec208cf73d9ee43f1dac245/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /Users/huhao/.cache/huggingface/hub/models--bigscience--mt0-base/snapshots/0c500042c1f80b6b6ec208cf73d9ee43f1dac245/config.json\n",
      "Model config MT5Config {\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/huhao/.cache/huggingface/hub/models--bigscience--mt0-base/snapshots/0c500042c1f80b6b6ec208cf73d9ee43f1dac245/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "Could not locate the custom_generate/generate.py inside bigscience/mt0-base.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "model_name = \"bigscience/mt0-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = PeftModel.from_pretrained(base_model, \"./mt0-lora-mac-final\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5200bf8-a508-4dcc-8df1-35b0a5df8216",
   "metadata": {},
   "source": [
    "测试生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e04d193-50db-4950-824a-4c5ef99f4035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对于那些被困的人来说,这些老地方的生存状况对那些居住者来说,尤其是老人、老人和孩子,经常生活在极其困难的困难之中。\n"
     ]
    }
   ],
   "source": [
    "text = '''Translate English to Chinese: \n",
    "The difficult access to these older sites for displaced persons gives rise to \n",
    "concerns about the living conditions of the people who are residents there, \n",
    "in particular widows, the elderly and children, \n",
    "who often live in intolerable hardship.'''\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=1024)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3139d0-feb3-4fe1-8390-4969e4d64174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
