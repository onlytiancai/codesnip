{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f640e0-5157-4742-ba70-6b6f8f3cd76a",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/peft/quicktour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9062bc1-17a3-4ce9-ab2c-8851e2c9ea27",
   "metadata": {},
   "source": [
    "关闭多线程分词，避免多进程冲突警告。\n",
    "\n",
    "Hugging Face 的 tokenizers 库在后台使用 Rust 多线程加速（并行分词）。\n",
    "但是，当 Python 的 multiprocessing 在已经启用了并行的情况下调用 fork() 时，会触发这个警告，以避免潜在的死锁。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a81ac-50c6-4367-ad12-7f5ef781e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 禁用分词器并行警告\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"  # 允许 MPS 不支持的算子自动 fallback 到 CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1b6ad-fd33-4862-a63c-728f2295e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639d6c8-1b96-4d44-92ea-2d5ecb5ea730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af083a78-ca1e-474b-bb04-ecdb6c6a7e51",
   "metadata": {},
   "source": [
    "## 一、使用 bigscience/mt0-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683a7fec-785b-4679-8b92-1bd303950d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"bigscience/mt0-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "\n",
    "inputs = tokenizer(\"Translate English to Chinese: Hello, how are you?\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1039b3bd-537e-4cb1-bda8-db4a246b0ccf",
   "metadata": {},
   "source": [
    "## 二、微调训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba10a25-cbf2-4f41-b108-57b780405907",
   "metadata": {},
   "source": [
    "每个 PEFT 方法都由一个 PeftConfig 类定义，该类存储构建 PeftModel 所需的所有重要参数。\n",
    "\n",
    "例如，要使用 LoRa 进行训练，请加载并创建一个 LoraConfig 类。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd45da2-0ef5-4da3-a81c-75f5c839ffbc",
   "metadata": {},
   "source": [
    "- task_type ：要训练的任务（本例中为序列到序列语言建模）\n",
    "- inference_mode ：是否使用该模型进行推理\n",
    "- r ：低秩矩阵的维数\n",
    "- lora_alpha ：低秩矩阵的缩放因子\n",
    "- lora_dropout ：LoRa 层的 dropout 概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d297e5-e2f3-428d-8eda-fc9ee375f0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "#peft_config = LoraConfig(task_type=\"SEQ_2_SEQ_LM\", r=4, lora_alpha=8, lora_dropout=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee869fa-4979-4651-92a7-6d582e909a79",
   "metadata": {},
   "source": [
    "LoraConfig 设置完成后，使用 get_peft_model() 函数创建一个 PeftModel 。\n",
    "\n",
    "该函数接受一个基础模型（可以从 Transformers 库加载）和一个 LoraConfig， 其中包含用于配置模型以使用 LoRA 进行训练的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9292bf7-8b89-4a5b-98e1-45196a9ed5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b81e753-5e0f-4e40-a41c-ebd934a11f6b",
   "metadata": {},
   "source": [
    "现在你可以使用 Transformers Trainer 、Accelerate 或任何自定义的 PyTorch 训练循环来训练模型了。\n",
    "\n",
    "例如，要使用 Trainer 类进行训练，请设置一个 TrainingArguments 类，并设置一些训练超参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2656a75f-f21b-4783-ac23-d258643aa16c",
   "metadata": {},
   "source": [
    "加载一个翻译数据集，英文到中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565dd5f6-96c1-4b31-8e7a-ff8a4579626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"opus100\", \"en-zh\", split=\"train[:1%]\").train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483c2ab3-cba1-4023-8d66-c052730cb6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03293237-0dc2-484a-8986-570ced432883",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90fecfc-a90a-4c87-a6e3-e2101d5020ce",
   "metadata": {},
   "source": [
    "对数据进行预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d7b276-60d7-4d26-880e-71728b3199e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义预处理函数：把原始文本构造成给模型的 prompt，并进行分词\n",
    "def preprocess(examples):\n",
    "    # 构造输入 prompt（mt0 是 instruction-style，因此加上任务说明）   \n",
    "    inputs = [f\"translate English to Chinese: {ex['en']}\" for ex in examples[\"translation\"]] # 把每个英文句子包装成翻译指令\n",
    "    targets = [ex[\"zh\"] for ex in examples[\"translation\"]]\n",
    "\n",
    "    # 使用 tokenizer 对 inputs 做编码（截断 + padding 到 max_length）\n",
    "    model_inputs = tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=128)  # 编码输入，统一长度\n",
    "\n",
    "    # 对目标文本进行编码，得到 labels\n",
    "    labels = tokenizer(targets, truncation=True, padding=\"max_length\", max_length=128)  # 编码目标文本\n",
    "\n",
    "    # 将 labels 的 pad token 替换为 -100，这是 transformers 损失函数忽略的标记\n",
    "    # 这样在计算交叉熵时，对 pad 部分不会贡献损失\n",
    "    labels_input_ids = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in label] for label in labels[\"input_ids\"]\n",
    "    ]  # 将 labels 中的 pad id 转为 -100\n",
    "\n",
    "    # 把处理好的 labels 放回 model_inputs 字典，Trainer 会读取 \"labels\" 字段用于计算 loss\n",
    "    model_inputs[\"labels\"] = labels_input_ids  # 为返回的数据添加 labels\n",
    "\n",
    "    # 返回最终的编码后的输入字典（包含 input_ids, attention_mask, labels）\n",
    "    return model_inputs  # 返回每个样本的字典\n",
    "\n",
    "# 把预处理函数映射到整个数据集（batched=True 表示按批次处理以加速）\n",
    "tokenized_datasets = dataset.map(preprocess, batched=True)  # 对 dataset 的每个 split 应用 preprocess 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c3d2d-948c-41f6-a0e0-a467040c3e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e075faa-e9ec-4a36-8cfb-6c5ddf1bca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = tokenized_datasets[\"train\"][0]\n",
    "print(\"Input:\", tokenizer.decode(example[\"input_ids\"], skip_special_tokens=True))\n",
    "print(\"Label:\", tokenizer.decode([id for id in example[\"labels\"] if id != -100], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e4840d-0d9c-4d58-bb16-9993c0f01fe9",
   "metadata": {},
   "source": [
    "加载评测指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b4f038-b7c0-4978-a2cf-8efa73abc25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate  # Hugging Face evaluate 库，用于加载评测指标\n",
    "# 加载评测指标（示例使用 sacrebleu 用于机器翻译）\n",
    "metric = evaluate.load(\"sacrebleu\")  # 加载 sacrebleu 指标用于 BLEU 风格评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7294095-f047-4d84-9de1-b1cb2667bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 compute_metrics，用于 Trainer 在 eval 时计算指标\n",
    "def compute_metrics(eval_pred):\n",
    "    # eval_pred 的典型形式是 (predictions, labels)；这里 predictions 是生成器输出的 token id 矩阵\n",
    "    preds, labels = eval_pred  # 解包预测结果和标签\n",
    "\n",
    "    # 如果 predictions 是 logits（非生成模式），则需要取 argmax；但 predict_with_generate=True 时 preds 已是 token id\n",
    "    if isinstance(preds, tuple):  # 万一 preds 是 tuple（某些版本返回 logits 等），我们取第一个\n",
    "        preds = preds[0]\n",
    "\n",
    "    # 将预测中的 -100（如果有的话）替换为 tokenizer.pad_token_id，便于 decode\n",
    "    # 但正常生成结果里不应该有 -100，这里是稳健处理\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)  # 将 -100 换为 pad id（以便 decode）\n",
    "\n",
    "    # 把预测和真实标签解码成字符串（跳过 special tokens）\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)  # 解码预测文本\n",
    "    # labels 里仍可能有 -100，需要把它们换成 pad_token_id 再解码\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)  # 将 labels 中的 -100 替回 pad id\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)  # 解码真实文本\n",
    "\n",
    "    # sacrebleu 的 compute 需要 references 是 list of list 的形式：[[ref1], [ref2], ...]\n",
    "    references = [[l] for l in decoded_labels]  # 为每个样本创建单一引用列表\n",
    "\n",
    "    # 调用 sacrebleu 指标进行计算并返回结果字典（sacrebleu 返回 'score' 键表示 BLEU）\n",
    "    result = metric.compute(predictions=decoded_preds, references=references)  # 计算 sacrebleu 分数\n",
    "    # 将结果包装成 dict 返回，Trainer 会把它显示在 eval 输出里\n",
    "    return {\"bleu\": result[\"score\"]}  # 返回一个包含 BLEU 分数的字典"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4082f25-a56d-4316-8bc6-5f0215c22452",
   "metadata": {},
   "source": [
    "训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f788e1-f559-4c53-bff0-bed2f9be80e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "# ---- 训练参数 ----\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mt0-lora\",                  # 模型输出和检查点保存目录\n",
    "    per_device_train_batch_size=1,            # 每个 GPU/设备上的训练 batch size（根据显存调整）, # Mac上必须非常小，否则内存爆\n",
    "    per_device_eval_batch_size=1,             # 验证时的 batch size\n",
    "    gradient_accumulation_steps=8,            # # 模拟 batch size=8\n",
    "    learning_rate=5e-5,                       # 学习率，默认5e-5，内存小调低学习率\n",
    "    num_train_epochs=1,                       # 训练轮次，#先跑1个epoch测试稳定性\n",
    "    eval_strategy=\"epoch\",                    # 评估频率（这里设置为每个 epoch 评估一次）\n",
    "    predict_with_generate=True,               # 在评估时使用 generate() 来产生文本（必需用于语言生成任务）\n",
    "    save_strategy=\"epoch\",                    # epoch 表示每个 epoch 保存一次checkpoint），# no 不保存中间检查点，节省内存\n",
    "    logging_strategy=\"steps\",                 # 日志策略（按 steps）\n",
    "    logging_steps=100,                        # 每多少 step 打印一次日志\n",
    "    fp16=False,                               # 如果支持则开启半精度训练以节省显存（需要 CUDA + 支持）#  MPS 不支持 fp16\n",
    "    load_best_model_at_end=True,              # 在训练结束加载最佳验证结果对应的模型\n",
    "    dataloader_pin_memory=False,              # 在 CUDA 上能让 DataLoader 更快地把数据拷贝到 GPU。MPS 不支持\n",
    "    metric_for_best_model=\"bleu\",             # 根据哪个指标判断最佳模型\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a59d0a-051f-4412-98c0-251563ca53f8",
   "metadata": {},
   "source": [
    "创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb94df67-e709-4a65-83b3-baecaac8720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                              # 注入（已包装 LoRA 的）模型\n",
    "    args=training_args,                       # 训练参数\n",
    "    train_dataset=tokenized_datasets[\"train\"],# 训练集\n",
    "    eval_dataset=tokenized_datasets[\"validation\"] if \"validation\" in tokenized_datasets else tokenized_datasets[\"test\"],  # 优先使用 validation split，否则用 test\n",
    "    processing_class=tokenizer,     # tokenizer（用于 decode/generation 时）\n",
    "    compute_metrics=compute_metrics,          # 评估时要计算的指标函数\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae02949-d0c6-46a5-95f4-f31f953758e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 启动训练 ----\n",
    "trainer.train()  # 运行训练过程；训练期间会按 training_args 指定的策略保存 checkpoint 并在 eval 时计算指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a19f3e-ec16-4b7c-b80a-1c25393425d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 保存 LoRA 权重\n",
    "trainer.save_model(\"./mt0-lora-mac-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb862b-63ea-46a5-ad21-9d930efda290",
   "metadata": {},
   "source": [
    "# 测试生成的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c324185-ed88-41e9-94d7-59893a4a90c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载微调后模型\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "model_name = \"bigscience/mt0-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = PeftModel.from_pretrained(base_model, \"./mt0-lora-mac-final\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e04d193-50db-4950-824a-4c5ef99f4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试生成\n",
    "inputs = tokenizer(\"Translate English to Chinese: The difficult access to these older sites for displaced persons gives rise to concerns about the living conditions of the people who are residents there, in particular widows, the elderly and children, who often live in intolerable hardship.\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=1024)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
