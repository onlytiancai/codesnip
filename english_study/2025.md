# 2025-11-28

å•è¯

- integrable /ËˆÉªntÉ™É¡rÉ™bl/
- integral /ËˆÉªntÉªÉ¡rÉ™l/
- integrability /ÉªntÉªÉ¡rÉ™ËˆbÉªlÉªti/
- linear /ËˆlÉªniÉ™r/
- algebra  /ËˆÃ¦ldÊ’ÉªbrÉ™/
- Euclidean /juËËˆklÉªdiÉ™n; ËˆjuËklÉªdiÉ™n/
- Intuition /ËŒÉªntuËˆÉªÊƒ(É™)n/
- intuitively /ÉªnËˆtuËÉªtÉªvli/
- infinite /ËˆÉªnfÉªnÉ™t/
- finite /ËˆfaÉªnaÉªt/
- dimensional /daÉªËˆmenÊƒÉ™n(É™)l/
- interpretation /ÉªnËŒtÉœËrprÉ™ËˆteÉªÊƒ(É™)n/
- representation /ËŒreprÉªzenËˆteÉªÊƒ(É™)n/
- notation /noÊŠËˆteÉªÊƒn/
- denote /dÉªËˆnoÊŠt/
- irrational /ÉªËˆrÃ¦ÊƒÉ™n(É™)l/
- generality /ËŒdÊ’enÉ™ËˆrÃ¦lÉ™ti/
- generalization /ËŒdÊ’en(É™)rÉ™lÉ™ËˆzeÉªÊƒ(É™)n/
- equivalent /ÉªËˆkwÉªvÉ™lÉ™nt/
- energy /ËˆenÉ™rdÊ’i/
- foremost /ËˆfÉ”ËrmoÊŠst/
- converge /kÉ™nËˆvÉœËrdÊ’/
- properties /ËˆprÉ‘ËpÉ™rtiz/
- demonstrated /ËˆdemÉ™nstreÉªtÉªd/
- readily /ËˆredÉªli/
- plenty /Ëˆplenti/
- Therefore /ËˆÃ°erfÉ”Ër/
- exists /ÉªÉ¡ËˆzÉªsts/
- satisfies /ËˆsÃ¦tÉªsfaÉªz/



å¥å­

- The tools of linear algebra are extremely useful
- Using the function notation we can write v
- a standard interpretation is an ordered list of numbers. 
- Intuition - functions as infinite-dimensional vectors
- thereâ€™s an infinite number of them, and most of the indices are irrational which donâ€™t even have a finite representation
- A function f is said to be square integrable.
- The set of such functions is commonly denoted L^2
- The integral over the square of the function is equivalent to the Euclidean norm for vectors; 
- intuitively, it acts as a measure of length
- For functions, itâ€™s typically referred to as energy
- The function version is just the generalization of this sum over an infinite range, using an integral.
- first and foremost we have to show that 
- if the integral doesnâ€™t converge, itâ€™s not something we can work with
- This is where the square integrability of functions in  comes into play - without being square integrable, the inner product would be impossible to define.
- The other properties of inner products can also be demonstrated readily, and there are plenty of resources online that show how
- Therefore L^2, coupled with the inner product operation shown here forms an inner product space.
- Once again, because our functions in  are square integrable, the norm exists and itâ€™s easy to show it satisfies all the usual requirements for a norm.

link

- https://eli.thegreenplace.net/2025/hilbert-space-treating-functions-as-vectors/

# 2025-11-29

> ç»™å‡ºä¸€äº›è‹±è¯­è‡ªç„¶æ‹¼è¯»çš„è§„åˆ™å’Œä¾‹å­ï¼Œæœ‰ç®€å•åˆ°å¤æ‚

ä¸‹é¢ç»™ä½ ä¸€å¥— **ä»æœ€ç®€å•åˆ°é€æ¸å¤æ‚** çš„è‹±è¯­è‡ªç„¶æ‹¼è¯»è§„åˆ™ï¼ˆPhonicsï¼‰ï¼Œæ¯æ¡éƒ½é… **æ¸…æ™°ä¾‹å­**ï¼Œä½ å¯ä»¥ä»å¤´åˆ°å°¾å¾ªåºæŒæ¡ã€‚

---

# âœ… **ç¬¬ä¸€å±‚ï¼šæœ€åŸºç¡€çš„å­—æ¯â€”éŸ³ç´ å¯¹åº”**

### **1. å•ä¸ªè¾…éŸ³å­—æ¯çš„åŸºæœ¬éŸ³**

* **b** â†’ /b/ï¼š*bat*
* **d** â†’ /d/ï¼š*dog*
* **f** â†’ /f/ï¼š*fan*
* **g**ï¼ˆç¡¬éŸ³ï¼‰â†’ /g/ï¼š*go*
* **h** â†’ /h/ï¼š*hat*
* **k** â†’ /k/ï¼š*kite*
* **l** â†’ /l/ï¼š*lip*
* **m** â†’ /m/ï¼š*map*

ï¼ˆè¿™äº›æ˜¯æœ€ç¨³å®šçš„éƒ¨åˆ†ã€‚ï¼‰

---

# âœ… **ç¬¬äºŒå±‚ï¼šçŸ­å…ƒéŸ³ï¼ˆæœ€å¸¸è§ï¼‰**

### **2. äº”ä¸ªçŸ­å…ƒéŸ³**

* **a â†’ /Ã¦/**ï¼š*cat, map, back*
* **e â†’ /É›/**ï¼š*bed, pen, tell*
* **i â†’ /Éª/**ï¼š*sit, pin, milk*
* **o â†’ /É’/ æˆ– /É‘/**ï¼š*hot, box, mom*
* **u â†’ /ÊŒ/**ï¼š*sun, cup, bus*

---

# âœ… **ç¬¬ä¸‰å±‚ï¼šCVC ç»“æ„ï¼ˆè¾…éŸ³-å…ƒéŸ³-è¾…éŸ³ï¼‰æœ€ç¨³çš„è¯»æ³•**

CVC å•è¯å‡ ä¹ä¸€å®šè¯» **çŸ­å…ƒéŸ³**ï¼š

* *cat*
* *pen*
* *sit*
* *hot*
* *sun*

æŒæ¡ CVC = è¯» 70% çš„ç®€å•å•è¯ã€‚

---

# âœ… **ç¬¬å››å±‚ï¼šé•¿å…ƒéŸ³ï¼ˆåå­—éŸ³ï¼‰**

å…ƒéŸ³é‡åˆ°æŸäº›ç»“æ„ä¼šè¯»æˆè‡ªå·±çš„â€œå­—æ¯åâ€éŸ³ï¼š

### **3. å…ƒéŸ³ + è¾…éŸ³ + eï¼ˆmagic e / silent eï¼‰â†’ é•¿å…ƒéŸ³**

* **a-e â†’ /eÉª/**ï¼š*cake, name*
* **i-e â†’ /aÉª/**ï¼š*bike, time*
* **o-e â†’ /oÊŠ/**ï¼š*home, nose*
* **u-e â†’ /juË/ æˆ– /uË/**ï¼š*cube, mute, rule*
* **e-e â†’ /iË/**ï¼š*these*

---

# âœ… **ç¬¬äº”å±‚ï¼šå¸¸è§åŒå­—æ¯ç»„åˆï¼ˆdigraphsï¼‰**

### **4. è¾…éŸ³ç»„åˆ**

* **sh â†’ /Êƒ/**ï¼š*ship, fish*
* **ch â†’ /tÊƒ/**ï¼š*chat, lunch*
* **th â†’ /Î¸/ æˆ– /Ã°/**ï¼š*think, this*
* **ph â†’ /f/**ï¼š*phone, graph*

### **5. å…ƒéŸ³ç»„åˆï¼ˆå¸¸è§é•¿å…ƒéŸ³æ¥æºï¼‰**

* **ai â†’ /eÉª/**ï¼š*rain, tail*
* **ay â†’ /eÉª/**ï¼š*day, play*
* **ee â†’ /iË/**ï¼š*see, green*
* **ea â†’ /iË/ æˆ– /É›/**ï¼š*eat, read, bread*
* **oa â†’ /oÊŠ/**ï¼š*boat, road*
* **ow â†’ /oÊŠ/ æˆ– /aÊŠ/**ï¼š*snow, cow*
* **ou/ow â†’ /aÊŠ/**ï¼š*out, cow*

---

# âœ… **ç¬¬å…­å±‚ï¼šå¸¸è§è¯å°¾ç¨³å®šè¯»æ³•**

### **6. -ck â†’ /k/**

*back, rock, black*

### **7. -tch â†’ /tÊƒ/**

*catch, match*

### **8. -dge â†’ /dÊ’/**

*fudge, edge*

---

# âœ… **ç¬¬ä¸ƒå±‚ï¼šè½¯éŸ³ c å’Œè½¯éŸ³ gï¼ˆå¼€å§‹æœ‰ç‚¹å¤æ‚ï¼‰**

### **9. c åœ¨ e, i, y å‰ â†’ /s/**

* *city, face, cycle*

å¦åˆ™å‘ /k/ï¼š

* *cat, cold, cut*

### **10. g åœ¨ e, i, y å‰ â†’ /dÊ’/**

* *giant, gym, gentle*

å¦åˆ™è¯» /g/ï¼š

* *go, gum, got*

ï¼ˆä¸æ˜¯ 100% è§„åˆ™ï¼Œä½†é«˜æ¦‚ç‡ã€‚ï¼‰

---

# âœ… **ç¬¬å…«å±‚ï¼šr æ§å…ƒéŸ³ï¼ˆR-controlled vowelsï¼‰**

r ä¼šæ”¹å˜å‰é¢å…ƒéŸ³çš„å‘éŸ³ï¼š

* **ar â†’ /É‘Ër/**ï¼š*car, star*

* **er / ir / ur â†’ /ÉË/**ï¼ˆå‡ ä¹ç›¸åŒéŸ³ï¼‰

  * *her, bird, turn*

* **or â†’ /É”Ër/**ï¼š*fork, corn*

è¿™æ˜¯è‹±è¯­â€œè‡ªç„¶æ‹¼è¯»æœ€éš¾éƒ¨åˆ†â€ä¹‹ä¸€ã€‚

---

# âœ… **ç¬¬ä¹å±‚ï¼šé‡éŸ³æ”¹å˜å‘éŸ³ï¼ˆé«˜çº§è§„å¾‹ï¼‰**

å°¤å…¶æ˜¯åœ¨å¤šéŸ³èŠ‚è¯ä¸­ï¼š

### **11. éé‡è¯»éŸ³èŠ‚é‡Œçš„å…ƒéŸ³ç»å¸¸å¼±è¯»æˆ schwaï¼ˆ/É™/ï¼‰**

* **about** â†’ /É™ËˆbaÊŠt/
* **banana** â†’ /bÉ™ËˆnÃ¦nÉ™/
* **support** â†’ /sÉ™ËˆpÉ”Ërt/

Schwa æ˜¯è‹±è¯­æœ€å¸¸è§ä½†æœ€éš¾æ•™çš„å…ƒéŸ³ã€‚

---

# âœ… **ç¬¬åå±‚ï¼šè¯æ ¹å˜åŒ–å¯¼è‡´çš„éŸ³å˜ï¼ˆæ›´é«˜çº§ï¼‰**

è¿™æ˜¯ä¸ºä»€ä¹ˆ *finite / infinite*ã€*sign / signature* ä¼šè¯»ä¸åŒéŸ³ã€‚

### **12. è¯çš„æ´¾ç”Ÿä¼šå¼±è¯»æˆ–æ”¹å˜å‘éŸ³**

* *sign /saÉªn/* â†’ *signature /ËˆsÉªgnÉ™tÊƒÉ™r/*
* *crime /kraÉªm/* â†’ *criminal /ËˆkrÉªmÉªnÉ™l/*
* *divine /dÉªËˆvaÉªn/* â†’ *divinity /dÉªËˆvÉªnÉªti/*

åŸå› ï¼šé‡éŸ³ç§»åŠ¨ + å…ƒéŸ³å¼±åŒ– + å†å²è¯æºã€‚


> åšä¸€ä»½æœ€å…³é”®çš„ä¾‹è¯è¡¨ï¼ˆå¯ç›´æ¥ç”¨æ¥èƒŒï¼‰,å•è¯ï¼ŒéŸ³æ ‡ï¼Œç®€çŸ­é‡Šä¹‰éƒ½è¦

ä¸‹é¢ç»™ä½ ä¸€ä»½ **æœ€å…³é”®ã€æœ€å¸¸ç”¨ã€æŒ‰è‡ªç„¶æ‹¼è¯»è§„å¾‹æ•´ç†çš„ä¾‹è¯è¡¨**ï¼ˆå¯ç›´æ¥èƒŒï¼‰ã€‚
åŒ…å« **å•è¯ï½œéŸ³æ ‡ï½œç®€çŸ­é‡Šä¹‰**ã€‚
ä»æœ€åŸºç¡€åˆ°ç¨å¤æ‚ï¼Œæ€»å…± **100 è¯æ ¸å¿ƒèƒŒè¯µè¡¨**ï¼Œè¦†ç›–ç»å¤§å¤šæ•° phonics è§„åˆ™ã€‚

---

# ğŸ“˜ **æ ¸å¿ƒè‡ªç„¶æ‹¼è¯»ä¾‹è¯è¡¨ï¼ˆ100 è¯ï¼‰**

## **â‘  CVCï¼ˆçŸ­å…ƒéŸ³ï¼‰æœ€ç¨³çš„è¯»æ³•**

| å•è¯  | éŸ³æ ‡     | ç®€é‡Š |
| --- | ------ | -- |
| cat | /kÃ¦t/  | çŒ«  |
| bag | /bÃ¦g/  | åŒ…  |
| map | /mÃ¦p/  | åœ°å›¾ |
| pen | /pÉ›n/  | é’¢ç¬” |
| bed | /bÉ›d/  | åºŠ  |
| sit | /sÉªt/  | å  |
| pig | /pÉªg/  | çŒª  |
| hot | /hÉ’t/  | çƒ­çš„ |
| box | /bÉ’ks/ | ç›’å­ |
| sun | /sÊŒn/  | å¤ªé˜³ |
| cup | /kÊŒp/  | æ¯å­ |
| run | /rÊŒn/  | è·‘  |

---

## **â‘¡ Magic eï¼ˆé•¿å…ƒéŸ³ï¼‰**

| å•è¯   | éŸ³æ ‡      | ç®€é‡Š  |
| ---- | ------- | --- |
| cake | /keÉªk/  | è›‹ç³•  |
| name | /neÉªm/  | åå­—  |
| bike | /baÉªk/  | è‡ªè¡Œè½¦ |
| time | /taÉªm/  | æ—¶é—´  |
| note | /noÊŠt/  | ç¬”è®°  |
| home | /hoÊŠm/  | å®¶   |
| cube | /kjuËb/ | ç«‹æ–¹ä½“ |
| cute | /kjuËt/ | å¯çˆ±  |

---

## **â‘¢ å¸¸è§è¾…éŸ³ç»„åˆï¼ˆsh, ch, th, phï¼‰**

| å•è¯    | éŸ³æ ‡        | ç®€é‡Š   |
| ----- | --------- | ---- |
| ship  | /ÊƒÉªp/     | èˆ¹    |
| fish  | /fÉªÊƒ/     | é±¼    |
| shop  | /ÊƒÉ’p/     | å•†åº—   |
| chat  | /tÊƒÃ¦t/    | èŠå¤©   |
| lunch | /lÊŒntÊƒ/   | åˆé¤   |
| rich  | /rÉªtÊƒ/    | å¯Œæœ‰   |
| thin  | /Î¸Éªn/     | ç˜¦çš„   |
| this  | /Ã°Éªs/     | è¿™ï¼Œè¿™ä¸ª |
| phone | /foÊŠn/    | ç”µè¯   |
| photo | /ËˆfoÊŠtoÊŠ/ | ç…§ç‰‡   |

---

## **â‘£ å¸¸è§é•¿å…ƒéŸ³ç»„åˆï¼ˆai, ay, ee, ea, oaï¼‰**

| å•è¯    | éŸ³æ ‡      | ç®€é‡Š  |
| ----- | ------- | --- |
| rain  | /reÉªn/  | é›¨   |
| tail  | /teÉªl/  | å°¾å·´  |
| day   | /deÉª/   | å¤©ï¼›æ—¥ |
| play  | /pleÉª/  | ç©   |
| see   | /siË/   | çœ‹è§  |
| green | /griËn/ | ç»¿è‰²  |
| eat   | /iËt/   | åƒ   |
| read  | /riËd/  | é˜…è¯»  |
| bread | /brÉ›d/  | é¢åŒ…  |
| boat  | /boÊŠt/  | èˆ¹   |
| road  | /roÊŠd/  | é“è·¯  |

---

## **â‘¤ ou / owï¼ˆ/aÊŠ/ï¼‰**

| å•è¯   | éŸ³æ ‡     | ç®€é‡Š  |
| ---- | ------ | --- |
| out  | /aÊŠt/  | å¤–é¢  |
| loud | /laÊŠd/ | å¤§å£°çš„ |
| cow  | /kaÊŠ/  | ç‰›   |
| now  | /naÊŠ/  | ç°åœ¨  |

---

## **â‘¥ è½¯éŸ³ c / g è§„å¾‹ï¼ˆc â†’ /s/, g â†’ /dÊ’/ï¼‰**

| å•è¯     | éŸ³æ ‡         | ç®€é‡Š  |
| ------ | ---------- | --- |
| city   | /ËˆsÉªti/    | åŸå¸‚  |
| face   | /feÉªs/     | è„¸   |
| cycle  | /ËˆsaÉªkÉ™l/  | å¾ªç¯  |
| giant  | /ËˆdÊ’aÉªÉ™nt/ | å·¨äºº  |
| gym    | /dÊ’Éªm/     | å¥èº«æˆ¿ |
| gentle | /ËˆdÊ’É›ntÉ™l/ | æ¸©æŸ”çš„ |

---

## **â‘¦ r æ§å…ƒéŸ³ï¼ˆR-controlled vowelsï¼‰**

| å•è¯   | éŸ³æ ‡      | ç®€é‡Š |
| ---- | ------- | -- |
| car  | /kÉ‘Ër/  | æ±½è½¦ |
| star | /stÉ‘Ër/ | æ˜Ÿæ˜Ÿ |
| her  | /hÉË/   | å¥¹çš„ |
| bird | /bÉËd/  | é¸Ÿ  |
| turn | /tÉËn/  | è½¬åŠ¨ |
| fork | /fÉ”Ërk/ | å‰å­ |
| corn | /kÉ”Ërn/ | ç‰ç±³ |

---

## **â‘§ åŒå†™ç»“å°¾ -ck / -tch / -dge**

| å•è¯     | éŸ³æ ‡      | ç®€é‡Š    |
| ------ | ------- | ----- |
| back   | /bÃ¦k/   | åé¢    |
| black  | /blÃ¦k/  | é»‘è‰²    |
| catch  | /kÃ¦tÊƒ/  | æ¥ä½    |
| match  | /mÃ¦tÊƒ/  | ç«æŸ´ï¼›æ¯”èµ› |
| edge   | /É›dÊ’/   | è¾¹ç¼˜    |
| bridge | /brÉªdÊ’/ | æ¡¥     |

---

## **â‘¨ å¤šéŸ³èŠ‚é«˜é¢‘è¯ï¼ˆå¸¦å¼±è¯» schwaï¼‰**

| å•è¯      | éŸ³æ ‡         | ç®€é‡Š |
| ------- | ---------- | -- |
| about   | /É™ËˆbaÊŠt/   | å…³äº |
| ago     | /É™ËˆgoÊŠ/    | ä»¥å‰ |
| police  | /pÉ™ËˆliËs/  | è­¦å¯Ÿ |
| animal  | /ËˆÃ¦nÉªmÉ™l/  | åŠ¨ç‰© |
| family  | /ËˆfÃ¦mÉ™li/  | å®¶åº­ |
| support | /sÉ™ËˆpÉ”Ërt/ | æ”¯æŒ |

---

## **â‘© å¸¸è§è¯æ ¹å˜åŒ–ï¼ˆè‹±è¯­éš¾ç‚¹ä½†é«˜é¢‘ï¼‰**

| å•è¯        | éŸ³æ ‡           | ç®€é‡Š     |
| --------- | ------------ | ------ |
| sign      | /saÉªn/       | ç¬¦å·ï¼›ç­¾å  |
| signature | /ËˆsÉªgnÉ™tÊƒÉ™r/ | ç­¾åï¼ˆåè¯ï¼‰ |
| crime     | /kraÉªm/      | çŠ¯ç½ª     |
| criminal  | /ËˆkrÉªmÉªnÉ™l/  | ç½ªçŠ¯     |
| divine    | /dÉªËˆvaÉªn/    | ç¥åœ£çš„    |
| divinity  | /dÉªËˆvÉªnÉªti/  | ç¥æ€§     |

---

å•è¯

- state-of-the-art æœ€å…ˆè¿›çš„ï¼›å·²ç»å‘å±•çš„ï¼›è¾¾åˆ°æœ€é«˜æ°´å‡†çš„
- iterative /ËˆÉªtÉ™rÉ™tÉªv/

å¥å­

- Fara-7B is an ultra-compact Computer Use Agent (CUA) that achieves state-of-the-art performance 
- Then you can iterative query it with:

# 2025-12-02

links

- https://www.seangoedecke.com/bad-code-at-big-companies/
- https://bell.bz/its-been-a-very-hard-year/

å¥å­

- Every couple of years somebody notices that large tech companies sometimes produce surprisingly sloppy code. 
    - surprisingly /sÉ™rËˆpraÉªzÉªÅ‹li/
    - sloppy /ËˆslÉ‘Ëpi/
- Big tech companies pay well enough to attract many competent engineers. 
    - competent /ËˆkÉ‘ËmpÉªtÉ™nt/
- I think the main reason is that big companies are full of engineers working outside their area of expertise. 
    - expertise /ËŒekspÉœËrËˆtiËz/ 
- In fact, big tech compensation packages are typically designed to put a four-year cap on engineer tenure
    - compensation /ËŒkÉ‘ËmpenËˆseÉªÊƒ(É™)n/
    - tenure /ËˆtenjÉ™r/
- after four years, the initial share grant is fully vested, causing engineers to take what can be a 50% pay cut. 
    - vested /ËˆvestÉªd/ 
- Companies do extend temporary yearly refreshes, but it obviously incentivizes engineers to go find another job where they donâ€™t have to wonder if theyâ€™re going to get the other half of their compensation each year.
    - temporary /ËˆtempÉ™reri/
    - obviously /ËˆÉ‘ËbviÉ™sli/
    - incentivize /ÉªnËˆsentÉªvaÉªz/
- I think a naive person would blame it all on the seemingly industry-wide attitude of â€œAI can just do this for usâ€.
    - attitude /ËˆÃ¦tÉªtuËd/


# 2025-12-03

links
- https://github.com/henrythe9th/ai-crash-course?tab=readme-ov-file
- https://www.swyx.io/learn-in-public
- https://www.latent.space/p/not-old

å¥å­

- AI Crash Course to help busy builders catch up to the public frontier of AI research in 2 weeks
- As a traditional software founder, I needed to quickly catch up to the frontier of AI research to figure out where the next opportunities and gaps were.
- I compiled a list of resources that were essential for me and should get you caught up within 2 weeks.
    - essential /ÉªËˆsenÊƒ(É™)l/
- Then get up to speed via Survey papers, Follow the ideas in the survey paper that interest you and dig deeper
- If thereâ€™s a golden rule, itâ€™s this one, so I put it first. All the other rules are more or less elaborations of this rule.
    - elaborations /ÉªËŒlÃ¦bÉ™ËˆreÉªÊƒnz/
- You already know that you will never be done learning. 
- But most people â€œlearn in privateâ€, and lurk. 
    - lurk /lÉœËrk/
- They consume content without creating any themselves. 
- Again, thatâ€™s fine, but weâ€™re here to talk about being in the top quintile. 
    - quintile /ËˆkwÉªntaÉªl/
- What you do here is to have a habit of creating learning exhaust: 
    - habit /ËˆhÃ¦bÉªt/
    - exhaust /ÉªÉ¡ËˆzÉ”Ëst/
- Write blogs and tutorials and cheatsheets.
- Speak at meetups and conferences.
- Ask and answer things on Stackoverflow or Reddit. Avoid the walled gardens like Slack and Discord, theyâ€™re not public.
- Make Youtube videos or Twitch streams.
- Start a newsletter.
- Draw cartoons (people loooove cartoons!).

# 2025-12-04

- https://arxiv.org/pdf/2402.06196v2

å¥å­

- Language modeling is a long-standing research topic

å¥å­

- Language modeling is a long-standing research topic, 
    - modeling /ËˆmÉ‘Ëd(É™)lÉªÅ‹/
- dating back to the 1950s with Shannonâ€™s application of information theory to human language, 
    - theory /ËˆÎ¸iËÉ™ri/
- where he measured how well simple n-gram language models predict or compress natural language text [3]. 
- Since then, statistical language modeling became fundamental to many natural language understanding and generation tasks, 
    - statistical /stÉ™ËˆtÉªstÉªk(É™)l/
    - fundamental /ËŒfÊŒndÉ™Ëˆment(É™)l/
- ranging from speech recognition, machine translation, to information retrieval 
    - retrieval /rÉªËˆtriËv(É™)l/
- The recent advances on transformer-based large language models (LLMs), 
- pretrained on Web-scale text corpora, significantly extended the capabilities of language models (LLMs).
    - corpora /ËˆkÉ”ËrpÉ™rÉ™/
    - significantly /sÉªÉ¡ËˆnÉªfÉªkÉ™ntli/ 
    - capabilities /ËŒkeÉªpÉ™ËˆbÉªlÉ™tiz/
- For example, OpenAIâ€™s ChatGPT and GPT-4 can be used not only for natural language processing, 
- but also as general task solvers to power Microsoftâ€™s Co-Pilot systems
    - solver /ËˆsÉ‘ËlvÉ™r/
- for instance, can follow human instructions of complex new tasks performing multi-step reasoning when needed. 
- LLMs are thus becoming the basic building block for the development of general-purpose AI agents or artificial general intelligence (AGI).
    - artificial /ËŒÉ‘ËrtÉªËˆfÉªÊƒ(É™)l/
- As the field of LLMs is moving fast, with new findings, models and techniques being published in a matter of months or weeks
- AI researchers and practitioners often find it challenging to figure out the best recipes to build LLM-powered AI systems for their tasks.
    - practitioners /prÃ¦kËˆtÉªÊƒÉ™nÉ™r/
    - recipes /ËˆresÉ™pi/
- This paper gives a timely survey of the recent advances on LLMs. 
    - survey /ËˆsÉœËrveÉª/ è¿™é‡Œæ˜¯æ¦‚è¿°ï¼Œå…¨é¢ç»¼è¿°çš„æ„æ€ï¼Œä¸æ˜¯è°ƒæŸ¥çš„æ„æ€
    - advances /É™dËˆvÃ¦nsÉªz/
- We hope this survey will prove a valuable and accessible resource for students, researchers and developers.
    - prove
- LLMs are large-scale, pre-trained, statistical language models based on neural networks. 
    - statistical /stÉ™ËˆtÉªstÉªk(É™)l/
- The recent success of LLMs is an accumulation of decades of research and development of language models
    - accumulation /É™ËŒkjuËmjÉ™ËˆleÉªÊƒn/
    - decades /ËˆdekeÉªdz/
- which can be categorized into four waves that have different starting points and velocity
    - categorized /ËˆkÃ¦tÉ™É¡É™raÉªzd/
- statistical language models, neural language models, pre-trained language models and LLMs.
    - statistical /stÉ™ËˆtÉªstÉªk(É™)l/
    - neural /ËˆnÊŠrÉ™l/
- Statistical language models (SLMs) view text as a sequence of words, and estimate the probability of text as the product of their word probabilities. 
    - sequence /ËˆsiËkwÉ™ns/
    - estimate /ËˆestÉªmÉ™t; ËˆestÉªmeÉªt/
    - probability /ËŒprÉ‘ËbÉ™ËˆbÉªlÉ™ti/
- The dominating form of SLMs are Markov chain models known as the n-gram models, 
    - dominating /ËˆdÉ‘ËmÉªneÉªtÉªÅ‹/
    - chain /tÊƒeÉªn/
- which compute the probability of a word conditioned on its immediate proceeding n âˆ’ 1 words.
    - conditioned /kÉ™nËˆdÉªÊƒnd/
    - immediate /ÉªËˆmiËdiÉ™t/
    - proceeding /prÉ™ËˆsiËdÉªÅ‹/
- Since word probabilities are estimated using word and n-gram counts collected from text corpora, 
- the model needs to deal with data sparsity (i.e., assigning zero probabilities to unseen words or n-grams) by using smoothing, 
    - sparsity /ËˆspÉ‘ËrsÉ™ti/
    - assigning /É™ËˆsaÉªnÉªÅ‹/
    - smoothing /ËˆsmuËÃ°ÉªÅ‹/
- where some probability mass of the model is reserved for unseen n-grams [12].
- N-gram models are widely used in many NLP systems. 
    - widely /ËˆwaÉªdli/
- However, these models are incomplete in that they cannot fully capture the diversity and variability of natural language due to data sparsity.
    - diversity /daÉªËˆvÉœËrsÉ™ti; dÉªËˆvÉœËsÉªti/
    - variability /ËŒveriÉ™ËˆbÉªlÉ™ti/
    - natual /ËˆnÃ¦tÊƒ(É™)rÉ™l/
- Early neural language models (NLMs) deal with data sparsity by mapping words to low-dimensional continuous vectors (embedding vectors) 
    - continuous /kÉ™nËˆtÉªnjuÉ™s/
- and predict the next word based on the aggregation of the embedding vectors of its proceeding words using neural networks. 

# 2025-12-09

links

- https://sam.zeloof.xyz/second-ic/

sentences

- In 2018 I made the first lithographically fabricated integrated circuits in my garage fab.
    - fabricated 'fÃ¦brikeit
    - circuits 'sÓ™:kit
- I was a senior in high school when I made the Z1 amplifier, 
    - amplifier 'Ã¦mplifaiÓ™
- and now Iâ€™m a senior in college so there are some long overdue improvements to the amateur silicon process.
    - overdue 'Ó™uvÓ™'dju:
    - amateur 'Ã¦mÓ™tÓ™
    - silicon 'silikÓ™n

# 2025-12-10

- https://blog.jim-nielsen.com/2025/icons-in-menus/
- I complained about this on the socials, but I didnâ€™t get it all out of my system. So now I write a blog post.
- Iâ€™ve never liked the philosophy of â€œput an icon in every menu item by defaultâ€.
    - philosophy fi'lÉ’sÓ™fi
- Google Sheets, for example, does this. Go to â€œFileâ€ or â€œEditâ€ or â€œViewâ€ and youâ€™ll see a menu with a list of options, 
- every single one having an icon (same thing with the right-click context menu).

 
