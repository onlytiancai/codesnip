### 2019-10-15

08:38 < hooxen> I have not seen syntax like `-e .[dev,examples]` in a pip requirements.txt before. i am assuming(ËÆ§‰∏∫ÔºåÂÅáËÆæ) that means in local directory look for dev and examples but in my example (mitmproxy from github) there is no `dev` directory after cloning and `examples` is a directory with no .txt requirements in it...what does this syntax say or where can i learn about it?
08:39 < energizer> hooxen: [these] are extras . look in setup.py
08:40 < papna> https://packaging.python.org/tutorials/installing-packages/#installing-setuptools-extras
08:40 < hooxen> energizer: i see it there now, i assume related to setuptools
08:40 < energizer> yes
08:41 < hooxen> awesome thanks,i know what to look for now
08:41 < _amas_> Elodin, energizer: I guess the walrusÔºàÊµ∑Ë±°Ôºâ answer would be: key_phone = {val[1]: val[3] for href in hrefs if (val := href.split("'"))}
08:42 < papna> _amas_: That code doesn't do quite the samething
08:43 < squirrel> papna: how so
08:43 < papna> dict(operator.itemgetter(1, 3)(href.split("'") for href in hrefs) ?
08:44 < papna> squirrel: Oops, I remembered str.split working differently
08:45 < papna> squirrel: If hrefs is a string, indeedÔºàÁöÑÁ°ÆÔºâ the behavior shouldn't change, but for the subtleÔºàÂæÆÂ¶ôÔºâ reason that the condition is always true.
08:46 < energizer> well that's not great... the walrus hasn't made a good first impression on me
08:46 < nedbat> energizer: why not?
08:46 < _amas_> It'd probably make more senseÔºàÊõ¥ÊúâÊÑè‰πâÔºâ to change the conditional to something real, since evenÔºàÂõ†‰∏∫Ôºâ if href is an empty string, bool(''.split('something')) == True
08:46 < squirrel> the only difference i see is split() call is cached here
08:47 < papna> squirrel: Right, the whole ideaÔºàÊÄª‰ΩìÊÑèÊÄùÔºâ here was to prematurelyÔºàÂ∞ΩÊó©ÁöÑÔºâ optimize the repeated split away
08:47 < squirrel> right
08:48 < energizer> nedbat: if the condition is always true (which it is for non- empty strings), we shouldn't be using an `if` at all, since that's misleadingÔºàËØØÂØºÔºâ
08:48 < nedbat> Elodin: key_phone = {}; for href in hrefs: hs = href.split("'"); key_phone[hs[1]] = hs[3]
08:48 < energizer> nedbat: i thought this was the kind of problemÔºàÈóÆÈ¢òÊâÄÂú®Ôºâ := was supposedÔºàÂ∫îËØ•Ôºâ to make more readable, not less
08:48 < _amas_> energizer: I don't think the example I used it in was a good one
08:48 < nedbat> energizer: just because this use was wrong doesn't mean walrus is always bad?
08:49 < energizer> nedbat: i thought this was the sort ofÔºàÈÇ£‰∫õÔºåÈÇ£‰∏ÄÁ±ªÔºâ thing it was supposed to help with. i guess it's more narrowly scopedÔºàÊõ¥Á™ÑÁöÑËåÉÂõ¥Ôºâ than that
08:49 < nedbat> Elodin: sometimes it's best to write it out longhand
08:49 < altendky> energizer: more succinctÔºàÁÆÄÊ¥ÅÔºâ I would think it's use. Which sometimes is more readable and efficientÔºàÊúâÊïàÔºâ because of reduced repetitionÔºàÂáèÂ∞ë‰∫ÜÈáçÂ§çÔºâ.
08:49 < altendky> *is its use
08:50 < nedbat> if you want it in a compÔºàÊØîËæÉÔºâ:  {hs[1]: hs[3] for href in hrefs for h s in [href.split("'")]}   #UGH
08:50 < papna> nedbat: You're my hero.
08:50 < energizer> i've done that before :D

### 2019-10-25

09:16 < blackleitus> I think it occurs in the 5th loop
09:16 < IRCNew> I would like to select these items using faster methods then a for loop since that slows me down a lot
09:17 < IRCNew> item as in the pixel locations
09:17 < grym> IRCNew: yes, read that link
09:17 < grym> IRCNew: and adapt it to your needs
09:17 < IRCNew> that just gets everything I already have
09:17 < IRCNew> or highlights the object
09:18 < IRCNew> which I'm already extracting from an image and highlighting
09:18 < ali1234> look at the second answer
09:19 < ali1234> basically use np.where, and then use np.min/np.max on the result
09:22 < ali1234> if your image is black you can probably just use np.nonzero
09:22 < ali1234> i mean, if the bits you don't care about are black
09:30 < _habnabit> aw1, i'm not sure what you're asking
09:32 < aw1> _habnabit, let me try again. x in set or dict is order(1) but is x in dict.values() also order(1)?
09:32 < _habnabit> aw1, no it is not
09:32 < _habnabit> aw1, dicts don't have any sort of index on their values
09:32 < aw1> _habnabit, thanks
09:36 < Widdershins> aw1: you can think of sets as algorithmically similar to dicts, but without any associated values
09:37 < aw1> sure
09:37 < Widdershins> aw1: in short, to answer your initial question: yes, membership in dict values is O(n)
09:39 < aw1> got that
09:39 < nisstyre> apparently python dicts use random probing to resolve collisions
09:39 < nisstyre> I didn't know that
09:40 < nisstyre> so you would hopefully get roughly the same time complexity even if you had lots of collisions
09:40 < Widdershins> nisstyre: the probing isn't 'random'; it's deterministic. it's quadratic, with hash-based perturbation
09:41 < nisstyre> yeah, I was reading a simplification of it
09:41 < Widdershins> that is, in further iterations the slots probed are determined by upper bits of the hash
09:41 < nisstyre> I'm looking at the comments in the code now
09:41 < Widdershins> yep
09:44 < nisstyre> Widdershins: if you squint hard enough though, j = ((5*j) + 1) mod 2**i is very similar to how an LCG based PRNG works
09:44 < nisstyre> or j = (5*j) + 1 + perturb; rather
09:44 < Widdershins> funny story, ruby 2.4(?) stole the new python dict implementation including the hash probing essentially wholesale
09:45 < nisstyre> I mean, good for Ruby
09:45 < Widdershins> prior to this it was a chaining hash table with a linked list by insertion order and a load factor of 5.0(!)
09:45 < nisstyre> any way they can improve the language is good
09:45 < voldial> a ha! there are my farads https://bpaste.net/show/HP4S6
09:46 < Widdershins> nisstyre: i mean it essentially has the same goals in mind as a PRNG, but with a vastly lower threshold for 'good enough'
09:47 < nisstyre> yep, which is fine
09:47 < nisstyre> because eventually you'll find the correct value, if it exists
09:47 < nisstyre> just has to be good enough for most cases to do it quickly
09:48 < Widdershins> i mean that is always the case even without perturbation, even with linear probing (see: robin hood). the point is that for hashes whose lower bits match it should deterministically diverge as soon as possible.
09:48 < blackleitus> I found the issue is because it doesnt found a paginate `next` every start=0 , 10 , 20 . it will be the same
09:49 < blackleitus> in this case it will be the need of selenium to interact with it right?
10:03 < Kaedenn> I've never learned metaclasses. Where can I go to learn about writing and using metaclasses?
10:03 < Kaedenn> It's one of the few things in core Python I never learned.
10:05 < energizer> consider instead not using them
10:06 < FunkyBob> I gave a talk on them at PyCon NZ... but a better talk was given at PyConAU this year
10:07 < energizer> "metaclasses" means "the type of a class". you can explore __init__, __new__, and __prepare__ by trying them out
10:07 < _habnabit> Kaedenn, i've never made a metaclass since class decorators were added
10:09 < Kaedenn> aha
10:09 < Kaedenn> well  then, looks like that's knowledge I'm comfortable not having, until I need to maintain something using them
10:10 < FunkyBob> there have been some new class-level interfaces added that significantly reduce the number of cases which might justify using a metaclass
10:10 < _habnabit> Kaedenn, they were overrated, and now they're even more overrated
10:10 < FunkyBob> https://www.youtube.com/watch?v=ZpV3tel0xtQ&t=1802s
10:22 < Kaedenn> What happens to PyArg_ParseTupleAndKeywords if kwlist has an entry with a space in it?
10:22 < Kaedenn> static char* kwlist[] = {"eventName ", "physicsClientId", NULL}; specifically
10:29 < Kaedenn> bleh, I want to write a program that generates docstrings based on the source code of these functions, specifically the module definitions, kwlist variable, the argument parsing function, and the variables' default values in the function's scope
10:29 < Kaedenn> this module is terribly documented
10:30 < energizer> is the module written in python?
10:32 < Widdershins> i dunno, i'm looking at the cpython source and haven't necessarily found the place where those names are validated yet; it's possible that they would simply just not match any of the keywords ("eventName " does not match the real kwarg name, "eventName")
10:35 < Widdershins> also here's a possibility Kaedenn: it would perform the call normally, and python's function mechanisms would validate the keyword name as normal. it's possible to call python functions with keywords name of almost any unicode string
10:35 < Kaedenn> huh, interesting
10:35 < Widdershins> example, try in the python repl: def do_it(*args, **kwargs): print(args, kwargs)
10:36 < Widdershins> do_it(**{" what?\0\t  ": 5})
10:36 < Kaedenn> I did not expect that to work. Fascinating.
10:38 < Kaedenn> Although it makes sense that if it would work regardless, then there'd be no enforcing otherwise
10:38 < Widdershins> so it's revealed that keyword argument names can be ANYTHING, not just identifiers, and this is totally fine. it's just *more convenient* for them to be valid identifiers, because there is syncactic sugar for that, which you are used to
10:38 < Widdershins> you can enforce any requirements on your **keyword argument bags long-form by simply raising TypeError explicitly, there are just built-in mechanisms to do so
10:39 < Widdershins> the only thing it seems to do (in python) is enforce that the keywords themselves are of type 'str'
10:40 < Widdershins> which, it seems, there is no mechanism to do otherwise in C
10:40 < Widdershins> so that makes sense
10:40 < _habnabit> Kaedenn, also don't write new code with the crufty old Python.h API
10:40 < Kaedenn> _habnabit: maintaining an existing API. new features go in a side-band python module
10:41 < Widdershins> Kaedenn: for example you can enforce rules about which keywords must be present in what situations in your function, by using kwargs.pop() and then raising TypeError about the arguments when kwargs is non-empty at the end when no more kwargs should have been given
10:41 < Kaedenn> I wish they wrote this using cython instead of Python.h
10:41 < _habnabit> Kaedenn, anything is better than Python.h
10:42 < Kaedenn> Widdershins: too much work. the usual method of parameter handling does 99% of what I need
10:42 < Kaedenn> _habnabit: agreed
10:42 < Widdershins> _habnabit: manually forward-declaring all the python symbols you use yourself üòè
10:42 < _habnabit> Widdershins, it's synecdoche
10:43 < Widdershins> fair
10:43 < tinga> Hi. I'm trying to implement an iterator class. I'm currently not inheriting from any class (since no text I've seen online says what class I should inherit from so). When trying to use my iterator, Python3 says TypeError: 'ListIter' object is not an iterator
10:43 < tinga> (ListIter is the name of my class)
10:43 < xall_> I made a custom MyList class. I want `list(MyList([1]))` to eq [1]. What is the terminology to look up how to do this?
10:44 < tinga> Any idea? Or example? I just couldn't find anything online.
10:44 < Widdershins> xall_: define __iter__ in your MyList class and `yield` the values you want it to represent
10:44 < _habnabit> tinga, xall_, define __iter__ as a method that has a yield/yield from
10:44 < Widdershins> tinga: same thing to you actually, wow that was spooky
10:44 < b1tninja> make it an iterable?
10:44 < b1tninja> idk
10:44 < tinga> Heh
10:45 < Widdershins> your names even hash to the same color in my irc client, and are the same length, i thought you were the same person
10:45 < xall_> just a glitch in the matrix. nothing to see here
10:45 < Widdershins> üêà    üêà
10:46 < tinga> Widdershins, _habnabit, my problem is not that my custom data type doesn't have an __iter__; my problem is that the iterator that I return doesn't satisfy Python.
10:46 < Widdershins> but yes, i recommend *always* making your types iterable by defining __iter__ as a generator function
10:46 < Widdershins> tinga: don't make it *return* anything, write it as a generator function which `yield`s the values
10:47 < tinga> I'm writing this to teach someone how yield works. So I made an example implementation of an iterator class. So I do *not* want to use yield.
10:47 < Widdershins> tinga: xall_: https://wiki.python.org/moin/Generators
10:48 < _habnabit> tinga, there's already examples online
10:48 < Widdershins> tinga: that article has both the clean (yield only) and coarser (__next__ etc.) APIs exemplified
10:49 < Widdershins> typically an iterable returns a new stateful iterator object every time you call __iter__(), and an iterator always returns *itself* when its __iter__ is called, and has __next__ which returns the next value until it is exhausted, at which point it raises StopIteration
10:49 < Widdershins> that's the api
10:49 < Widdershins> generator functions with `yield` wrap up all this functionality for you with a neat little bow
10:50 < tinga> Turns out that I need to call the next method __next__ for python3
10:50 < Widdershins> including stack frame parking, which is a super useful functionality which you'll miss a lot in a lot of other languages
10:50 < xall_> So MyList is actually a LinkedList and I've already defined __iter__ to yield nodes but I need the list conversion to return the values from the node. I wonder if this is workable without changing my existing __iter__
10:50 < Widdershins> tinga: oh yeah it used to just be called `next` in python2 didn't it. that was a mistake.
10:50 < xall_> I'll read up on what you've referenced. Thanks.
10:51 < Widdershins> xall_: if the __iter__ yields all the elements in order, list(your_linked_list_object) will just work as-is (because list() simply consumes every iterated item from the parameter you pass)
10:53 < Widdershins> this also works recursively with trees and stuff. you can yield all the items in a tree... for example, you might iterate a tree in-order like `yield from self.left_child; yield self.value; yield from self.right_child`
10:54 < Widdershins> (`yield from x` basically meaning `for item in x: yield item`)
10:54 < Widdershins> (but... faster? i haven't benchmarked it.)
10:54 < xall_> I'm not sure I can express this without sharing code or getting into the weeds but I'm given unit tests and they're failing because they are expecting [3, 2, 1] but I'm getting [<node>, <node>, <node>]
10:55 < xall_> but my impl of __iter__ is used elsewhere sucessfully
10:57 < xall_> so list(MyList) gives Node but I need Node.value
10:57 < Kaedenn> How do I get argparse to stop parsing at -- ?
10:57 < Kaedenn> Do I preformat sys.argv myself before parsing?
11:00 < xall_> sounds like I need to change the tests or change my __iter__ to work with the tests
11:01 < Widdershins> xall_: you should change the __iter__ for the linked list class to yield values of the nodes, not the nodes themselves
11:01 < Widdershins> or the iterator to return them, as it were
11:01 < xall_> Yeah, I just realized the problem. It was staring me in the face. Sorry for the confusion. Thanks for the help
11:02 < Widdershins> having a data structure which yields its internal nodes (an implementation detail!) is not the design you wanna end up at :)
11:02 < Widdershins> no prob
11:04 < xall_> What's the circumstance for implementing __next__? Is __iter__ usually all you need?
11:04 < Widdershins> in my opinion if you are implementing __next__ it's code smell.
11:05 < Widdershins> it would be a strange situation indeed if you actually *wanted* to do so
11:06 < ammar2> Kaedenn: looks like parser.add_argument('rest', nargs='*') is all you need
11:07 < ammar2> >>> parser.parse_args(['--foo', 'bar', '--', 'asdf', '123', '456'])
11:07 < ammar2> Namespace(foo='bar', rest=['asdf', '123', '456'])
11:08 < Widdershins> xall_: yeah, i can't think of any situation in which you would have to implement next. i don't think one exists.
11:09 < Widdershins> __next__ is purely an implementation detail of iterables in python, and you should never have to call it or define it in your code
11:10 < xall_> Widdershins: roger - thanks
11:14 < Nomad_> Odd one.  I've got a python3 script that has "import requests"  It keeps saying ModuleNotFoundError: No module named 'requests',   but "python3 -m pip install requests" reports it's already installed.  Any ideas?
11:14 < ammar2> pip --force-reinstall requests?
11:15 < Kaedenn> ammar2: thank you
11:15 < Nomad_> trying
11:16 < Nomad_> hmm, saying no such option as "--force-reinstall"
11:16 < beastDivision> This may be less of a python question and more of a linux question, but is there a way to ensure that only one user / one instance of a python script can be executed at a time? meaning if user A ssh's in and runs myscript.py, and user B also SSH's in, they are blocked from running myscript.py if user A is still running it.
11:18 < Widdershins> Nomad_: are you sure you're installing requests in the same python3 interpreter that's running the script?
11:19 < Nomad_> if I do a pip3 freeze, I can see requests listed
11:19 < Nomad_> Widdershins: I'm running it in a docker container
11:19 < Widdershins> beastDivision: you are probably better off arbitrating that restriction explicitly inside the script itself, by taking out a central lock somewhere available to everyone before running
11:19 < ammar2> beastDivision: usually you'd use a lock file
11:19 < ammar2> like apt and /var/dpkg/lock or whatever
11:19 < Widdershins> (not that specific lock file lol)
11:19 < Widdershins> (i mean you could... it would work... <__<)
11:20 < beastDivision> I'm tracking, yeah that seems like a reasonable solution. thank you
11:20 < Widdershins> but yeah, taking out a lock file in a commonly accessible folder, or maybe even on an external service like a database or something
11:21 < Widdershins> or redis
11:21 < Widdershins> whatever fits your use case
11:23 < energizer> Nomad_: if you use `python3 -m pip install requests` and then use `python3 myscript.py` do you get the same error
11:24 < energizer> Nomad_: if you use `python3 -m pip install --force requests` and then use `python3 myscript.py` do you get the same error
11:24 < Nomad_> energizer: testing
11:28 < Nomad_> interesting, Docker has python3.6 in /usr/local/bin and python3.7 in /usr/bin
11:30 < ammar2> which docker image?
11:32 < Nomad_> I was explicitly using python3.6, just changed to python3.7, but same versions exist, odd
11:33 < offby1> yeah, saying "Docker has" is kinda like saying "Linux has"
11:33 < dreemer> Wow, PyGObject is pretty awesome
11:33 < Kaedenn> Can I use tkinter to make an open file dialog?
11:34 < Nomad_> I'm using the default python3.6 and python3.7 from dockerhub
11:34 < Kaedenn> ...actually, I can combine tkinter and zenity because this library recommends zenity anyway
11:45 < c_nick> Hi i wanted to create a generic get function which will return me back the mentioned private variable in the class something like def get(self,name): return self.__name
11:48 < Kaedenn> c_nick: 1) why? That sounds like a bad idea. 2) getattr(self, "__" + name)
11:48 < Kaedenn> 3) __ is special. That might be weird. I haven't used __ attributes in forever.
11:50 < ammar2> it's not
11:50 < ammar2> as a prefix, it's just a convention
11:51 < russw> where are all the python build options documented? e.g. `--enable-loadable-sqlite-extensions`.  Is there anything beyond sifting through the `configure` file and googling around for changenotes when they were added?
11:51 < ammar2> some dunder methods and attributes are special tho
11:51 < c_nick> Kaedenn, I want to make a class which will have few constants but i dont want them to be accessable from outside .. rather let it be accessible from the class itself a leading _ can be accessed from outside however __ doesnt
11:51 < ammar2> russw: ./configure --help doesn't list all of them?
11:52 < Kaedenn> c_nick: I recommend forgetting the idea of protecting your innards from the world
11:52 < ammar2> but yeah, usually there's not too much of a description beyond the name of the flag
11:52 < Kaedenn> Python doesn't have a neat way to do private attributes. A single leading underscore is good enough to tell people not to muck around with the values.
11:52 < ammar2> are you trying to find something in particular?
11:52 < russw> ammar2: it does list them, but I'm looking for more detail.
11:53 < ammar2> then your strategy is probably the best
11:53 < Kaedenn> If someone wants to mess with _ attributes, let them. The program will break and it'll be their fault. Morale of the story: don't muck with _ attributes unless you own the object in question, or know what you're doing
11:53 < russw> ammar2: nothing in particular. Just perusing options. e.g. that `--enable-loadable-sqlite-extensions` one is interesting and I'm looking into.
11:54 < Kaedenn> The only way I know to implement truly private attributes in Python is to write the code in a language other than Python, such as C or whatever, and expose only the functions and data to Python.
11:54 < ammar2> c_nick: __ can also be accessed from the outside though
11:54 < Kaedenn> (as in, private in the sense of C++ or Java)
11:54 < c_nick> ammar2 i couldnt using Python 3.6.x
11:55 < Kaedenn> (as in, private only in terms of obscurity and implementation details and madness)
11:55 < ammar2> >>> a = A()
11:55 < ammar2> >>> a.__x__ = 1
11:55 < ammar2> >>> a.__x__
11:55 < ammar2> 1
11:55 < c_nick> Kaedenn whats the difference between _ and __ ?
11:55 < ammar2> what error do you get?
11:55 < Kaedenn> c_nick: _ means private. __ means special.
11:55 < Kaedenn> By convention.
11:55 < russw> c_nick: the double _ just adds some obfuscation. Nothing more.
11:56 < Kaedenn> c_nick: you can still get at attributes with two leading underscores. dir(), getattr(), etc. It's perfectly possible.
11:56 < russw> c_nick: basically just a hint to the reader that you *really* shouldn't be using that "private" var
11:56 < Kaedenn> Use a single one and be happy.
11:56 < c_nick> hmm ok .
11:56 < Kaedenn> c_nick: now if you have problems with single underscores, then the problem is elsewhere in your code :)
11:57 < russw> Kaedenn: or the person who wrote the code you are using. :)
11:57 < c_nick> ammar2 if you declare __name and then try MyClass().__name you get object does not have an attribute __name
11:57 < ammar2> no?
11:57 < ammar2> >>> a.__name = 'abcd'
11:57 < ammar2> >>> a.__name
11:57 < ammar2> 'abcd'
11:57 < ammar2> you might have made a mistake elsewhere
11:57 < Kaedenn> c_nick: Not true. Well, true only in specific situations.
11:58 < c_nick> class MyClass():        __name = 'Nimish' and then print(MyClass().__name)
11:58 < Kaedenn> ammar2: difference is (I believe) self.__name = value vs object.__name = value in __init__, but it's been too log
11:58 < Kaedenn> long*
11:58 < ammar2> oh weird
11:58 < Kaedenn> c_nick: ... __name should be self.__name
11:58 < ammar2> it totally does!
11:59 < Kaedenn> c_nick: in that example __name is a class attribute not an attribute of the instance
11:59 < Kaedenn> MyClass.__name would work, MyClass().__name does not as you see
11:59 < Kaedenn> c_nick: also it's not inside a function, so it's a class attribute regardless of what you do to it
11:59 < c_nick> even if i do MyClass.__name it doesnt work
12:00 < russw> ammar2: observe the obfuscation here: https://bpaste.net/show/SLDWA
12:01 < Kaedenn> c_nick: because MyClass.__name is really MyClass._MyClass__name
12:01 < Kaedenn> c_nick: again, do not use __ attributes.
12:01 < ammar2> c_nick: https://docs.python.org/3/tutorial/classes.html?highlight=name%20mangling#private-variables
12:02 < Kaedenn> c_nick: To get that attribute from the outside you'll need to mangle it manually. To ensure you get it from the inside you'd need to do the same thing. Just use a single underscore and be happy.
12:02 < c_nick> ok
12:03 < Kaedenn> Name-mangled attributes are fickle to deal with and not worth it in my opinion. They're available to prevent objects from inheriting them and for child classes to define their own versions of them
12:03 < Kaedenn> So a base class and a child class can both have a __name attribute that's unique to their own class
12:04 < c_nick> ok thanks guys
12:04 < Kaedenn> Long story short: don't use them unless you have to use them (avoiding name collision between inheriting objects)
12:05 < Kaedenn> also, afaik, __obj__ attributes are not mangled. only __obj attributes.
12:06 < ammar2> Kaedenn: yeah, the linked docs there say: "(at least two leading underscores, at most one trailing underscore)"
12:06 < energizer> __these__ are supposed to be reserved for use by the language
12:06 < Kaedenn> ^
12:08  * c_nick be back later
12:08 < c_nick> bdw how to send message in chat to someone
12:09 < c_nick> i used to use Gnone earlier but now in Mirc i dont think the tab works
12:10 < c_nick> Kaedenn: thanks
12:10 < c_nick> ammar2: thanks
12:15 < blackleitus> how can I keep the same array json structure after deleting duplicates https://dpaste.de/CjdF# ?
12:19 < Wulf> blackleitus: by looking at the example only, it appears like  result = orig["jobs"][0]
12:22 < heyitsrama> blackleitus: why is it not unique(self.data_extracted['jobs']
12:39 < blackleitus> I have like this `for d in unique(self.data_extracted['jobs']):` , but I want to keep the same structure as before [{"jobs":{}}]`
12:41 < papna> blackleitus: You open the file inside your loop. Each time, you write the whole file with just `d`, so only the last one will be around when you're done
12:41 < papna> blackleitus: Presumably you want to write something bigger to the file.
12:42 < papna> blackleitus: I don't know what your actual code does or what unique is, but perhaps you want `with open(self.file, 'w') as f: json.dump(unique(self.data_extracted)) instead of lines 13-15
12:43 < papna> *dump(unique(self.data_extracted), f, indent=4)
12:45 < blackleitus> the for it only deletes duplicates in json
12:46 < blackleitus> I only want to remove duplicates , but keep the same structure
12:46 < Wulf> blackleitus: provide a better example
12:47 < blackleitus> https://dpaste.de/vOpd
12:47 < blackleitus> imagine the first one contains a duplicate json , then remove , but looks like the after json
12:47 < Wulf> blackleitus: provide an even better example, one with values.
12:47 < blackleitus> okay let me ran the script
12:48 < Wulf> blackleitus: input should have at least 3 docs, output at least 2.
12:49 < blackleitus> https://dpaste.de/WLmJ
12:54 < Wulf> {"jobs": [dict(x) for x in frozenset(frozenset(x.items()) for x in your_input["jobs"])]}
12:54 < Wulf> might work, might not.
12:54 < dreemer> ...
12:55 < rocketmagnet> hi everyone
12:55 < rocketmagnet> is here someone that knows javascript/jquery ? i try to send a json-rpc request with jquery to my json-rpc server
12:56 < blackleitus> this case how ?
12:57 < Wulf> rocketmagnet: wrong channel?
12:57 < dreemer> This is a snake channel, not coffee.
12:58 < rocketmagnet> right, i just thought someone might can translate r = requests.post('http://localhost:4000', data=json.dumps({"jsonrpc": "2.0", "method": "simple_add", "params": {"first": 10, "second": 20}, "id": 1})) into jquery.ajax
12:59 < Wulf> rocketmagnet: requests.post(url, json={...})
13:04 < ammar2> $.post('http://localhost:4000', {"jsonrpc": "2.0", "method": "simple_add", "params": {"first": 10, "second": 20}, "id": 1})
13:04 < ammar2> pretty much the same
13:15 < nabil__> anyone with experience with nats here ?

### 2019-12-11
Prerequisites

"Hilo" is a series of articles and sample applications that show how you can leverage the power of Windows 7, using the Visual Studio 2010 and Visual C++ development systems to build high performance, responsive rich client applications. 

Windows Template Library (WTL) is a C++ library for developing Windows applications and UI components. It extends ATL (Active Template Library) and provides a set of classes for controls, dialogs, frame windows, GDI objects, and more.


I written it two years ago for that I was astonished by Everything's search speed.

Everything is a proprietary freeware Windows desktop search engine that can rapidly find files and folders by name on an NTFS volume.


### 2019-12-12

I tried using the change journal to easily detect changes made on an NTFS volume, but the change journal would show a lot of records, most of them relating to small temporary files created and destroyed. 

You can enumerate all the files on a volume using FSCTL_ENUM_USN_DATA. This is a fast process (my tests returned better than 6000 records per second even on a very old machine, and 20000+ is more typical) and only includes files that currently exist.

One approach would be to use a buffer large enough to hold all the file records simultaneously, and search through the records to find the matching parent for each file you need to back up.

For large volumes you would probably need to process the directory records into a more efficient data structure, perhaps a hash table.

Alternately, you can read/reread the records for the parent directories as needed. This would be less efficient, but the performance might still be satisfactory depending on how many files are being backed up. 

Windows does appear to cache the data returned by FSCTL_ENUM_USN_DATA.


Because Windows creates and deletes a lot of temporary files, and it would be futile to see if they are still in existence or not.


I work in the technical support group of the School of Computing and Mathematical Sciences, The University of Waikato, New Zealand. My primary responsibility in this role is maintenance and support for several computer labs, comprising over 200 Windows PCs used for teaching.

I am married with two sons, aged 15 and 12.

I can be contacted at harry.maurice.johnston@gmail.com.

Actually, you can use the USN returned by this method to determine which files are changed since the last run. However, you still need to read the journal once to get the current USN before starting the scan (otherwise modifications made during the scan could be missed on both this and the subsequent scan). And don't the the USN filtering capability of MFT_ENUM_DATA, you do need to enumerate all records in order to get information on parent directories.

Hey Ben, in my last implementation, I used to store the USN record number in the database. But yes, we have to store the file reference number and the parent file reference number of each file and query it to get the path.

@Ben, the call to FSCTL_QUERY_USN_JOURNAL provides you with the current USN. You don't actually need to read the journal. (I'm not saying it might not be preferable to do so, but I don't think it is essential.) ‚Äì

I think it might be reasonable to use the USN filtering in FSCTL_ENUM_USN_DATA to find the changed files, then make separate calls to look up the parent directories. That way you'd only be processing data for parent directories that you actually needed. I'm not sure about performance, but I gather the MFT is pretty well optimised, so as long as you cache the data you get I think you might be OK

this sample code will be greatly improved in speed if within show_record() you don't read a megabyte worth of data but only a single USN record 


 the size of a USN record is variable, so that's a bit tricky - it isn't just the filename, it is also documented that extra members might be added to the structure in future releases. On the other hand, we don't need a megabyte, a kilobyte would be plenty. In production code you might want to further optimize, e.g., start by assuming that there are no extra entries and only increase the read size if it actually turns out to be necessary.
 
 
 enumerating the MFT is always read-only, and if non-administrators could do it, they could, e.g., see file names in other user's homes. That would violate the security model.
 
 
 An automatic backup application is one example of a program that must check for changes to the state of a volume to perform its task. The brute force method of checking for changes in directories or files is to scan the entire volume. However, this is often not an acceptable approach because of the decrease in system performance it would cause. Another method is for the application to register a directory notification (by calling the FindFirstChangeNotification or ReadDirectoryChangesW functions) for the directories to be backed up. This is more efficient than the first method, however, it requires that an application be running at all times. Also, if a large number of directories and files must be backed up, the amount of processing and memory overhead for such an application might also cause the operating system's performance to decrease.

 
 To avoid these disadvantages, the NTFS file system maintains an update sequence number (USN) change journal. When any change is made to a file or directory in a volume, the USN change journal for that volume is updated with a description of the change and the name of the file or directory.
 
 
 To avoid these disadvantages, the NTFS file system maintains an update sequence number (USN) change journal. When any change is made to a file or directory in a volume, the USN change journal for that volume is updated with a description of the change and the name of the file or directory.
 
 
 Change journals are also needed to recover file system indexing for example after a computer or volume failure. The ability to recover indexing means the file system can avoid the time-consuming process of reindexing the entire volume in such cases.
 
 
 As files, directories, and other NTFS file system objects are added, deleted, and modified, the NTFS file system enters change journal records in streams, one for each volume on the computer. Each record indicates the type of change and the object changed. The offset from the beginning of the stream for a particular record is called the update sequence number (USN) for the particular record. New records are appended to the end of the stream.

The NTFS file system may delete old records to conserve space. If needed records have been deleted, the indexing service recovers by re-indexing the volume, as it does when no change journal exists.

The change journal logs only the fact of a change to a file and the reason for the change (for example, write operations, truncation, lengthening, deletion, and so on). It does not record enough information to allow reversing the change.

In addition, multiple changes to the same file may result in only one reason flag being added to the current record. If the same kind of change occurs more than once, the NTFS file system does not write a new record for the changes after the first. For example, several write operations with no intervening close and reopen operations result in only one change record with the reason flag USN_REASON_DATA_OVERWRITE set.

To illustrate how the change journal works, suppose a user accesses a file in the following order:


The change journal accumulates a series of records between the first opening and last closing of a file. Each record has a new reason flag set, indicating that a new kind of change has occurred. The sequence of records gives a partial history of the file. The final record, created when the file is closed, adds the USN_REASON_CLOSE flag. This record represents a summary of changes to the file, but unlike the prior records, gives no indication of the order of the changes.


### 2019-12-13

WizFile is a very rapid file search utility. Search for files on your hard drive by name and get the results almost instantly. It uses the same technology as WizTree to scan for files so it's very quick indeed!

Find files by name on your hard drives almost instantly
VERY Fast scanning! WizFile reads the master file table (MFT) directly from NTFS formatted hard drives so it's ready for use very quickly after start up.
Search by file name or full path name
Multiple search terms supported
Wild card file search (* and ? wild cards supported)
User defined filters
Monitors for file changes while active - your visible file search results will always be up to date
Keeps track of folder sizes - any changes made to the file system will automatically update the folder sizes
Does not require a separate database file - all file data is kept in memory


I need to access Windows MFT(Master File Table) using C# in my .net application.
I have googled about this and couldn't find any good results. I have been searching for the information from the past 2 days but have been unable to find any information on the same.

I am not looking for exact code to do the same, I am just looking for some information which can get me started.

The only thing I have been able to figure out is that I have to use P/Invoke.
I want to know the functions I would be using to have access to MFT.
If you are able to provide some code sample then that would be great.

First, you have to have and assert sufficient privileges to access the MFT - this is a pain all by itself. Then, you have to get a handle to a file/folder on the volume	

for the calls in the last step...which is to call a Windows API (called DeviceIOControl) in a loop and read the entries from the returned API call 
and this is it's own special headache.

Conceptually - this looks like:

If you take each of these in turn, asserting sufficient privileges is the most obscure part. 

There's a Windows API to change the privileges of the running token - 

and you use that to add the necessary privileges. 

Here's an excerpt from a class that I use to assert those privileges. 
You could assert a bunch more privileges
but this should be sufficient for reading the MFT.

First time this method is called, it attempts to set backup privileges for the current process.
Subsequently, it returns the results of that first call.

Once you're past that hurdle, the rest is - well...still a festival of obscurity.

You have to get a handle to a file or folder - with backup semantics.

You can more-than-likely just open a FileStream on any old file on the volume you're after and the FileStream will have a handle you can use for subsequent calls. 

This isn't precisely what my application did - but my app had to do things this answer doesn't have to do.

NTFS - "New Technology File System" is the preferred native file system for Windows NT series.

It is a more sophisticated, powerful and complicated file system than FAT file system.

It is much efficient for larger disks. 

This project aims at providing an API in .NET to gather directory/files information of an NTFS volume very efficiently.

(in contrast to the classic approach like FindFirst/FindNext which is really slow on large directories.


### 2019-12-20

Specify how much of the available CPU resources a container can use. For instance, if the host machine has two CPUs and you set --cpus="1.5", the container is guaranteed at most one and a half of the CPUs. This is the equivalent of setting --cpu-period="100000" and --cpu-quota="150000". Available in Docker 1.13 and higher.

Specify the CPU CFS scheduler period, which is used alongside --cpu-quota. Defaults to 100 micro-seconds. Most users do not change this from the default. If you use Docker 1.13 or higher, use --cpus instead.

Impose a CPU CFS quota on the container. The number of microseconds per --cpu-period that the container is limited to before throttled. As such acting as the effective ceiling. If you use Docker 1.13 or higher, use --cpus instead.

Set this flag to a value greater or less than the default of 1024 to increase or reduce the container‚Äôs weight, and give it access to a greater or lesser proportion of the host machine‚Äôs CPU cycles. This is only enforced when CPU cycles are constrained. When plenty of CPU cycles are available, all containers use as much CPU as they need. In that way, this is a soft limit. --cpu-shares does not prevent containers from being scheduled in swarm mode. It prioritizes container CPU resources for the available CPU cycles. It does not guarantee or reserve any specific CPU access.


CPU shares determines the allocation of processing power allocated to a Virtual Machine in the Virtual Machine in the Cloud service.

The shares value represents the percentage of one CPU or "core" of the physical server. eApps Hosting uses dual quad core servers, meaning that the physical server has 8 CPU units. A technical feature of these servers, called hyperthreading, allows the physical server to appear to actually have 16 CPU units. Hyperthreading takes advantage of the fact that CPUs are often idle, waiting for other relatively slower processes such as disk drive accces, to complete. 

So for the calculation of CPU power, assume that the physical server really has 16 CPUs. On this basis, CPU shares represents the allocation of those 16 CPUs. For example, a CPU share of 100 means that the virtual machine receives the equivalent of 100% of one CPU, or 1/16th of the total power of the physical machine. A CPU share of 200 represents 200% or two full CPUs, ie 1/8th of the power of the total physical server. A CPU share of 400 represents 100% of 4 (400%) CPUs, or 1/4 of the of the total power (1600) of the machine.

To reserve a given amount of memory or number of CPUs for a service, use the --reserve-memory or --reserve-cpu flags. If no available nodes can satisfy the requirement (for instance, if you request 4 CPUs and no node in the swarm has 4 CPUs), the service remains in a pending state until an appropriate node is available to run its tasks.

### 2019-12-21

‚ÄúNatural Language Processing with Python‚Äù (read my review) has lots of motivating examples for natural language processing. I quickly found it valuable to build indices ahead of time ‚Äì I have a corpus of legal texts, and build a set of n-gram indices from it.

The first index is a list of just tokenized text, with all text contents combined. You may wish to add a special token that indicates where files end.

This filters the tokens ‚Äì depending on the situation you may wish to modify this (e.g. whether you want punctuation or not).

The following turns the token index into an n-gram index. This runs pretty quickly while there is memory ‚Äì this could easily be extended to work on parts of a document set, and combine with the results with a map-reduce operation by addition, especially if the output was sorted. One surprising result is that the 5-gram index can be quite a bit larger than the original data.


### 2019-12-23

Apache Flink¬Æ ‚Äî Stateful Computations over Data Streams


All streaming use cases
Event-driven Applications
Stream & Batch Analytics
Data Pipelines & ETL

Guaranteed correctness
Exactly-once state consistency
Event-time processing
Sophisticated late data handling

Layered APIs
SQL on Stream & Batch Data
DataStream API & DataSet API
ProcessFunction (Time & State)

Operational Focus
Flexible deployment
High-availability setup
Savepoints

Scales to any use case
Scale-out architecture
Support for very large state
Incremental checkpointing

Excellent Performance
Low latency
High throughput
In-Memory computing

Apache Flink is an excellent choice to develop and run many different types of applications due to its extensive features set. Flink‚Äôs features include support for stream and batch processing, sophisticated state management, event-time processing semantics, and exactly-once consistency guarantees for state. Moreover, Flink can be deployed on various resource providers such as YARN, Apache Mesos, and Kubernetes but also as stand-alone cluster on bare-metal hardware. Configured for high availability, Flink does not have a single point of failure. Flink has been proven to scale to thousands of cores and terabytes of application state, delivers high throughput and low latency, and powers some of the world‚Äôs most demanding stream processing applications.

### 2019-12-30

A JupyterLab extension for VNC.

This extension will not work as a standalone.

It is meant to be used in the Docker image created with docker/Dockerfile.

This extension creates a menu, palette and a launcher for opening a VNC frame in a Jupyter tab.
This extension does not provide a VNC server. The VNC server must be started by the docker container in which this extension is installed.


### 2019-12-31

There is currently no way to select the parent of an element in CSS.

If there was a way to do it, it would be in either of the current CSS selectors specs:

Selectors Level 3 Spec
CSS 2.1 Selectors Spec
In the meantime, you'll have to resort to JavaScript if you need to select a parent element.

One of the important functions of CSS is to position elements. Margin, padding, top, left, right, bottom, position, and z-index are just a few of the major players in CSS positioning. By using the above spacing properties and the z-index "layering" property, we can make your elements overlap each other.