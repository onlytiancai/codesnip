# The Little Book of Linear Algebra
çº¿æ€§ä»£æ•°å°å†Œå­

A concise, beginner-friendly introduction to the core ideas of linear algebra.
ç®€æ´ã€é€‚åˆåˆå­¦è€…çš„çº¿æ€§ä»£æ•°æ ¸å¿ƒæ€æƒ³ä»‹ç»ã€‚

## Formats
æ ¼å¼

*   [Download PDF](book.pdf) â€“ print-ready version
    [ä¸‹è½½ PDF](book.pdf) â€“ å¯æ‰“å°ç‰ˆæœ¬
*   [Download EPUB](book.epub) â€“ e-reader friendly
    [ä¸‹è½½ EPUB](book.epub) â€“ é€‚åˆç”µå­é˜…è¯»å™¨
*   [View LaTeX](book.tex) â€“ Latex source
    [æŸ¥çœ‹ LaTeX](book.tex) â€“ Latex æº

# Chapter 1. Vectors
ç¬¬ 1 ç«  å‘é‡

## 1.1 Scalars and Vectors
1.1 æ ‡é‡å’ŒçŸ¢é‡

A scalar is a single numerical quantity, most often taken from the real numbers, denoted by $\mathbb{R}$. Scalars are the fundamental building blocks of arithmetic: they can be added, subtracted, multiplied, and, except in the case of zero, divided. In linear algebra, scalars play the role of coefficients, scaling factors, and entries of larger structures such as vectors and matrices. They provide the weights by which more complex objects are measured and combined. A vector is an ordered collection of scalars, arranged either in a row or a column. When the scalars are real numbers, the vector is said to belong to *real* $n$\-dimensional space, written
æ ‡é‡æ˜¯ä¸€ä¸ªå•ä¸€çš„æ•°å€¼ï¼Œé€šå¸¸å–è‡ªå®æ•°ï¼Œç”¨ $\mathbb{R}$ è¡¨ç¤ºã€‚æ ‡é‡æ˜¯ç®—æœ¯çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ï¼šå®ƒä»¬å¯ä»¥è¿›è¡ŒåŠ ã€å‡ã€ä¹˜å’Œé™¤ï¼ˆé›¶é™¤å¤–ï¼‰ã€‚åœ¨çº¿æ€§ä»£æ•°ä¸­ï¼Œæ ‡é‡å……å½“ç³»æ•°ã€æ¯”ä¾‹å› å­ä»¥åŠå‘é‡å’ŒçŸ©é˜µç­‰æ›´å¤§ç»“æ„ä¸­çš„å…ƒç´ ã€‚å®ƒä»¬æä¾›æƒé‡ï¼Œç”¨äºæµ‹é‡å’Œç»„åˆæ›´å¤æ‚çš„å¯¹è±¡ã€‚å‘é‡æ˜¯æŒ‰è¡Œæˆ–åˆ—æ’åˆ—çš„æ ‡é‡çš„æœ‰åºé›†åˆã€‚å½“æ ‡é‡ä¸ºå®æ•°æ—¶ï¼Œè¯¥å‘é‡è¢«ç§°ä¸ºå±äº*å®* $n$ ç»´ç©ºé—´ï¼Œå†™ä¸º

$$
\mathbb{R}^n = \{ (x_1, x_2, \dots, x_n) \mid x_i \in \mathbb{R} \}.
$$

An element of $\mathbb{R}^n$ is called a vector of dimension $n$ or an *n*\-vector. The number $n$ is called the dimension of the vector space. Thus $\mathbb{R}^2$ is the space of all ordered pairs of real numbers, $\mathbb{R}^3$ the space of all ordered triples, and so on.
$\mathbb{R}^n$ ä¸­çš„ä¸€ä¸ªå…ƒç´ ç§°ä¸ºç»´åº¦ä¸º $n$ çš„å‘é‡æˆ– *n* å‘é‡ã€‚æ•°å­— $n$ ç§°ä¸ºå‘é‡ç©ºé—´çš„ç»´æ•°ã€‚å› æ­¤ï¼Œ $\mathbb{R}^2$ æ˜¯æ‰€æœ‰æœ‰åºå®æ•°å¯¹çš„ç©ºé—´ï¼Œ $\mathbb{R}^3$ æ˜¯æ‰€æœ‰æœ‰åºä¸‰å…ƒç»„çš„ç©ºé—´ï¼Œç­‰ç­‰ã€‚

Example 1.1.1.
ä¾‹ 1.1.1ã€‚

*   A 2-dimensional vector: $(3, -1) \in \mathbb{R}^2$.
    äºŒç»´å‘é‡ï¼š $(3, -1) \in \mathbb{R}^2$ ã€‚
*   A 3-dimensional vector: $(2, 0, 5) \in \mathbb{R}^3$.
    ä¸‰ç»´å‘é‡ï¼š $(2, 0, 5) \in \mathbb{R}^3$ ã€‚
*   A 1-dimensional vector: $(7) \in \mathbb{R}^1$, which corresponds to the scalar 7 itself.
    ä¸€ç»´å‘é‡ï¼š $(7) \in \mathbb{R}^1$ ï¼Œå¯¹åº”äºæ ‡é‡ 7 æœ¬èº«ã€‚

Vectors are often written vertically in column form, which emphasizes their role in matrix multiplication:
å‘é‡é€šå¸¸ä»¥åˆ—çš„å½¢å¼å‚ç›´ä¹¦å†™ï¼Œè¿™å¼ºè°ƒäº†å®ƒä»¬åœ¨çŸ©é˜µä¹˜æ³•ä¸­çš„ä½œç”¨ï¼š

$$
\mathbf{v} = \begin{bmatrix}2 \\0 \\5 \end{bmatrix} \in \mathbb{R}^3.
$$

The vertical layout makes the structure clearer when we consider linear combinations or multiply matrices by vectors.
å½“æˆ‘ä»¬è€ƒè™‘çº¿æ€§ç»„åˆæˆ–çŸ©é˜µä¹˜ä»¥å‘é‡æ—¶ï¼Œå‚ç›´å¸ƒå±€ä½¿ç»“æ„æ›´åŠ æ¸…æ™°ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

In $\mathbb{R}^2$, a vector $(x_1, x_2)$ can be visualized as an arrow starting at the origin $(0,0)$ and ending at the point $(x_1, x_2)$. Its length corresponds to the distance from the origin, and its orientation gives a direction in the plane. In $\mathbb{R}^3$, the same picture extends into three dimensions: a vector is an arrow from the origin to $(x_1, x_2, x_3)$. Beyond three dimensions, direct visualization is no longer possible, but the algebraic rules of vectors remain identical. Even though we cannot draw a vector in $\mathbb{R}^{10}$, it behaves under addition, scaling, and transformation exactly as a 2- or 3-dimensional vector does. This abstract point of view is what allows linear algebra to apply to data science, physics, and machine learning, where data often lives in very high-dimensional spaces. Thus a vector may be regarded in three complementary ways:
åœ¨ $\mathbb{R}^2$ ä¸­ï¼Œå‘é‡ $(x_1, x_2)$ å¯ä»¥å¯è§†åŒ–ä¸ºä¸€ä¸ªä»åŸç‚¹ $(0,0)$ å¼€å§‹åˆ°ç‚¹ $(x_1, x_2)$ ç»“æŸçš„ç®­å¤´ã€‚å®ƒçš„é•¿åº¦å¯¹åº”äºä¸åŸç‚¹çš„è·ç¦»ï¼Œå…¶æ–¹å‘ç»™å‡ºäº†å¹³é¢å†…çš„æ–¹å‘ã€‚åœ¨ $\mathbb{R}^3$ ä¸­ï¼ŒåŒæ ·çš„å›¾åƒå»¶ä¼¸åˆ°ä¸‰ç»´ç©ºé—´ï¼šå‘é‡æ˜¯ä¸€ä¸ªä»åŸç‚¹æŒ‡å‘ $(x_1, x_2, x_3)$ çš„ç®­å¤´ã€‚è¶…è¿‡ä¸‰ç»´ç©ºé—´åï¼Œç›´æ¥å¯è§†åŒ–å°±ä¸å†å¯èƒ½ï¼Œä½†å‘é‡çš„ä»£æ•°è§„åˆ™ä¿æŒä¸å˜ã€‚å³ä½¿æˆ‘ä»¬æ— æ³•åœ¨ $\mathbb{R}^{10}$ ä¸­ç»˜åˆ¶å‘é‡ï¼Œå®ƒåœ¨åŠ æ³•ã€ç¼©æ”¾å’Œå˜æ¢ä¸‹çš„è¡Œä¸ºä¸äºŒç»´æˆ–ä¸‰ç»´å‘é‡å®Œå…¨ç›¸åŒã€‚è¿™ç§æŠ½è±¡çš„è§‚ç‚¹ä½¿å¾—çº¿æ€§ä»£æ•°èƒ½å¤Ÿåº”ç”¨äºæ•°æ®ç§‘å­¦ã€ç‰©ç†å­¦å’Œæœºå™¨å­¦ä¹ ï¼Œè¿™äº›é¢†åŸŸçš„æ•°æ®é€šå¸¸å­˜åœ¨äºéâ€‹â€‹å¸¸é«˜ç»´çš„ç©ºé—´ä¸­ã€‚å› æ­¤ï¼Œå‘é‡å¯ä»¥ä»ä¸‰ä¸ªäº’è¡¥çš„è§’åº¦æ¥çœ‹å¾…ï¼š

1.  As a point in space, described by its coordinates.
    ä½œä¸ºç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ï¼Œç”±å…¶åæ ‡æè¿°ã€‚
2.  As a displacement or arrow, described by a direction and a length.
    ä½œä¸ºä½ç§»æˆ–ç®­å¤´ï¼Œç”±æ–¹å‘å’Œé•¿åº¦æè¿°ã€‚
3.  As an abstract element of a vector space, whose properties follow algebraic rules independent of geometry.
    ä½œä¸ºå‘é‡ç©ºé—´çš„æŠ½è±¡å…ƒç´ ï¼Œå…¶å±æ€§éµå¾ªä¸å‡ ä½•æ— å…³çš„ä»£æ•°è§„åˆ™ã€‚

### Notation
ç¬¦å·

*   Vectors are written in boldface lowercase letters: $\mathbf{v}, \mathbf{w}, \mathbf{x}$.
    å‘é‡ä»¥ç²—ä½“å°å†™å­—æ¯è¡¨ç¤ºï¼š $\mathbf{v}, \mathbf{w}, \mathbf{x}$ ã€‚
*   The *i*\-th entry of a vector $\mathbf{v}$ is written $v_i$, where indices begin at 1.
    å‘é‡ $\mathbf{v}$ çš„ç¬¬ - ä¸ªå…ƒç´ å†™ä¸º ğ‘£ ğ‘– v i â€‹ ï¼Œå…¶ä¸­ç´¢å¼•ä» 1 å¼€å§‹ã€‚
*   The set of all *n*\-dimensional vectors over $\mathbb{R}$ is denoted $\mathbb{R}^n$.
    $\mathbb{R}$ ä¸Šçš„æ‰€æœ‰ *n* ç»´å‘é‡çš„é›†åˆè®°ä¸º $\mathbb{R}^n$ ã€‚
*   Column vectors will be the default form unless otherwise stated.
    é™¤éå¦æœ‰è¯´æ˜ï¼Œåˆ—å‘é‡å°†æ˜¯é»˜è®¤å½¢å¼ã€‚

### Why begin here?
ä¸ºä»€ä¹ˆä»è¿™é‡Œå¼€å§‹ï¼Ÿ

Scalars and vectors form the atoms of linear algebra. Every structure we will build-vector spaces, linear transformations, matrices, eigenvalues-relies on the basic notions of number and ordered collection of numbers. Once vectors are understood, we can define operations such as addition and scalar multiplication, then generalize to subspaces, bases, and coordinate systems. Eventually, this framework grows into the full theory of linear algebra, with powerful applications to geometry, computation, and data.
æ ‡é‡å’Œå‘é‡æ„æˆäº†çº¿æ€§ä»£æ•°çš„åŸå­ã€‚æˆ‘ä»¬å°†è¦æ„å»ºçš„æ¯ä¸€ä¸ªç»“æ„â€”â€”å‘é‡ç©ºé—´ã€çº¿æ€§å˜æ¢ã€çŸ©é˜µã€ç‰¹å¾å€¼â€”â€”éƒ½ä¾èµ–äºæ•°å’Œæœ‰åºæ•°é›†çš„åŸºæœ¬æ¦‚å¿µã€‚ä¸€æ—¦ç†è§£äº†å‘é‡ï¼Œæˆ‘ä»¬å°±å¯ä»¥å®šä¹‰è¯¸å¦‚åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•ä¹‹ç±»çš„è¿ç®—ï¼Œç„¶åæ¨å¹¿åˆ°å­ç©ºé—´ã€åŸºå’Œåæ ‡ç³»ã€‚æœ€ç»ˆï¼Œè¿™ä¸ªæ¡†æ¶å°†å‘å±•æˆä¸ºå®Œæ•´çš„çº¿æ€§ä»£æ•°ç†è®ºï¼Œå¹¶åœ¨å‡ ä½•ã€è®¡ç®—å’Œæ•°æ®é¢†åŸŸæ‹¥æœ‰å¼ºå¤§çš„åº”ç”¨ã€‚

### Exercises 1.1
ç»ƒä¹  1.1

1.  Write three different vectors in $\mathbb{R}^2$ and sketch them as arrows from the origin. Identify their coordinates explicitly.
    åœ¨ $\mathbb{R}^2$ ä¸­å†™å‡ºä¸‰ä¸ªä¸åŒçš„å‘é‡ï¼Œå¹¶å°†å®ƒä»¬ç”»æˆä»åŸç‚¹å‡ºå‘çš„ç®­å¤´ã€‚æ˜ç¡®æŒ‡å‡ºå®ƒä»¬çš„åæ ‡ã€‚
2.  Give an example of a vector in $\mathbb{R}^4$. Can you visualize it directly? Explain why high-dimensional visualization is challenging.
    ç»™å‡º $\mathbb{R}^4$ ä¸­ä¸€ä¸ªå‘é‡çš„ä¾‹å­ã€‚ä½ èƒ½ç›´æ¥å°†å…¶å¯è§†åŒ–å—ï¼Ÿè§£é‡Šä¸ºä»€ä¹ˆé«˜ç»´å¯è§†åŒ–å…·æœ‰æŒ‘æˆ˜æ€§ã€‚
3.  Let $\mathbf{v} = (4, -3, 2)$. Write $\mathbf{v}$ in column form and state $v_1, v_2, v_3$.
    ä»¤ $\mathbf{v} = (4, -3, 2)$ ã€‚å°† $\mathbf{v}$ å†™æˆåˆ—å½¢å¼ï¼Œå¹¶è¯´æ˜ ğ‘£ 1 , ğ‘£ 2 , ğ‘£ 3 v 1 â€‹ ï¼Œv 2 â€‹ ï¼Œv 3 â€‹ .
4.  In what sense is the set $\mathbb{R}^1$ both a line and a vector space? Illustrate with examples.
    åœ¨ä»€ä¹ˆæ„ä¹‰ä¸Šé›†åˆ $\mathbb{R}^1$ æ—¢æ˜¯çº¿ç©ºé—´åˆæ˜¯å‘é‡ç©ºé—´ï¼Ÿè¯·ä¸¾ä¾‹è¯´æ˜ã€‚
5.  Consider the vector $\mathbf{u} = (1,1,\dots,1) \in \mathbb{R}^n$. What is special about this vector when $n$ is large? What might it represent in applications?
    è€ƒè™‘å‘é‡ $\mathbf{u} = (1,1,\dots,1) \in \mathbb{R}^n$ ã€‚å½“ $n$ å¾ˆå¤§æ—¶ï¼Œè¿™ä¸ªå‘é‡æœ‰ä»€ä¹ˆç‰¹æ®Šä¹‹å¤„ï¼Ÿå®ƒåœ¨åº”ç”¨ä¸­å¯èƒ½ä»£è¡¨ä»€ä¹ˆï¼Ÿ

## 1.2 Vector Addition and Scalar Multiplication
1.2 å‘é‡åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•

Vectors in linear algebra are not static objects; their power comes from the operations we can perform on them. Two fundamental operations define the structure of vector spaces: addition and scalar multiplication. These operations satisfy simple but far-reaching rules that underpin the entire subject.
çº¿æ€§ä»£æ•°ä¸­çš„å‘é‡å¹¶éé™æ€å¯¹è±¡ï¼›å®ƒä»¬çš„åŠ›é‡æºäºæˆ‘ä»¬å¯ä»¥å¯¹å®ƒä»¬æ‰§è¡Œçš„è¿ç®—ã€‚ä¸¤ä¸ªåŸºæœ¬è¿ç®—å®šä¹‰äº†å‘é‡ç©ºé—´çš„ç»“æ„ï¼šåŠ æ³•å’Œæ ‡é‡ä¹˜æ³•ã€‚è¿™ä¸¤ä¸ªè¿ç®—æ»¡è¶³ä¸€äº›ç®€å•å´å½±å“æ·±è¿œçš„è§„åˆ™ï¼Œè¿™äº›è§„åˆ™æ„æˆäº†æ•´ä¸ªçº¿æ€§ä»£æ•°å­¦ç§‘çš„åŸºç¡€ã€‚

### Vector Addition
å‘é‡åŠ æ³•

Given two vectors of the same dimension, their sum is obtained by adding corresponding entries. Formally, if
ç»™å®šä¸¤ä¸ªç›¸åŒç»´åº¦çš„å‘é‡ï¼Œå®ƒä»¬çš„å’Œå¯ä»¥é€šè¿‡æ·»åŠ ç›¸åº”çš„å…ƒç´ æ¥è·å¾—ã€‚å½¢å¼ä¸Šï¼Œå¦‚æœ

$$
\mathbf{u} = (u_1, u_2, \dots, u_n), \quad\mathbf{v} = (v_1, v_2, \dots, v_n),
$$

then their sum is
é‚£ä¹ˆå®ƒä»¬çš„æ€»å’Œæ˜¯

$$
\mathbf{u} + \mathbf{v} = (u_1+v_1, u_2+v_2, \dots, u_n+v_n).
$$

Example 1.2.1. Let $\mathbf{u} = (2, -1, 3)$ and $\mathbf{v} = (4, 0, -5)$. Then
ä¾‹ 1.2.1ã€‚ è®¾ $\mathbf{u} = (2, -1, 3)$ å’Œ $\mathbf{v} = (4, 0, -5)$ ã€‚åˆ™

$$
\mathbf{u} + \mathbf{v} = (2+4, -1+0, 3+(-5)) = (6, -1, -2).
$$

Geometrically, vector addition corresponds to the *parallelogram rule*. If we draw both vectors as arrows from the origin, then placing the tail of one vector at the head of the other produces the sum. The diagonal of the parallelogram they form represents the resulting vector.
ä»å‡ ä½•å­¦ä¸Šè®²ï¼Œå‘é‡åŠ æ³•å¯¹åº”äº*å¹³è¡Œå››è¾¹å½¢æ³•åˆ™* ã€‚å¦‚æœæˆ‘ä»¬å°†ä¸¤ä¸ªå‘é‡éƒ½ç”»æˆä»åŸç‚¹å‡ºå‘çš„ç®­å¤´ï¼Œé‚£ä¹ˆå°†ä¸€ä¸ªå‘é‡çš„å°¾éƒ¨æ”¾åœ¨å¦ä¸€ä¸ªå‘é‡çš„å¤´éƒ¨ï¼Œå°±èƒ½å¾—åˆ°å‘é‡çš„å’Œã€‚å®ƒä»¬æ„æˆçš„å¹³è¡Œå››è¾¹å½¢çš„å¯¹è§’çº¿ä»£è¡¨æœ€ç»ˆçš„å‘é‡ã€‚

### Scalar Multiplication
æ ‡é‡ä¹˜æ³•

Multiplying a vector by a scalar stretches or shrinks the vector while preserving its direction, unless the scalar is negative, in which case the vector is also reversed. If $c \in \mathbb{R}$ and
å°†çŸ¢é‡ä¹˜ä»¥æ ‡é‡ä¼šæ‹‰ä¼¸æˆ–æ”¶ç¼©çŸ¢é‡ï¼ŒåŒæ—¶ä¿æŒå…¶æ–¹å‘ï¼Œé™¤éæ ‡é‡ è´Ÿæ•°ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹å‘é‡ä¹Ÿä¼šåè½¬ã€‚å¦‚æœ $c \in \mathbb{R}$ å’Œ

$$
\mathbf{v} = (v_1, v_2, \dots, v_n),
$$

then
ç„¶å

$$
c \mathbf{v} = (c v_1, c v_2, \dots, c v_n).
$$

Example 1.2.2. Let $\mathbf{v} = (3, -2)$ and $c = -2$. Then
ä¾‹ 1.2.2ã€‚ è®¾ $\mathbf{v} = (3, -2)$ å’Œ $c = -2$ ã€‚åˆ™

$$
c\mathbf{v} = -2(3, -2) = (-6, 4).
$$

This corresponds to flipping the vector through the origin and doubling its length.
è¿™ç›¸å½“äºé€šè¿‡åŸç‚¹ç¿»è½¬å‘é‡å¹¶ä½¿å…¶é•¿åº¦åŠ å€ã€‚

### Linear Combinations
çº¿æ€§ç»„åˆ

The interaction of addition and scalar multiplication allows us to form *linear combinations*. A linear combination of vectors $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k$ is any vector of the form
åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•çš„ç›¸äº’ä½œç”¨ä½¿æˆ‘ä»¬èƒ½å¤Ÿå½¢æˆ*çº¿æ€§ç»„åˆ* ã€‚å‘é‡ğ‘£çš„çº¿æ€§ç»„åˆ 1 , ğ‘£ 2 , â€¦ , ğ‘£ ğ‘˜ v 1 â€‹ ï¼Œv 2 â€‹ ï¼Œâ€¦ï¼Œv k â€‹ æ˜¯ä»»æ„å½¢å¼çš„å‘é‡

$$
c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_k \mathbf{v}_k, \quad c_i \in \mathbb{R}.
$$

Linear combinations are the mechanism by which we generate new vectors from existing ones. The span of a set of vectors-the collection of all their linear combinations-will later lead us to the idea of a subspace.
çº¿æ€§ç»„åˆæ˜¯ä¸€ç§ä»ç°æœ‰å‘é‡ç”Ÿæˆæ–°å‘é‡çš„æœºåˆ¶ã€‚ä¸€ç»„å‘é‡çš„è·¨åº¦â€”â€”å®ƒä»¬æ‰€æœ‰çº¿æ€§ç»„åˆçš„é›†åˆâ€”â€”ç¨åä¼šå¼•å‡ºå­ç©ºé—´çš„æ¦‚å¿µã€‚

Example 1.2.3. Let $\mathbf{v}_1 = (1,0)$ and $\mathbf{v}_2 = (0,1)$. Then any vector $(a,b)\in\mathbb{R}^2$ can be expressed as
ä¾‹ 1.2.3ã€‚ è®¾ $\mathbf{v}_1 = (1,0)$ å’Œ $\mathbf{v}_2 = (0,1)$ ã€‚åˆ™ä»»æ„å‘é‡ $(a,b)\in\mathbb{R}^2$ å¯ä»¥è¡¨ç¤ºä¸º

$$
a\mathbf{v}_1 + b\mathbf{v}_2.
$$

Thus $(1,0)$ and $(0,1)$ form the basic building blocks of the plane.
å› æ­¤ $(1,0)$ å’Œ $(0,1)$ æ„æˆäº†å¹³é¢çš„åŸºæœ¬æ„é€ å—ã€‚

### Notation
ç¬¦å·

*   Addition: $\mathbf{u} + \mathbf{v}$ means component-wise addition.
    åŠ æ³•ï¼š $\mathbf{u} + \mathbf{v}$ è¡¨ç¤ºé€ä¸ªç»„ä»¶çš„åŠ æ³•ã€‚
*   Scalar multiplication: $c\mathbf{v}$ scales each entry of $\mathbf{v}$ by $c$.
    æ ‡é‡ä¹˜æ³•ï¼š $c\mathbf{v}$ å°† $\mathbf{v}$ çš„æ¯ä¸ªæ¡ç›®ä¹˜ä»¥ $c$ ã€‚
*   Linear combination: a sum of the form $c_1 \mathbf{v}_1 + \cdots + c_k \mathbf{v}_k$.
    çº¿æ€§ç»„åˆï¼šğ‘ å½¢å¼çš„å’Œ 1 ğ‘£ 1 + â‹¯ + ğ‘ ğ‘˜ ğ‘£ ğ‘˜ c 1 â€‹ v 1 â€‹ +â‹¯+c k â€‹ v k â€‹ .

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Vector addition and scalar multiplication are the defining operations of linear algebra. They give structure to vector spaces, allow us to describe geometric phenomena like translation and scaling, and provide the foundation for solving systems of equations. Everything that follows-basis, dimension, transformations-builds on these simple but profound rules.
å‘é‡åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•æ˜¯çº¿æ€§ä»£æ•°çš„å®šä¹‰è¿ç®—ã€‚å®ƒä»¬èµ‹äºˆå‘é‡ç©ºé—´ç»“æ„ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæè¿°å¹³ç§»å’Œç¼©æ”¾ç­‰å‡ ä½•ç°è±¡ï¼Œå¹¶ä¸ºæ–¹ç¨‹ç»„çš„æ±‚è§£å¥ å®šåŸºç¡€ã€‚ä¹‹åçš„ä¸€åˆ‡â€”â€”åŸºã€ç»´åº¦ã€å˜æ¢â€”â€”éƒ½å»ºç«‹åœ¨è¿™äº›ç®€å•è€Œæ·±åˆ»çš„è§„åˆ™ä¹‹ä¸Šã€‚

### Exercises 1.2
ç»ƒä¹  1.2

1.  Compute $\mathbf{u} + \mathbf{v}$ where $\mathbf{u} = (1,2,3)$ and $\mathbf{v} = (4, -1, 0)$.
    è®¡ç®— $\mathbf{u} + \mathbf{v}$ ï¼Œå…¶ä¸­ $\mathbf{u} = (1,2,3)$ å’Œ $\mathbf{v} = (4, -1, 0)$ ã€‚
2.  Find $3\\mathbf{v}$where$\\mathbf{v} = (-2,5)$. Sketch both vectors to illustrate the scaling.
    æ±‚ $3\\mathbf{v} $where$ \\mathbf{v} = (-2,5)$ã€‚ç”»å‡ºä¸¤ä¸ªå‘é‡çš„ç¤ºæ„å›¾ï¼Œä»¥è¯´æ˜ç¼©æ”¾å…³ç³»ã€‚
3.  Show that $(5,7)$ can be written as a linear combination of $(1,0)$ and $(0,1)$.
    è¯æ˜ $(5,7)$ å¯ä»¥å†™æˆ $(1,0)$ å’Œ $(0,1)$ çš„çº¿æ€§ç»„åˆã€‚
4.  Write $(4,4)$ as a linear combination of $(1,1)$ and $(1,-1)$.
    å°† $(4,4)$ å†™ä¸º $(1,1)$ å’Œ $(1,-1)$ çš„çº¿æ€§ç»„åˆã€‚
5.  Prove that if $\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$, then $(c+d)(\mathbf{u}+\mathbf{v}) = c\mathbf{u} + c\mathbf{v} + d\mathbf{u} + d\mathbf{v}$ for scalars $c,d \in \mathbb{R}$.
    è¯æ˜å¦‚æœ $\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$ ï¼Œåˆ™å¯¹äºæ ‡é‡ $c,d \in \mathbb{R}$ æœ‰ $(c+d)(\mathbf{u}+\mathbf{v}) = c\mathbf{u} + c\mathbf{v} + d\mathbf{u} + d\mathbf{v}$ ã€‚

## 1.3 Dot Product, Norms, and Angles
1.3 ç‚¹ç§¯ã€èŒƒæ•°å’Œè§’

The dot product is the fundamental operation that links algebra and geometry in vector spaces. It allows us to measure lengths, compute angles, and determine orthogonality. From this single definition flow the notions of *norm* and *angle*, which give geometry to abstract vector spaces.
ç‚¹ç§¯æ˜¯å‘é‡ç©ºé—´ä¸­è¿æ¥ä»£æ•°å’Œå‡ ä½•çš„åŸºæœ¬è¿ç®—ã€‚å®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿæµ‹é‡é•¿åº¦ã€è®¡ç®—è§’åº¦å¹¶ç¡®å®šæ­£äº¤æ€§ã€‚ä»è¿™ä¸ªå•ä¸€å®šä¹‰ä¸­è¡ç”Ÿå‡º*èŒƒ*æ•°å’Œ *è§’åº¦* ï¼Œå®ƒä¸ºæŠ½è±¡å‘é‡ç©ºé—´æä¾›å‡ ä½•å½¢çŠ¶ã€‚

### The Dot Product
ç‚¹ç§¯

For two vectors in $\mathbb{R}^n$, the dot product (also called the inner product) is defined by
å¯¹äº $\mathbb{R}^n$ ä¸­çš„ä¸¤ä¸ªå‘é‡ï¼Œç‚¹ç§¯ï¼ˆä¹Ÿç§°ä¸ºå†…ç§¯ï¼‰å®šä¹‰ä¸º

$$
\mathbf{u} \cdot \mathbf{v} = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n.
$$

Equivalently, in matrix notation:
ç­‰æ•ˆåœ°ï¼Œç”¨çŸ©é˜µè¡¨ç¤ºæ³•è¡¨ç¤ºï¼š

$$
\mathbf{u} \cdot \mathbf{v} = \mathbf{u}^T \mathbf{v}.
$$

Example 1.3.1. Let $\mathbf{u} = (2, -1, 3)$ and $\mathbf{v} = (4, 0, -2)$. Then
ä¾‹ 1.3.1ã€‚ è®¾ $\mathbf{u} = (2, -1, 3)$ å’Œ $\mathbf{v} = (4, 0, -2)$ ã€‚åˆ™

$$
\mathbf{u} \cdot \mathbf{v} = 2\cdot 4 + (-1)\cdot 0 + 3\cdot (-2) = 8 - 6 = 2.
$$

The dot product outputs a single scalar, not another vector.
ç‚¹ç§¯è¾“å‡ºå•ä¸ªæ ‡é‡ï¼Œè€Œä¸æ˜¯å¦ä¸€ä¸ªå‘é‡ã€‚

### Norms (Length of a Vector)
èŒƒæ•°ï¼ˆå‘é‡çš„é•¿åº¦ï¼‰

The *Euclidean norm* of a vector is the square root of its dot product with itself:
å‘é‡çš„*æ¬§å‡ é‡Œå¾—èŒƒæ•°*æ˜¯å…¶ä¸è‡ªèº«çš„ç‚¹ç§¯çš„å¹³æ–¹æ ¹ï¼š

$$
\|\mathbf{v}\| = \sqrt{\mathbf{v} \cdot \mathbf{v}} = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}.
$$

This generalizes the Pythagorean theorem to arbitrary dimensions.
è¿™å°†å‹¾è‚¡å®šç†æ¨å¹¿åˆ°ä»»æ„ç»´åº¦ã€‚

Example 1.3.2. For $\mathbf{v} = (3, 4)$,
ä¾‹ 1.3.2ã€‚ å¯¹äº $\mathbf{v} = (3, 4)$ ï¼Œ

$$
\|\mathbf{v}\| = \sqrt{3^2 + 4^2} = \sqrt{25} = 5.
$$

This is exactly the length of the vector as an arrow in the plane.
è¿™æ­£æ˜¯å¹³é¢ä¸­ç®­å¤´æ‰€æŒ‡çš„çŸ¢é‡çš„é•¿åº¦ã€‚

### Angles Between Vectors
å‘é‡ä¹‹é—´çš„è§’åº¦

The dot product also encodes the angle between two vectors. For nonzero vectors $\mathbf{u}, \mathbf{v}$,
ç‚¹ç§¯ä¹Ÿç¼–ç äº†ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„è§’åº¦ã€‚å¯¹äºéé›¶å‘é‡ $\mathbf{u}, \mathbf{v}$ ï¼Œ

$$
\mathbf{u} \cdot \mathbf{v} = \|\mathbf{u}\| \, \|\mathbf{v}\| \cos \theta,
$$

where $\theta$ is the angle between them. Thus,
å…¶ä¸­ $\theta$ æ˜¯å®ƒä»¬ä¹‹é—´çš„è§’åº¦ã€‚å› æ­¤ï¼Œ

$$
\cos \theta = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\|\|\mathbf{v}\|}.
$$

Example 1.3.3. Let $\mathbf{u} = (1,0)$ and $\mathbf{v} = (0,1)$. Then
ä¾‹ 1.3.3ã€‚ è®¾ $\mathbf{u} = (1,0)$ å’Œ $\mathbf{v} = (0,1)$ ã€‚åˆ™

$$
\mathbf{u} \cdot \mathbf{v} = 0, \quad \|\mathbf{u}\| = 1, \quad \|\mathbf{v}\| = 1.
$$

Hence
å› æ­¤

$$
\cos \theta = \frac{0}{1\cdot 1} = 0 \quad \Rightarrow \quad \theta = \frac{\pi}{2}.
$$

The vectors are perpendicular.
è¿™äº›å‘é‡æ˜¯å‚ç›´çš„ã€‚

### Orthogonality
æ­£äº¤æ€§

Two vectors are said to be orthogonal if their dot product is zero:
å¦‚æœä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯ä¸ºé›¶ï¼Œåˆ™ç§°å®ƒä»¬æ­£äº¤ï¼š

$$
\mathbf{u} \cdot \mathbf{v} = 0.
$$

Orthogonality generalizes the idea of perpendicularity from geometry to higher dimensions.
æ­£äº¤æ€§å°†å‚ç›´æ€§çš„æ¦‚å¿µä»å‡ ä½•å­¦æ¨å¹¿åˆ°æ›´é«˜ç»´åº¦ã€‚

### Notation
ç¬¦å·

*   Dot product: $\mathbf{u} \cdot \mathbf{v}$.
    ç‚¹ç§¯ï¼š $\mathbf{u} \cdot \mathbf{v}$ ã€‚
*   Norm (length): $|\mathbf{v}|$.
    è§„èŒƒï¼ˆé•¿åº¦ï¼‰ï¼š $|\mathbf{v}|$ ã€‚
*   Orthogonality: $\mathbf{u} \perp \mathbf{v}$ if $\mathbf{u} \cdot \mathbf{v} = 0$.
    æ­£äº¤æ€§ï¼šå¦‚æœä¸º $\mathbf{u} \cdot \mathbf{v} = 0$ ï¼Œåˆ™ä¸º $\mathbf{u} \perp \mathbf{v}$ ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

The dot product turns vector spaces into geometric objects: vectors gain lengths, angles, and notions of perpendicularity. This foundation will later support the study of orthogonal projections, Gramâ€“Schmidt orthogonalization, eigenvectors, and least squares problems.
ç‚¹ç§¯å°†å‘é‡ç©ºé—´è½¬åŒ–ä¸ºå‡ ä½•å¯¹è±¡ï¼šå‘é‡è·å¾—é•¿åº¦ã€è§’åº¦å’Œå‚ç›´åº¦çš„æ¦‚å¿µã€‚è¿™ä¸€åŸºç¡€å°†ä¸ºåç»­çš„æ­£äº¤æŠ•å½±ã€æ ¼æ‹‰å§†-æ–½å¯†ç‰¹æ­£äº¤åŒ–ã€ç‰¹å¾å‘é‡å’Œæœ€å°äºŒä¹˜é—®é¢˜çš„ç ”ç©¶å¥ å®šåŸºç¡€ã€‚

### Exercises 1.3
ç»ƒä¹  1.3

1.  Compute $\mathbf{u} \cdot \mathbf{v}$ for $\mathbf{u} = (1,2,3)$, $\mathbf{v} = (4,5,6)$.
    è®¡ç®— $\mathbf{u} = (1,2,3)$ ã€ $\mathbf{v} = (4,5,6)$ çš„ $\mathbf{u} \cdot \mathbf{v}$ ã€‚
2.  Find the norm of $\mathbf{v} = (2, -2, 1)$.
    æ±‚å‡º $\mathbf{v} = (2, -2, 1)$ çš„èŒƒæ•°ã€‚
3.  Determine whether $\mathbf{u} = (1,1,0)$ and $\mathbf{v} = (1,-1,2)$ are orthogonal.
    ç¡®å®š $\mathbf{u} = (1,1,0)$ å’Œ $\mathbf{v} = (1,-1,2)$ æ˜¯å¦æ­£äº¤ã€‚
4.  Let $\mathbf{u} = (3,4)$, $\mathbf{v} = (4,3)$. Compute the angle between them.
    ä»¤ $\mathbf{u} = (3,4)$ , $\mathbf{v} = (4,3)$ ã€‚è®¡ç®—å®ƒä»¬ä¹‹é—´çš„è§’åº¦ã€‚
5.  Prove that $|\mathbf{u} + \mathbf{v}|^2 = |\mathbf{u}|^2 + |\mathbf{v}|^2 + 2\mathbf{u}\cdot \mathbf{v}$. This identity is the algebraic version of the Law of Cosines.
    è¯æ˜ $|\mathbf{u} + \mathbf{v}|^2 = |\mathbf{u}|^2 + |\mathbf{v}|^2 + 2\mathbf{u}\cdot \mathbf{v}$ ã€‚è¿™ä¸ªæ’ç­‰å¼æ˜¯ä½™å¼¦å®šç†çš„ä»£æ•°å½¢å¼ã€‚

## 1.4 Orthogonality
1.4 æ­£äº¤æ€§

Orthogonality captures the notion of perpendicularity in vector spaces. It is one of the most important geometric ideas in linear algebra, allowing us to decompose vectors, define projections, and construct special bases with elegant properties.
æ­£äº¤æ€§æ•æ‰äº†å‘é‡ç©ºé—´ä¸­å‚ç›´æ€§çš„æ¦‚å¿µã€‚å®ƒæ˜¯çº¿æ€§ä»£æ•°ä¸­æœ€é‡è¦çš„å‡ ä½•æ¦‚å¿µä¹‹ä¸€ï¼Œå®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ†è§£å‘é‡ã€å®šä¹‰æŠ•å½±ï¼Œå¹¶æ„é€ å…·æœ‰ä¼˜é›…æ€§è´¨çš„ç‰¹æ®ŠåŸºã€‚

### Definition
å®šä¹‰

Two vectors $\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$ are said to be orthogonal if their dot product is zero:
å¦‚æœä¸¤ä¸ªå‘é‡ $\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$ çš„ç‚¹ç§¯ä¸ºé›¶ï¼Œåˆ™ç§°å®ƒä»¬æ­£äº¤ï¼š

$$
\mathbf{u} \cdot \mathbf{v} = 0.
$$

This condition ensures that the angle between them is $\pi/2$ radians (90 degrees).
æ­¤æ¡ä»¶ç¡®ä¿å®ƒä»¬ä¹‹é—´çš„è§’åº¦ä¸º $\pi/2$ å¼§åº¦ï¼ˆ90 åº¦ï¼‰ã€‚

Example 1.4.1. In $\mathbb{R}^2$, the vectors $(1,2)$ and $(2,-1)$ are orthogonal since
ä¾‹ 1.4.1ã€‚ åœ¨ $\mathbb{R}^2$ ä¸­ï¼Œå‘é‡ $(1,2)$ å’Œ $(2,-1)$ æ˜¯æ­£äº¤çš„ï¼Œå› ä¸º

$$
(1,2) \cdot (2,-1) = 1\cdot 2 + 2\cdot (-1) = 0.
$$

### Orthogonal Sets
æ­£äº¤é›†

A collection of vectors is called orthogonal if every distinct pair of vectors in the set is orthogonal. If, in addition, each vector has norm 1, the set is called orthonormal.
å¦‚æœä¸€ç»„å‘é‡ä¸­æ¯å¯¹ä¸åŒçš„å‘é‡éƒ½æ˜¯æ­£äº¤çš„ï¼Œåˆ™ç§°è¯¥é›†åˆä¸ºæ­£äº¤å‘é‡ã€‚æ­¤å¤–ï¼Œå¦‚æœæ¯ä¸ªå‘é‡çš„èŒƒæ•°å‡ä¸º 1ï¼Œåˆ™è¯¥é›†åˆç§°ä¸ºæ ‡å‡†æ­£äº¤å‘é‡é›†ã€‚

Example 1.4.2. In $\mathbb{R}^3$, the standard basis vectors
ä¾‹ 1.4.2ã€‚ åœ¨ $\mathbb{R}^3$ ä¸­ï¼Œæ ‡å‡†åŸºå‘é‡

$$
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3 = (0,0,1)
$$

form an orthonormal set: each has length 1, and their dot products vanish when the indices differ.
å½¢æˆä¸€ä¸ªæ­£äº¤é›†ï¼šæ¯ä¸ªé›†çš„é•¿åº¦ä¸º 1ï¼Œå¹¶ä¸”å½“ç´¢å¼•ä¸åŒæ—¶ï¼Œå®ƒä»¬çš„ç‚¹ç§¯æ¶ˆå¤±ã€‚

### Projections
é¢„æµ‹

Orthogonality makes possible the decomposition of a vector into two components: one parallel to another vector, and one orthogonal to it. Given a nonzero vector $\mathbf{u}$ and any vector $\mathbf{v}$, the projection of $\mathbf{v}$ onto $\mathbf{u}$ is
æ­£äº¤æ€§ä½¿å¾—å°†ä¸€ä¸ªå‘é‡åˆ†è§£ä¸ºä¸¤ä¸ªåˆ†é‡æˆä¸ºå¯èƒ½ï¼šä¸€ä¸ªä¸å¦ä¸€ä¸ªå‘é‡å¹³è¡Œï¼Œå¦ä¸€ä¸ª ä¸å…¶æ­£äº¤ã€‚ç»™å®šä¸€ä¸ªéé›¶å‘é‡ $\mathbf{u}$ å’Œä»»æ„å‘é‡ $\mathbf{v}$ ï¼Œåˆ™ $\mathbf{v}$ çš„æŠ•å½± åˆ° $\mathbf{u}$ æ˜¯

$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{\mathbf{u} \cdot \mathbf{v}}{\mathbf{u} \cdot \mathbf{u}} \mathbf{u}.
$$

The difference
åŒºåˆ«

$$
\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})
$$

is orthogonal to $\mathbf{u}$. Thus every vector can be decomposed uniquely into a parallel and perpendicular part with respect to another vector.
ä¸ $\mathbf{u}$ æ­£äº¤ã€‚å› æ­¤ï¼Œæ¯ä¸ªå‘é‡éƒ½å¯ä»¥å”¯ä¸€åœ°åˆ†è§£ä¸ºç›¸å¯¹äºå¦ä¸€ä¸ªå‘é‡å¹³è¡Œå’Œå‚ç›´çš„éƒ¨åˆ†ã€‚

Example 1.4.3. Let $\mathbf{u} = (1,0)$, $\mathbf{v} = (2,3)$. Then
ä¾‹ 1.4.3ã€‚ ä»¤ $\mathbf{u} = (1,0)$ ï¼Œ $\mathbf{v} = (2,3)$ ã€‚ç„¶å

$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{(1,0)\cdot(2,3)}{(1,0)\cdot(1,0)} (1,0)= \frac{2}{1}(1,0) = (2,0).
$$

Thus
å› æ­¤

$$
\mathbf{v} = (2,3) = (2,0) + (0,3),
$$

where $(2,0)$ is parallel to $(1,0)$ and $(0,3)$ is orthogonal to it.
å…¶ä¸­ $(2,0)$ ä¸ $(1,0)$ å¹³è¡Œï¼Œ $(0,3)$ ä¸ $(1,0)$ æ­£äº¤ã€‚

### Orthogonal Decomposition
æ­£äº¤åˆ†è§£

In general, if $\mathbf{u} \neq \mathbf{0}$ and $\mathbf{v} \in \mathbb{R}^n$, then
ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœ $\mathbf{u} \neq \mathbf{0}$ å’Œ $\mathbf{v} \in \mathbb{R}^n$ ï¼Œé‚£ä¹ˆ

$$
\mathbf{v} = \text{proj}\_{\mathbf{u}}(\mathbf{v}) + \big(\mathbf{v} - \text{proj}\_{\mathbf{u}}(\mathbf{v})\big),
$$

where the first term is parallel to $\mathbf{u}$ and the second term is orthogonal. This decomposition underlies methods such as least squares approximation and the Gramâ€“Schmidt process.
å…¶ä¸­ç¬¬ä¸€é¡¹å¹³è¡Œäº $\mathbf{u}$ ï¼Œç¬¬äºŒé¡¹æ­£äº¤ã€‚è¿™ç§åˆ†è§£æ˜¯æœ€å°äºŒä¹˜è¿‘ä¼¼å’Œæ ¼æ‹‰å§†-æ–½å¯†ç‰¹è¿‡ç¨‹ç­‰æ–¹æ³•çš„åŸºç¡€ã€‚

### Notation
ç¬¦å·

*   $\mathbf{u} \perp \mathbf{v}$: vectors $\mathbf{u}$ and $\mathbf{v}$ are orthogonal.
    $\mathbf{u} \perp \mathbf{v}$ ï¼šå‘é‡ $\mathbf{u}$ å’Œ $\mathbf{v}$ æ­£äº¤ã€‚
*   An orthogonal set: vectors pairwise orthogonal.
    æ­£äº¤é›†ï¼šå‘é‡ä¸¤ä¸¤æ­£äº¤ã€‚
*   An orthonormal set: pairwise orthogonal, each of norm 1.
    æ­£äº¤é›†ï¼šä¸¤ä¸¤æ­£äº¤ï¼Œæ¯ç»„èŒƒæ•°ä¸º 1ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Orthogonality gives structure to vector spaces. It provides a way to separate independent directions cleanly, simplify computations, and minimize errors in approximations. Many powerful algorithms in numerical linear algebra and data science (QR decomposition, least squares regression, PCA) rely on orthogonality.
æ­£äº¤æ€§èµ‹äºˆå‘é‡ç©ºé—´ç»“æ„ã€‚å®ƒæä¾›äº†ä¸€ç§æ¸…æ™°åœ°åˆ†ç¦»ç‹¬ç«‹æ–¹å‘ã€ç®€åŒ–è®¡ç®—å¹¶æœ€å°åŒ–è¿‘ä¼¼è¯¯å·®çš„æ–¹æ³•ã€‚æ•°å€¼çº¿æ€§ä»£æ•°å’Œæ•°æ®ç§‘å­¦ä¸­è®¸å¤šå¼ºå¤§çš„ç®—æ³•ï¼ˆä¾‹å¦‚ QR åˆ†è§£ã€æœ€å°äºŒä¹˜å›å½’ã€ä¸»æˆåˆ†åˆ†æï¼‰éƒ½ä¾èµ–äºæ­£äº¤æ€§ã€‚

### Exercises 1.4
ç»ƒä¹  1.4

1.  Verify that the vectors $(1,2,2)$ and $(2,0,-1)$ are orthogonal.
    éªŒè¯å‘é‡ $(1,2,2)$ å’Œ $(2,0,-1)$ æ˜¯å¦æ­£äº¤ã€‚
2.  Find the projection of $(3,4)$ onto $(1,1)$.
    æ‰¾åˆ° $(3,4)$ åˆ° $(1,1)$ çš„æŠ•å½±ã€‚
3.  Show that any two distinct standard basis vectors in $\mathbb{R}^n$ are orthogonal.
    è¯æ˜ $\mathbb{R}^n$ ä¸­çš„ä»»æ„ä¸¤ä¸ªä¸åŒçš„æ ‡å‡†åŸºå‘é‡éƒ½æ˜¯æ­£äº¤çš„ã€‚
4.  Decompose $(5,2)$ into components parallel and orthogonal to $(2,1)$.
    å°† $(5,2)$ åˆ†è§£ä¸ºä¸ $(2,1)$ å¹³è¡Œä¸”æ­£äº¤çš„åˆ†é‡ã€‚
5.  Let $\mathbf{u}, \mathbf{v}$ be orthogonal nonzero vectors. (a) Show that $(\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v})=\lVert \mathbf{u}\rVert^2-\lVert \mathbf{v}\rVert^2.$ (b) For what condition on $\mathbf{u}$ and $\mathbf{v}$ does $(\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v})=0$?
    ä»¤ $\mathbf{u}, \mathbf{v}$ ä¸ºæ­£äº¤éé›¶å‘é‡ã€‚ï¼ˆaï¼‰è¯æ˜ $(\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v})=\lVert \mathbf{u}\rVert^2-\lVert \mathbf{v}\rVert^2.$ ï¼ˆbï¼‰ $(\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v})=0$ å¯¹ $\mathbf{u}$ å’Œ $\mathbf{v}$ æ»¡è¶³ä»€ä¹ˆæ¡ä»¶ï¼Ÿ

# Chapter 2. Matrices
ç¬¬ 2 ç« çŸ©é˜µ

## 2.1 Definition and Notation
2.1 å®šä¹‰å’Œç¬¦å·

Matrices are the central objects of linear algebra, providing a compact way to represent and manipulate linear transformations, systems of equations, and structured data. A matrix is a rectangular array of numbers arranged in rows and columns.
çŸ©é˜µæ˜¯çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒå¯¹è±¡ï¼Œå®ƒæä¾›äº†ä¸€ç§ç®€æ´çš„æ–¹å¼æ¥è¡¨ç¤ºå’Œæ“ä½œçº¿æ€§å˜æ¢ã€æ–¹ç¨‹ç»„å’Œç»“æ„åŒ–æ•°æ®ã€‚çŸ©é˜µæ˜¯ç”±æŒ‰è¡Œå’Œåˆ—æ’åˆ—çš„æ•°å­—ç»„æˆçš„çŸ©å½¢é˜µåˆ—ã€‚

### Formal Definition
æ­£å¼å®šä¹‰

An $m \times n$ matrix is an array with $m$ rows and $n$ columns, written
$m \times n$ çŸ©é˜µæ˜¯å…·æœ‰ $m$ è¡Œå’Œ $n$ åˆ—çš„æ•°ç»„ï¼Œå†™ä¸º

$$
A =\begin{bmatrix}a_{11} & a_{12} & \cdots & a_{1n} \\a_{21} & a_{22} & \cdots & a_{2n} \\\vdots & \vdots & \ddots & \vdots \\a_{m1} & a_{m2} & \cdots & a_{mn}\end{bmatrix}.
$$

Each entry $a_{ij}$ is a scalar, located in the *i*\-th row and *j*\-th column. The size (or dimension) of the matrix is denoted by $m \times n$.
æ¯ä¸ªæ¡ç›®ğ‘ ğ‘– ğ‘— a ä¼Šå¥‡ â€‹ æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œä½äºç¬¬ - è¡Œå’Œç¬¬ - åˆ—ã€‚çŸ©é˜µçš„å¤§å°ï¼ˆæˆ–ç»´åº¦ï¼‰ç”¨ $m \times n$ è¡¨ç¤ºã€‚

*   If $m = n$, the matrix is square.
    å¦‚æœä¸º $m = n$ ï¼Œåˆ™çŸ©é˜µä¸ºæ–¹é˜µã€‚
*   If $m = 1$, the matrix is a row vector.
    å¦‚æœä¸º $m = 1$ ï¼Œåˆ™è¯¥çŸ©é˜µä¸ºè¡Œå‘é‡ã€‚
*   If $n = 1$, the matrix is a column vector.
    å¦‚æœä¸º $n = 1$ ï¼Œåˆ™çŸ©é˜µä¸ºåˆ—å‘é‡ã€‚

Thus, vectors are simply special cases of matrices.
å› æ­¤ï¼Œå‘é‡åªæ˜¯çŸ©é˜µçš„ç‰¹æ®Šæƒ…å†µã€‚

### Examples
ç¤ºä¾‹

Example 2.1.1. A 2Ã—3 matrix:
ä¾‹ 2.1.1. 2Ã—3 çŸ©é˜µï¼š

$$
A = \begin{bmatrix}1 & -2 & 4 \\0 & 3 & 5\end{bmatrix}.
$$

Here, $a_{12} = -2$, $a_{23} = 5$, and the matrix has 2 rows, 3 columns.
è¿™é‡Œï¼Œ $a_{12} = -2$ ï¼Œ $a_{23} = 5$ ï¼ŒçŸ©é˜µæœ‰ 2 è¡Œï¼Œ3 åˆ—ã€‚

Example 2.1.2. A 3Ã—3 square matrix:
ä¾‹ 2.1.2. 3Ã—3 æ–¹é˜µï¼š

$$
B = \begin{bmatrix}2 & 0 & 1 \\-1 & 3 & 4 \\0 & 5 & -2\end{bmatrix}.
$$

This will later serve as the representation of a linear transformation on $\mathbb{R}^3$.
è¿™ç¨åå°†ä½œä¸º $\mathbb{R}^3$ çš„çº¿æ€§å˜æ¢çš„è¡¨ç¤ºã€‚

### Indexing and Notation
ç´¢å¼•å’Œç¬¦å·

*   Matrices are denoted by uppercase bold letters: $A, B, C$.
    çŸ©é˜µç”¨å¤§å†™ç²—ä½“å­—æ¯è¡¨ç¤ºï¼š $A, B, C$ ã€‚
*   Entries are written as $a_{ij}$, with the row index first, column index second.
    æ¡ç›®å†™ä¸ºğ‘ ğ‘– ğ‘— a ä¼Šå¥‡ â€‹ ï¼Œå…¶ä¸­è¡Œç´¢å¼•åœ¨å‰ï¼Œåˆ—ç´¢å¼•åœ¨åã€‚
*   The set of all real $m \times n$ matrices is denoted $\mathbb{R}^{m \times n}$.
    æ‰€æœ‰å®æ•° $m \times n$ çŸ©é˜µçš„é›†åˆè¡¨ç¤ºä¸º $\mathbb{R}^{m \times n}$ ã€‚

Thus, a matrix is a function $A: {1,\dots,m} \times {1,\dots,n} \to \mathbb{R}$, assigning a scalar to each row-column position.
å› æ­¤ï¼ŒçŸ©é˜µæ˜¯ä¸€ä¸ªå‡½æ•° $A: {1,\dots,m} \times {1,\dots,n} \to \mathbb{R}$ ï¼Œä¸ºæ¯ä¸ªè¡Œåˆ—ä½ç½®åˆ†é…ä¸€ä¸ªæ ‡é‡ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Matrices generalize vectors and give us a language for describing linear operations systematically. They encode systems of equations, rotations, projections, and transformations of data. With matrices, algebra and geometry come together: a single compact object can represent both numerical data and functional rules.
çŸ©é˜µæ¨å¹¿äº†å‘é‡ï¼Œå¹¶ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§ç³»ç»Ÿåœ°æè¿°çº¿æ€§è¿ç®—çš„è¯­è¨€ã€‚å®ƒä»¬å¯¹æ–¹ç¨‹ç»„ã€æ—‹è½¬ã€æŠ•å½±å’Œæ•°æ®å˜æ¢è¿›è¡Œç¼–ç ã€‚çŸ©é˜µå°†ä»£æ•°å’Œå‡ ä½•ç»“åˆåœ¨ä¸€èµ·ï¼šä¸€ä¸ªç´§å‡‘çš„å¯¹è±¡æ—¢å¯ä»¥è¡¨ç¤ºæ•°å€¼æ•°æ®ï¼Œåˆå¯ä»¥è¡¨ç¤ºå‡½æ•°è§„åˆ™ã€‚

### Exercises 2.1
ç»ƒä¹  2.1

1.  Write a $3 \\times 2$matrix of your choice and identify its entries$a\_{ij}$.
    å†™å‡º $3 \\times 2 $matrix of your choice and identify its entries$ a\_{ij}$ã€‚
2.  Is every vector a matrix? Is every matrix a vector? Explain.
    æ¯ä¸ªå‘é‡éƒ½æ˜¯çŸ©é˜µå—ï¼Ÿæ¯ä¸ªçŸ©é˜µéƒ½æ˜¯å‘é‡å—ï¼Ÿè¯·è§£é‡Šã€‚
3.  Which of the following are square matrices: $A \in \mathbb{R}^{4\times4}$, $B \in \mathbb{R}^{3\times5}$, $C \in \mathbb{R}^{1\times1}$?
    ä¸‹åˆ—å“ªäº›æ˜¯æ­£æ–¹å½¢ çŸ©é˜µï¼š $A \in \mathbb{R}^{4\times4}$ ï¼Œ $B \in \mathbb{R}^{3\times5}$ ï¼Œ $C \in \mathbb{R}^{1\times1}$ ï¼Ÿ
4.  Let
    è®©

$$
D = \begin{bmatrix} 1 & 0 \\\\ 0 & 1 \end{bmatrix}
$$

What kind of matrix is this? 5. Consider the matrix
è¿™æ˜¯ä»€ä¹ˆç±»å‹çš„çŸ©é˜µï¼Ÿ5. è€ƒè™‘çŸ©é˜µ

$$
E = \begin{bmatrix} a & b \\ c & d \end{bmatrix}
$$

Express $e_{11}, e_{12}, e_{21}, e_{22}$ explicitly.
å¿«é€’ğ‘’ 11 , ğ‘’ 12 , ğ‘’ 21 , ğ‘’ 22 e 11 â€‹ ï¼Œe 12 â€‹ ï¼Œe 21 â€‹ ï¼Œe 22 â€‹ æ˜ç¡®åœ°ã€‚

## 2.2 Matrix Addition and Multiplication
2.2 çŸ©é˜µåŠ æ³•å’Œä¹˜æ³•

Once matrices are defined, the next step is to understand how they combine. Just as vectors gain meaning through addition and scalar multiplication, matrices become powerful through two operations: addition and multiplication.
å®šä¹‰å¥½çŸ©é˜µåï¼Œä¸‹ä¸€æ­¥å°±æ˜¯ç†è§£å®ƒä»¬æ˜¯å¦‚ä½•ç»„åˆçš„ã€‚æ­£å¦‚å‘é‡é€šè¿‡åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•è·å¾—æ„ä¹‰ä¸€æ ·ï¼ŒçŸ©é˜µä¹Ÿé€šè¿‡ä¸¤ç§è¿ç®—å˜å¾—å¼ºå¤§ï¼šåŠ æ³•å’Œä¹˜æ³•ã€‚

### Matrix Addition
çŸ©é˜µåŠ æ³•

Two matrices of the same size are added by adding corresponding entries. If
ä¸¤ä¸ªå¤§å°ç›¸åŒçš„çŸ©é˜µå¯ä»¥é€šè¿‡æ·»åŠ ç›¸åº”çš„å…ƒç´ æ¥ç›¸åŠ ã€‚å¦‚æœ

$$
A = [a_{ij}] \in \mathbb{R}^{m \times n}, \quadB = [b_{ij}] \in \mathbb{R}^{m \times n},
$$

then
ç„¶å

$$
A + B = [a_{ij} + b_{ij}] \in \mathbb{R}^{m \times n}.
$$

Example 2.2.1. Let
ä¾‹ 2.2.1. è®¾

$$
A = \begin{bmatrix}1 & 2 \\3 & 4\end{bmatrix}, \quadB = \begin{bmatrix}-1 & 0 \\5 & 2\end{bmatrix}.
$$

Then
ç„¶å

$$
A + B = \begin{bmatrix}1 + (-1) & 2 + 0 \\3 + 5 & 4 + 2\end{bmatrix} =\begin{bmatrix}0 & 2 \\8 & 6\end{bmatrix}.
$$

Matrix addition is commutative ($A+B = B+A$) and associative ($(A+B)+C = A+(B+C)$). The zero matrix, with all entries 0, acts as the additive identity.
çŸ©é˜µåŠ æ³•æ»¡è¶³äº¤æ¢å¾‹ ( $A+B = B+A$ ) å’Œç»“åˆå¾‹ ( $(A+B)+C = A+(B+C)$ )ã€‚é›¶çŸ©é˜µï¼ˆæ‰€æœ‰å…ƒç´ å‡ä¸º 0ï¼‰å……å½“åŠ æ³•æ’ç­‰å¼ã€‚

### Scalar Multiplication
æ ‡é‡ä¹˜æ³•

For a scalar $c \in \mathbb{R}$ and a matrix $A = [[a_{ij}]$, we define
å¯¹äºæ ‡é‡ $c \in \mathbb{R}$ å’ŒçŸ©é˜µ $A = [[a_{ij}]$ ï¼Œæˆ‘ä»¬å®šä¹‰

$$
cA = [c \cdot a_{ij}].
$$

This stretches or shrinks all entries of the matrix uniformly.
è¿™ä¼šå‡åŒ€åœ°æ‹‰ä¼¸æˆ–æ”¶ç¼©çŸ©é˜µçš„æ‰€æœ‰æ¡ç›®ã€‚

Example 2.2.2. If
ä¾‹ 2.2.2. å¦‚æœ

$$
A = \begin{bmatrix}2 & -1 \\0 & 3\end{bmatrix}, \quad c = -2,
$$

then
ç„¶å

$$
cA = \begin{bmatrix}-4 & 2 \\0 & -6\end{bmatrix}.
$$

### Matrix Multiplication
çŸ©é˜µä¹˜æ³•

The defining operation of matrices is multiplication. If
çŸ©é˜µçš„å®šä¹‰è¿ç®—æ˜¯ä¹˜æ³•ã€‚å¦‚æœ

$$
A \in \mathbb{R}^{m \times n}, \quad B \in \mathbb{R}^{n \times p},
$$

then their product is the $m \times p$ matrix
é‚£ä¹ˆå®ƒä»¬çš„ä¹˜ç§¯å°±æ˜¯ $m \times p$ çŸ©é˜µ

$$
AB = C = [c_{ij}], \quad c_{ij} = \sum_{k=1}^n a_{ik} b_{kj}.
$$

Thus, the entry in the $i$\-th row and $j$\-th column of $AB$ is the dot product of the $i$\-th row of $A$ with the $j$\-th column of $B$.
å› æ­¤ï¼Œ $AB$ ç¬¬ $i$ è¡Œã€ç¬¬ $j$ åˆ—çš„æ¡ç›®æ˜¯ $A$ ç¬¬ $i$ è¡Œä¸ $B$ ç¬¬ $j$ åˆ—çš„ç‚¹ç§¯ã€‚

Example 2.2.3. Let
ä¾‹ 2.2.3. è®¾

$$
A = \begin{bmatrix}1 & 2 \\0 & 3\end{bmatrix}, \quadB = \begin{bmatrix}4 & -1 \\2 & 5\end{bmatrix}.
$$

Then
ç„¶å

$$
AB = \begin{bmatrix}1\cdot4 + 2\cdot2 & 1\cdot(-1) + 2\cdot5 \\0\cdot4 + 3\cdot2 & 0\cdot(-1) + 3\cdot5\end{bmatrix} =\begin{bmatrix}8 & 9 \\6 & 15\end{bmatrix}.
$$

Notice that matrix multiplication is not commutative in general: $AB \neq BA$. Sometimes $BA$ may not even be defined if dimensions do not align.
è¯·æ³¨æ„ï¼ŒçŸ©é˜µä¹˜æ³•é€šå¸¸ä¸æ»¡è¶³äº¤æ¢å¾‹ï¼š $AB \neq BA$ ã€‚å¦‚æœç»´åº¦ä¸ä¸€è‡´ï¼Œæœ‰æ—¶ç”šè‡³å¯èƒ½æ— æ³•å®šä¹‰ $BA$ ã€‚

### Geometric Meaning
å‡ ä½•æ„ä¹‰

Matrix multiplication corresponds to the composition of linear transformations. If $A$ transforms vectors in $\mathbb{R}^n$ and $B$ transforms vectors in $\mathbb{R}^p$, then $AB$ represents applying $B$ first, then $A$. This makes matrices the algebraic language of transformations.
çŸ©é˜µä¹˜æ³•å¯¹åº”äºçº¿æ€§å˜æ¢çš„å¤åˆã€‚å¦‚æœ $A$ å˜æ¢ $\mathbb{R}^n$ ä¸­çš„å‘é‡ï¼Œ $B$ å˜æ¢ $\mathbb{R}^p$ ä¸­çš„å‘é‡ï¼Œé‚£ä¹ˆ $AB$ è¡¨ç¤ºå…ˆåº”ç”¨ $B$ ï¼Œç„¶åå†åº”ç”¨ $A$ ã€‚è¿™ä½¿å¾—çŸ©é˜µæˆä¸ºå˜æ¢çš„ä»£æ•°è¯­è¨€ã€‚

### Notation
ç¬¦å·

*   Matrix sum: $A+B$.
    çŸ©é˜µå’Œï¼š $A+B$ ã€‚
*   Scalar multiple: $cA$.
    æ ‡é‡å€æ•°ï¼š $cA$ ã€‚
*   Product: $AB$, defined only when the number of columns of $A$ equals the number of rows of $B$.
    ä¹˜ç§¯ï¼š $AB$ ï¼Œä»…å½“ $A$ çš„åˆ—æ•°ç­‰äº $B$ çš„è¡Œæ•°æ—¶æ‰å®šä¹‰ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Matrix multiplication is the core mechanism of linear algebra: it encodes how transformations combine, how systems of equations are solved, and how data flows in modern algorithms. Addition and scalar multiplication make matrices into a vector space, while multiplication gives them an algebraic structure rich enough to model geometry, computation, and networks.
çŸ©é˜µä¹˜æ³•æ˜¯çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒæœºåˆ¶ï¼šå®ƒç¼–ç äº†å˜æ¢çš„ç»„åˆæ–¹å¼ã€æ–¹ç¨‹ç»„çš„æ±‚è§£æ–¹å¼ä»¥åŠç°ä»£ç®—æ³•ä¸­æ•°æ®æµåŠ¨çš„æ–¹å¼ã€‚åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•å°†çŸ©é˜µè½¬åŒ–ä¸ºå‘é‡ç©ºé—´ï¼Œè€Œä¹˜æ³•åˆ™èµ‹äºˆçŸ©é˜µä¸°å¯Œçš„ä»£æ•°ç»“æ„ï¼Œä½¿å…¶èƒ½å¤Ÿå¯¹å‡ ä½•ã€è®¡ç®—å’Œç½‘ç»œè¿›è¡Œå»ºæ¨¡ã€‚

### Exercises 2.2
ç»ƒä¹  2.2

1.  Compute $A+B$ for
    è®¡ç®— $A+B$

$$
A = \begin{bmatrix} 2 & 3 \\-1 & 0 \end{bmatrix}, \quadB = \begin{bmatrix} 4 & -2 \\5 & 7 \end{bmatrix}.
$$

2.  Find 3A where
    æŸ¥æ‰¾ 3A

$$
A = \begin{bmatrix} 1 & -4 \\2 & 6 \end{bmatrix}.
$$

3.  Multiply
    ä¹˜

$$
A = \begin{bmatrix} 1 & 0 & 2 \\-1 & 3 & 1 \end{bmatrix}, \quadB = \begin{bmatrix} 2 & 1 \\0 & -1 \\3 & 4 \end{bmatrix}.
$$

4.  Verify with an explicit example that $AB \neq BA$.
    é€šè¿‡æ˜ç¡®çš„ä¾‹å­æ¥éªŒè¯ $AB \neq BA$ ã€‚
5.  Prove that matrix multiplication is distributive: $A(B+C) = AB + AC$.
    è¯æ˜çŸ©é˜µä¹˜æ³•æ˜¯åˆ†é…çš„ï¼š $A(B+C) = AB + AC$ ã€‚

## 2.3 Transpose and Inverse
2.3 è½¬ç½®å’Œé€†

Two special operations on matrices-the transpose and the inverse-give rise to deep algebraic and geometric properties. The transpose rearranges a matrix by flipping it across its main diagonal, while the inverse, when it exists, acts as the undo operation for matrix multiplication.
çŸ©é˜µçš„ä¸¤ç§ç‰¹æ®Šè¿ç®—â€”â€”è½¬ç½®å’Œé€†â€”â€”å¼•å‡ºäº†æ·±åˆ»çš„ä»£æ•°å’Œå‡ ä½•æ€§è´¨ã€‚è½¬ç½®é€šè¿‡æ²¿çŸ©é˜µä¸»å¯¹è§’çº¿ç¿»è½¬æ¥é‡æ–°æ’åˆ—çŸ©é˜µï¼Œè€Œé€†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰åˆ™å……å½“çŸ©é˜µä¹˜æ³•çš„æ’¤æ¶ˆæ“ä½œã€‚

### The Transpose
è½¬ç½®

The transpose of an $m \times n$ matrix $A = [a_{ij}]$ is the $n \times m$ matrix $A^T = [a_{ji}]$, obtained by swapping rows and columns.
$m \times n$ çŸ©é˜µ $A = [a_{ij}]$ çš„è½¬ç½®æ˜¯é€šè¿‡äº¤æ¢è¡Œå’Œåˆ—è·å¾—çš„ $n \times m$ çŸ©é˜µ $A^T = [a_{ji}]$ ã€‚

Formally,
æ­£å¼åœ°ï¼Œ

$$
(A^T)\_{ij} = a\_{ji}.
$$

Example 2.3.1. If
ä¾‹ 2.3.1. å¦‚æœ

$$
A = \begin{bmatrix}1 & 4 & -2 \\0 & 3 & 5\end{bmatrix},
$$

then
ç„¶å

$$
A^T = \begin{bmatrix}1 & 0 \\4 & 3 \\-2 & 5\end{bmatrix}.
$$

Properties of the Transpose.
è½¬ç½®çš„å±æ€§ã€‚

1.  $(A^T)^T = A$.
2.  $(A+B)^T = A^T + B^T$.
3.  $(cA)^T = cA^T$, for scalar $c$.
    $(cA)^T = cA^T$ ï¼Œå¯¹äºæ ‡é‡ $c$ ã€‚
4.  $(AB)^T = B^T A^T$.

The last rule is crucial: the order reverses.
æœ€åä¸€æ¡è§„åˆ™è‡³å…³é‡è¦ï¼šé¡ºåºåè½¬ã€‚

### The Inverse
é€†å‘

A square matrix $A \in \mathbb{R}^{n \times n}$ is said to be invertible (or nonsingular) if there exists another matrix $A^{-1}$ such that
å¦‚æœå­˜åœ¨å¦ä¸€ä¸ªçŸ©é˜µ $A^{-1}$ æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼Œåˆ™ç§°æ–¹é˜µ $A \in \mathbb{R}^{n \times n}$ å¯é€†ï¼ˆæˆ–éå¥‡å¼‚ï¼‰

$$
AA^{-1} = A^{-1}A = I_n,
$$

where $I_n$ is the $n \times n$ identity matrix. In this case, $A^{-1}$ is called the inverse of $A$.
å…¶ä¸­ğ¼ ğ‘› I n â€‹ æ˜¯ $n \times n$ å•ä½çŸ©é˜µã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ $A^{-1}$ è¢«ç§°ä¸º $A$ çš„é€†ã€‚

Not every matrix is invertible. A necessary condition is that $\det(A) \neq 0$, a fact that will be developed in Chapter 6.
å¹¶éæ‰€æœ‰çŸ©é˜µéƒ½æ˜¯å¯é€†çš„ã€‚å¿…è¦æ¡ä»¶æ˜¯ $\det(A) \neq 0$ ï¼Œæˆ‘ä»¬å°†åœ¨ç¬¬ 6 ç« ä¸­è¿›ä¸€æ­¥é˜è¿°ã€‚

Example 2.3.2. Let
ä¾‹ 2.3.2. è®¾

$$
A = \begin{bmatrix}1 & 2 \\3 & 4\end{bmatrix}.
$$

Its determinant is $\det(A) = (1)(4) - (2)(3) = -2 \neq 0$. The inverse is
å®ƒçš„è¡Œåˆ—å¼æ˜¯ $\det(A) = (1)(4) - (2)(3) = -2 \neq 0$ ã€‚é€†æ˜¯

$$
A^{-1} = \frac{1}{\det(A)} \begin{bmatrix}4 & -2 \\-3 & 1\end{bmatrix} =\begin{bmatrix}-2 & 1 \\1.5 & -0.5\end{bmatrix}.
$$

Verification:
ç¡®è®¤ï¼š

$$
AA^{-1} = \begin{bmatrix}1 & 2 \\3 & 4\end{bmatrix}\begin{bmatrix}-2 & 1 \\1.5 & -0.5\end{bmatrix} =\begin{bmatrix}1 & 0 \\0 & 1\end{bmatrix}.
$$

### Geometric Meaning
å‡ ä½•æ„ä¹‰

*   The transpose corresponds to reflecting a linear transformation across the diagonal. For vectors, it switches between row and column forms.
    è½¬ç½®å¯¹åº”äºæ²¿å¯¹è§’çº¿åæ˜ çº¿æ€§å˜æ¢ã€‚å¯¹äºå‘é‡ï¼Œå®ƒåœ¨è¡Œå’Œåˆ—å½¢å¼ä¹‹é—´åˆ‡æ¢ã€‚
*   The inverse, when it exists, corresponds to reversing a linear transformation. For example, if $A$ scales and rotates vectors, $A^{-1}$ rescales and rotates them back.
    å¦‚æœå­˜åœ¨é€†å˜æ¢ï¼Œåˆ™å®ƒå¯¹åº”äºçº¿æ€§å˜æ¢çš„é€†å˜æ¢ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ $A$ ç¼©æ”¾å¹¶æ—‹è½¬äº†çŸ¢é‡ï¼Œåˆ™ $A^{-1}$ ä¼šå°†å…¶é‡æ–°ç¼©æ”¾å¹¶æ—‹è½¬å›å»ã€‚

### Notation
ç¬¦å·

*   Transpose: $A^T$.
    è½¬ç½®ï¼š $A^T$ ã€‚
*   Inverse: $A^{-1}$, defined only for invertible square matrices.
    é€†ï¼š $A^{-1}$ ï¼Œä»…ä¸ºå¯é€†æ–¹é˜µå®šä¹‰ã€‚
*   Identity: $I_n$, acts as the multiplicative identity.
    èº«ä»½ï¼šğ¼ ğ‘› I n â€‹ ï¼Œå……å½“ä¹˜æ³•æ’ç­‰å¼ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

The transpose allows us to define symmetric and orthogonal matrices, central to geometry and numerical methods. The inverse underlies the solution of linear systems, encoding the idea of undoing a transformation. Together, these operations set the stage for determinants, eigenvalues, and orthogonalization.
è½¬ç½®ä½¿æˆ‘ä»¬èƒ½å¤Ÿå®šä¹‰å¯¹ç§°çŸ©é˜µå’Œæ­£äº¤çŸ©é˜µï¼Œè¿™æ˜¯å‡ ä½•å’Œæ•°å€¼æ–¹æ³•çš„æ ¸å¿ƒã€‚é€†çŸ©é˜µæ˜¯çº¿æ€§ç³»ç»Ÿè§£çš„åŸºç¡€ï¼Œå®ƒè•´å«ç€æ’¤é”€å˜æ¢çš„æ€æƒ³ã€‚è¿™äº›è¿ç®—å…±åŒä¸ºè¡Œåˆ—å¼ã€ç‰¹å¾å€¼å’Œæ­£äº¤åŒ–å¥ å®šäº†åŸºç¡€ã€‚

### Exercises 2.3
ç»ƒä¹  2.3

1.  Compute the transpose of
    è®¡ç®—è½¬ç½®

$$
A = \begin{bmatrix} 2 & -1 & 3 \\ 0 & 4 & 5 \end{bmatrix}.
$$

2.  Verify that $(AB)^T = B^T A^T$ for
    éªŒè¯ $(AB)^T = B^T A^T$

$$
A = \begin{bmatrix}1 & 2 \\0 & 1 \end{bmatrix}, \quadB = \begin{bmatrix}3 & 4 \\5 & 6 \end{bmatrix}.
$$

3.  Determine whether
    ç¡®å®šæ˜¯å¦

$$
C = \begin{bmatrix}2 & 1 \\4 & 2 \end{bmatrix}
$$

is invertible. If so, find $C^{-1}$.
å¯é€†ã€‚å¦‚æœå¯é€†ï¼Œåˆ™æ±‚ $C^{-1}$ ã€‚

4.  Find the inverse of
    æ±‚é€†

$$
D = \begin{bmatrix}0 & 1 \\-1 & 0 \end{bmatrix},
$$

and explain its geometric action on vectors in the plane.
å¹¶è§£é‡Šå…¶å¯¹å¹³é¢å‘é‡çš„å‡ ä½•ä½œç”¨ã€‚

5.  Prove that if $A$ is invertible, then so is $A^T$, and $(A^T)^{-1} = (A^{-1})^T$.
    è¯æ˜å¦‚æœ $A$ å¯é€†ï¼Œåˆ™ $A^T$ å’Œ $(A^T)^{-1} = (A^{-1})^T$ ä¹Ÿå¯é€†ã€‚

## 2.4 Special Matrices
2.4 ç‰¹æ®ŠçŸ©é˜µ

Certain matrices occur so frequently in theory and applications that they are given special names. Recognizing their properties allows us to simplify computations and understand the structure of linear transformations more clearly.
æŸäº›çŸ©é˜µåœ¨ç†è®ºå’Œåº”ç”¨ä¸­å‡ºç°é¢‘ç‡å¾ˆé«˜ï¼Œå› æ­¤è¢«èµ‹äºˆäº†ç‰¹æ®Šçš„åç§°ã€‚äº†è§£å®ƒä»¬çš„æ€§è´¨å¯ä»¥ç®€åŒ–è®¡ç®—ï¼Œå¹¶æ›´æ¸…æ¥šåœ°ç†è§£çº¿æ€§å˜æ¢çš„ç»“æ„ã€‚

### The Identity Matrix
èº«ä»½çŸ©é˜µ

The identity matrix $I_n$ is the $n \times n$ matrix with ones on the diagonal and zeros elsewhere:
å•ä½çŸ©é˜µğ¼ ğ‘› I n â€‹ æ˜¯ $n \times n$ çŸ©é˜µï¼Œå¯¹è§’çº¿ä¸Šä¸º 1ï¼Œå…¶ä»–ä½ç½®ä¸º 0ï¼š

$$
I_n = \begin{bmatrix}1 & 0 & \cdots & 0 \\0 & 1 & \cdots & 0 \\\vdots & \vdots & \ddots & \vdots \\0 & 0 & \cdots & 1\end{bmatrix}.
$$

It acts as the multiplicative identity:
å®ƒå……å½“ä¹˜æ³•æ’ç­‰å¼ï¼š

$$
AI_n = I_nA = A, \quad \text{for all } A \in \mathbb{R}^{n \times n}.
$$

Geometrically, $I_n$ represents the transformation that leaves every vector unchanged.
ä»å‡ ä½•å­¦ä¸Šè®²ï¼Œğ¼ ğ‘› I n â€‹ è¡¨ç¤ºä¿æŒæ¯ä¸ªå‘é‡ä¸å˜çš„å˜æ¢ã€‚

### Diagonal Matrices
å¯¹è§’çŸ©é˜µ

A diagonal matrix has all off-diagonal entries zero:
å¯¹è§’çŸ©é˜µçš„æ‰€æœ‰éå¯¹è§’å…ƒç´ å‡ä¸ºé›¶ï¼š

$$
D = \begin{bmatrix}d_{11} & 0 & \cdots & 0 \\0 & d_{22} & \cdots & 0 \\\vdots & \vdots & \ddots & \vdots \\0 & 0 & \cdots & d_{nn}\end{bmatrix}.
$$

Multiplication by a diagonal matrix scales each coordinate independently:
ä¸å¯¹è§’çŸ©é˜µç›¸ä¹˜å¯ç‹¬ç«‹ç¼©æ”¾æ¯ä¸ªåæ ‡ï¼š

$$
D\mathbf{x} = (d_{11}x_1, d_{22}x_2, \dots, d_{nn}x_n).
$$

Example 2.4.1. Let
ä¾‹ 2.4.1. è®¾

$$
D = \begin{bmatrix} 2 & 0 & 0 \\0 & 3 & 0 \\0 & 0 & -1 \end{bmatrix}, \quad\mathbf{x} = \begin{bmatrix}1 \\4 \\-2 \end{bmatrix}.
$$

Then
ç„¶å

$$
D\mathbf{x} = \begin{bmatrix}2 \\12 \\2 \end{bmatrix}.
$$

### Permutation Matrices
ç½®æ¢çŸ©é˜µ

A permutation matrix is obtained by permuting the rows of the identity matrix. Multiplying a vector by a permutation matrix reorders its coordinates.
ç½®æ¢çŸ©é˜µæ˜¯é€šè¿‡å¯¹å•ä½çŸ©é˜µçš„è¡Œè¿›è¡Œç½®æ¢è€Œå¾—åˆ°çš„ã€‚å°†å‘é‡ä¹˜ä»¥ç½®æ¢çŸ©é˜µä¼šé‡æ–°æ’åºå…¶åæ ‡ã€‚

Example 2.4.2. Let
ä¾‹ 2.4.2. è®¾

$$
P = \begin{bmatrix}0 & 1 & 0 \\1 & 0 & 0 \\0 & 0 & 1\end{bmatrix}.
$$

Then
ç„¶å

$$
P\begin{bmatrix}a \\b \\c \end{bmatrix} =\begin{bmatrix} b \\a \\c \end{bmatrix}.
$$

Thus, $P$ swaps the first two coordinates.
å› æ­¤ï¼Œ $P$ äº¤æ¢å‰ä¸¤ä¸ªåæ ‡ã€‚

Permutation matrices are always invertible; their inverses are simply their transposes.
ç½®æ¢çŸ©é˜µæ€»æ˜¯å¯é€†çš„ï¼›å®ƒä»¬çš„é€†åªæ˜¯å®ƒä»¬çš„è½¬ç½®ã€‚

### Symmetric and Skew-Symmetric Matrices
å¯¹ç§°çŸ©é˜µå’Œæ–œå¯¹ç§°çŸ©é˜µ

A matrix is symmetric if
å¦‚æœçŸ©é˜µæ˜¯å¯¹ç§°çš„

$$
A^T = A,
$$

and skew-symmetric if Symmetric matrices appear in quadratic forms and optimization, while skew-symmetric matrices describe rotations and cross products in geometry.
å¦‚æœå¯¹ç§°çŸ©é˜µå‡ºç°åœ¨äºŒæ¬¡å½¢å¼å’Œä¼˜åŒ–ä¸­ï¼Œåˆ™ä¸ºæ–œå¯¹ç§°ï¼Œè€Œæ–œå¯¹ç§°çŸ©é˜µæè¿°å‡ ä½•ä¸­çš„æ—‹è½¬å’Œå‰ç§¯ã€‚

### Orthogonal Matrices
æ­£äº¤çŸ©é˜µ

A square matrix $Q$ is orthogonal if
æ–¹é˜µ $Q$ æ˜¯æ­£äº¤çš„ï¼Œå¦‚æœ

$$
Q^T Q = QQ^T = I.
$$

Equivalently, the rows (and columns) of $Q$ form an orthonormal set. Orthogonal matrices preserve lengths and angles; they represent rotations and reflections.
ç­‰ä»·åœ°ï¼Œ $Q$ çš„è¡Œï¼ˆå’Œåˆ—ï¼‰æ„æˆä¸€ä¸ªæ­£äº¤é›†ã€‚æ­£äº¤çŸ©é˜µä¿ç•™é•¿åº¦å’Œè§’åº¦ï¼›å®ƒä»¬è¡¨ç¤ºæ—‹è½¬å’Œåå°„ã€‚

Example 2.4.3. The rotation matrix in the plane:
ä¾‹2.4.3. å¹³é¢å†…çš„æ—‹è½¬çŸ©é˜µ:

$$
R(\theta) = \begin{bmatrix}\cos\theta & -\sin\theta \\\sin\theta & \cos\theta\end{bmatrix}
$$

is orthogonal, since
æ˜¯æ­£äº¤çš„ï¼Œå› ä¸º

$$
R(\theta)^T R(\theta) = I_2.
$$

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Special matrices serve as the building blocks of linear algebra. Identity matrices define the neutral element, diagonal matrices simplify computations, permutation matrices reorder data, symmetric and orthogonal matrices describe fundamental geometric structures. Much of modern applied mathematics reduces complex problems to operations involving these simple forms.
ç‰¹æ®ŠçŸ©é˜µæ˜¯çº¿æ€§ä»£æ•°çš„åŸºçŸ³ã€‚å•ä½çŸ©é˜µå®šä¹‰ä¸­æ€§å…ƒç´ ï¼Œå¯¹è§’çŸ©é˜µç®€åŒ–è®¡ç®—ï¼Œç½®æ¢çŸ©é˜µé‡æ–°æ’åºæ•°æ®ï¼Œå¯¹ç§°çŸ©é˜µå’Œæ­£äº¤çŸ©é˜µæè¿°åŸºæœ¬å‡ ä½•ç»“æ„ã€‚è®¸å¤šç°ä»£åº”ç”¨æ•°å­¦å°†å¤æ‚é—®é¢˜ç®€åŒ–ä¸ºæ¶‰åŠè¿™äº›ç®€å•å½¢å¼çš„è¿ç®—ã€‚

### Exercises 2.4
ç»ƒä¹  2.4

1.  Show that the product of two diagonal matrices is diagonal, and compute an example.
    è¯æ˜ä¸¤ä¸ªå¯¹è§’çŸ©é˜µçš„ä¹˜ç§¯æ˜¯å¯¹è§’çš„ï¼Œå¹¶è®¡ç®—ä¸€ä¸ªä¾‹å­ã€‚
2.  Find the permutation matrix that cycles $(a,b,c)$ into $(b,c,a)$.
    æ‰¾åˆ°å°† $(a,b,c)$ å¾ªç¯åˆ° $(b,c,a)$ çš„ç½®æ¢çŸ©é˜µã€‚
3.  Prove that every permutation matrix is invertible and its inverse is its transpose.
    è¯æ˜æ¯ä¸ªç½®æ¢çŸ©é˜µéƒ½æ˜¯å¯é€†çš„ï¼Œå¹¶ä¸”å®ƒçš„é€†æ˜¯å®ƒçš„è½¬ç½®ã€‚
4.  Verify that
    éªŒè¯

$$
Q = \begin{bmatrix}0 & 1 \\-1 & 0 \end{bmatrix}
$$

is orthogonal. What geometric transformation does it represent? 5. Determine whether
æ˜¯æ­£äº¤çš„ã€‚å®ƒä»£è¡¨ä»€ä¹ˆå‡ ä½•å˜æ¢ï¼Ÿ5. åˆ¤æ–­

$$
A = \begin{bmatrix}2 & 3 \\3 & 2 \end{bmatrix}, \quadB = \begin{bmatrix}0 & 5 \\-5 & 0 \end{bmatrix}
$$

are symmetric, skew-symmetric, or neither.
æ˜¯å¯¹ç§°çš„ã€æ–œå¯¹ç§°çš„ï¼Œæˆ–è€…éƒ½ä¸æ˜¯ã€‚

# Chapter 3. Systems of Linear Equations
ç¬¬ 3 ç« çº¿æ€§æ–¹ç¨‹ç»„

## 3.1 Linear Systems and Solutions
3.1 çº¿æ€§ç³»ç»ŸåŠå…¶è§£

One of the central motivations for linear algebra is solving systems of linear equations. These systems arise naturally in science, engineering, and data analysis whenever multiple constraints interact. Matrices provide a compact language for expressing and solving them.
çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒåŠ¨æœºä¹‹ä¸€æ˜¯æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„ã€‚åœ¨ç§‘å­¦ã€å·¥ç¨‹å’Œæ•°æ®åˆ†æé¢†åŸŸï¼Œå½“å¤šä¸ªçº¦æŸç›¸äº’ä½œç”¨æ—¶ï¼Œè¿™ç±»æ–¹ç¨‹ç»„è‡ªç„¶è€Œç„¶åœ°å‡ºç°ã€‚çŸ©é˜µæä¾›äº†ä¸€ç§ç®€æ´çš„è¯­è¨€æ¥è¡¨è¾¾å’Œæ±‚è§£å®ƒä»¬ã€‚

### Linear Systems
çº¿æ€§ç³»ç»Ÿ

A linear system consists of equations where each unknown appears only to the first power and with no products between variables. A general system of $m$ equations in $n$ unknowns can be written as:
çº¿æ€§ç³»ç»Ÿç”±æ–¹ç¨‹ç»„æˆï¼Œå…¶ä¸­æ¯ä¸ªæœªçŸ¥æ•°ä»…å‡ºç°ä¸€æ¬¡æ–¹ï¼Œå¹¶ä¸”ä¹‹é—´æ²¡æœ‰ä¹˜ç§¯ å˜é‡ã€‚åŒ…å« $n$ ä¸ªæœªçŸ¥æ•°çš„ $m$ ä¸ªæ–¹ç¨‹çš„ä¸€èˆ¬ç³»ç»Ÿå¯ä»¥å†™æˆï¼š

$$
\begin{aligned}a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= b_1, \\a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= b_2, \\&\vdots \\a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &= b_m.\end{aligned}
$$

Here the coefficients $a_{ij}$ and constants $b_i$ are scalars, and the unknowns are $x_1, x_2, \dots, x_n$.
è¿™é‡Œç³»æ•°ğ‘ ğ‘– ğ‘— a ä¼Šå¥‡ â€‹ å’Œå¸¸æ•°ğ‘ ğ‘– b i â€‹ æ˜¯æ ‡é‡ï¼ŒæœªçŸ¥æ•°æ˜¯ğ‘¥ 1 , ğ‘¥ 2 , â€¦ , ğ‘¥ ğ‘› x 1 â€‹ ï¼Œx 2 â€‹ ï¼Œâ€¦ï¼Œx n â€‹ .

### Matrix Form
çŸ©é˜µå½¢å¼

The system can be expressed compactly as:
è¯¥ç³»ç»Ÿå¯ä»¥ç®€æ´åœ°è¡¨ç¤ºä¸ºï¼š

$$
A\mathbf{x} = \mathbf{b},
$$

where
åœ¨å“ªé‡Œ

*   $A \in \mathbb{R}^{m \times n}$ is the coefficient matrix $[a_{ij}]$,
    $A \in \mathbb{R}^{m \times n}$ æ˜¯ç³»æ•°çŸ©é˜µ $[a_{ij}]$ ï¼Œ
*   $\mathbf{x} \in \mathbb{R}^n$ is the column vector of unknowns,
    $\mathbf{x} \in \mathbb{R}^n$ æ˜¯æœªçŸ¥æ•°çš„åˆ—å‘é‡ï¼Œ
*   $\mathbf{b} \in \mathbb{R}^m$ is the column vector of constants.
    $\mathbf{b} \in \mathbb{R}^m$ æ˜¯å¸¸æ•°åˆ—å‘é‡ã€‚

This formulation turns the problem of solving equations into analyzing the action of a matrix.
è¿™ä¸ªå…¬å¼å°†è§£æ–¹ç¨‹çš„é—®é¢˜è½¬åŒ–ä¸ºåˆ†æçŸ©é˜µçš„ä½œç”¨ã€‚

Example 3.1.1. The system
ä¾‹ 3.1.1. ç³»ç»Ÿ

$$
\begin{cases}x + 2y = 5, \\3x - y = 4\end{cases}
$$

can be written as
å¯ä»¥å†™æˆ

$$
\begin{bmatrix} 1 & 2 \\ 3 & -1 \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}=\begin{bmatrix} 5 \\ 4 \end{bmatrix}.
$$

### Types of Solutions
è§£å†³æ–¹æ¡ˆç±»å‹

A linear system may have:
çº¿æ€§ç³»ç»Ÿå¯èƒ½æœ‰ï¼š

1.  No solution (inconsistent): The equations conflict. Example:
    æ— è§£ï¼ˆä¸ä¸€è‡´ï¼‰ï¼šæ–¹ç¨‹å¼ç›¸äº’çŸ›ç›¾ã€‚ä¾‹å¦‚ï¼š

$$
\begin{cases}x + y = 1 \\x + y = 2\end{cases}
$$

This system has no solution.
è¿™ä¸ªç³»ç»Ÿæ²¡æœ‰è§£å†³æ–¹æ¡ˆã€‚

2.  Exactly one solution (unique): The systemâ€™s equations intersect at a single point.
    åªæœ‰ä¸€ä¸ªè§£ï¼ˆå”¯ä¸€ï¼‰ï¼šç³»ç»Ÿæ–¹ç¨‹åœ¨ä¸€ä¸ªç‚¹ç›¸äº¤ã€‚
    Example: The following coefficient matrix:
    ä¾‹å¦‚ï¼šä»¥ä¸‹ç³»æ•°çŸ©é˜µï¼š

$$
\begin{bmatrix}1 & 2 \\3 & -1\end{bmatrix}
$$

has a unique solution.
æœ‰ä¸€ä¸ªç‹¬ç‰¹çš„è§£å†³æ–¹æ¡ˆã€‚

3.  Infinitely many solutions: The equations describe overlapping constraints (e.g., multiple equations representing the same line or plane).
    æ— æ•°ä¸ªè§£ï¼šæ–¹ç¨‹æè¿°é‡å çš„çº¦æŸï¼ˆä¾‹å¦‚ï¼Œè¡¨ç¤ºåŒä¸€æ¡çº¿æˆ–å¹³é¢çš„å¤šä¸ªæ–¹ç¨‹ï¼‰ã€‚

The nature of the solution depends on the rank of $A$ and its relation to the augmented matrix $(A|\mathbf{b})$, which we will study later.
è§£çš„æ€§è´¨å–å†³äº $A$ çš„ç§©åŠå…¶ä¸å¢å¹¿çŸ©é˜µ $(A|\mathbf{b})$ çš„å…³ç³»ï¼Œæˆ‘ä»¬ç¨åä¼šç ”ç©¶ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

*   In $\mathbb{R}^2$, each linear equation represents a line. Solving a system means finding intersection points of lines.
    åœ¨ $\mathbb{R}^2$ ä¸­ï¼Œæ¯ä¸ªçº¿æ€§æ–¹ç¨‹ä»£è¡¨ä¸€æ¡ç›´çº¿ã€‚æ±‚è§£æ–¹ç¨‹ç»„æ„å‘³ç€æ‰¾åˆ°ç›´çº¿çš„äº¤ç‚¹ã€‚
*   In $\mathbb{R}^3$, each equation represents a plane. A system may have no solution (parallel planes), one solution (a unique intersection point), or infinitely many (a line of intersection).
    åœ¨ $\mathbb{R}^3$ ä¸­ï¼Œæ¯ä¸ªæ–¹ç¨‹ä»£è¡¨ä¸€ä¸ªå¹³é¢ã€‚ä¸€ä¸ªæ–¹ç¨‹ç»„å¯èƒ½æ²¡æœ‰è§£ï¼ˆå¹³è¡Œå¹³é¢ï¼‰ï¼Œå¯èƒ½æœ‰ä¸€ä¸ªè§£ï¼ˆå”¯ä¸€çš„äº¤ç‚¹ï¼‰ï¼Œä¹Ÿå¯èƒ½æœ‰æ— æ•°ä¸ªè§£ï¼ˆä¸€æ¡äº¤çº¿ï¼‰ã€‚
*   In higher dimensions, the picture generalizes: solutions form intersections of hyperplanes.
    åœ¨æ›´é«˜ç»´åº¦ä¸­ï¼Œè¯¥å›¾æ¦‚æ‹¬ä¸ºï¼šè§£å†³æ–¹æ¡ˆå½¢æˆè¶…å¹³é¢çš„äº¤ç‚¹ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Linear systems are the practical foundation of linear algebra. They appear in balancing chemical reactions, circuit analysis, least-squares regression, optimization, and computer graphics. Understanding how to represent and classify their solutions is the first step toward systematic solution methods like Gaussian elimination.
çº¿æ€§ç³»ç»Ÿæ˜¯çº¿æ€§ä»£æ•°çš„å®è·µåŸºç¡€ã€‚å®ƒä»¬å‡ºç°åœ¨å¹³è¡¡åŒ–å­¦ååº”ã€ç”µè·¯åˆ†æã€æœ€å°äºŒä¹˜å›å½’ã€ä¼˜åŒ–å’Œè®¡ç®—æœºå›¾å½¢å­¦ä¸­ã€‚äº†è§£å¦‚ä½•è¡¨ç¤ºå’Œåˆ†ç±»å®ƒä»¬çš„è§£æ˜¯è¿ˆå‘é«˜æ–¯æ¶ˆå…ƒæ³•ç­‰ç³»ç»Ÿæ±‚è§£æ–¹æ³•çš„ç¬¬ä¸€æ­¥ã€‚

### Exercises 3.1
ç»ƒä¹ 3.1

1.  Write the following system in matrix form:
    å°†ä»¥ä¸‹ç³»ç»Ÿå†™æˆçŸ©é˜µå½¢å¼ï¼š

$$
\begin{cases}2x + 3y - z = 7, \\x - y + 4z = 1, \\3x + 2y + z = 5\end{cases}
$$

2.  Determine whether the system
    ç¡®å®šç³»ç»Ÿæ˜¯å¦

$$
\begin{cases}x + y = 1, \\2x + 2y = 2\end{cases}
$$

has no solution, one solution, or infinitely many solutions.
æœ‰æ— è§£ã€æœ‰ä¸€ä¸ªè§£æˆ–æœ‰æ— æ•°ä¸ªè§£ã€‚

3.  Geometrically interpret the system
    å‡ ä½•è§£é‡Šç³»ç»Ÿ

$$
\begin{cases}x + y = 3, \\x - y = 1\end{cases}
$$

in the plane.
åœ¨é£æœºä¸Šã€‚

4.  Solve the system
    è§£å†³ç³»ç»Ÿ

$$
\begin{cases}2x + y = 1, \\x - y = 4\end{cases}
$$

and check your solution.
å¹¶æ£€æŸ¥æ‚¨çš„è§£å†³æ–¹æ¡ˆã€‚

5.  In $\mathbb{R}^3$, describe the solution set of
    åœ¨ $\mathbb{R}^3$ ä¸­ï¼Œæè¿°

$$
\begin{cases}x + y + z = 0, \\2x + 2y + 2z = 0\end{cases}
$$

What geometric object does it represent?
å®ƒä»£è¡¨ä»€ä¹ˆå‡ ä½•å¯¹è±¡ï¼Ÿ

## 3.2 Gaussian Elimination
3.2 é«˜æ–¯æ¶ˆå…ƒæ³•

To solve linear systems efficiently, we use Gaussian elimination: a systematic method of transforming a system into a simpler equivalent one whose solutions are easier to see. The method relies on elementary row operations that preserve the solution set.
ä¸ºäº†é«˜æ•ˆåœ°æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„ï¼Œæˆ‘ä»¬ä½¿ç”¨é«˜æ–¯æ¶ˆå…ƒæ³•ï¼šè¿™æ˜¯ä¸€ç§å°†æ–¹ç¨‹ç»„è½¬åŒ–ä¸ºæ›´ç®€å•ã€æ›´æ˜“è§£çš„ç­‰æ•ˆæ–¹ç¨‹çš„ç³»ç»Ÿæ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¾èµ–äºä¿ç•™è§£é›†çš„åŸºæœ¬è¡Œè¿ç®—ã€‚

### Elementary Row Operations
åˆç­‰è¡Œè¿ç®—

On an augmented matrix $(A|\mathbf{b})$, we are allowed three operations:
å¯¹äºå¢å¹¿çŸ©é˜µ $(A|\mathbf{b})$ ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œä¸‰ç§è¿ç®—ï¼š

1.  Row swapping: interchange two rows.
    æ¢è¡Œï¼šäº¤æ¢ä¸¤è¡Œã€‚
2.  Row scaling: multiply a row by a nonzero scalar.
    è¡Œç¼©æ”¾ï¼šå°†ä¸€è¡Œä¹˜ä»¥éé›¶æ ‡é‡ã€‚
3.  Row replacement: replace one row by itself plus a multiple of another row.
    è¡Œæ›¿æ¢ï¼šç”¨ä¸€è¡Œæœ¬èº«åŠ ä¸Šå¦ä¸€è¡Œçš„å€æ•°æ¥æ›¿æ¢ä¸€è¡Œã€‚

These operations correspond to re-expressing equations in different but equivalent forms.
è¿™äº›è¿ç®—å¯¹åº”äºä»¥ä¸åŒä½†ç­‰æ•ˆçš„å½¢å¼é‡æ–°è¡¨è¾¾æ–¹ç¨‹ã€‚

### Row Echelon Form
è¡Œæ¢¯é˜Ÿå½¢å¼

A matrix is in row echelon form (REF) if:
å¦‚æœæ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼Œåˆ™çŸ©é˜µä¸ºè¡Œé˜¶æ¢¯å½¢çŸ©é˜µï¼ˆREFï¼‰ï¼š

1.  All nonzero rows are above any zero rows.
    æ‰€æœ‰éé›¶è¡Œå‡ä½äºä»»ä½•é›¶è¡Œä¹‹ä¸Šã€‚
2.  Each leading entry (the first nonzero number from the left in a row) is to the right of the leading entry in the row above.
    æ¯ä¸ªå‰å¯¼æ¡ç›®ï¼ˆä¸€è¡Œä¸­ä»å·¦è¾¹å¼€å§‹çš„ç¬¬ä¸€ä¸ªéé›¶æ•°å­—ï¼‰ä½äºä¸Šä¸€è¡Œå‰å¯¼æ¡ç›®çš„å³ä¾§ã€‚
3.  All entries below a leading entry are zero.
    å‰å¯¼æ¡ç›®ä¸‹é¢çš„æ‰€æœ‰æ¡ç›®éƒ½ä¸ºé›¶ã€‚

Further, if each leading entry is 1 and is the only nonzero entry in its column, the matrix is in reduced row echelon form (RREF).
æ­¤å¤–ï¼Œå¦‚æœæ¯ä¸ªå‰å¯¼é¡¹éƒ½æ˜¯ 1ï¼Œå¹¶ä¸”æ˜¯å…¶åˆ—ä¸­å”¯ä¸€çš„éé›¶é¡¹ï¼Œåˆ™çŸ©é˜µä¸ºç®€åŒ–è¡Œé˜¶æ¢¯å½¢å¼ (RREF)ã€‚

### Algorithm of Gaussian Elimination
é«˜æ–¯æ¶ˆå…ƒæ³•

1.  Write the augmented matrix for the system.
    å†™å‡ºç³»ç»Ÿçš„å¢å¹¿çŸ©é˜µã€‚
2.  Use row operations to create zeros below each pivot (the leading entry in a row).
    ä½¿ç”¨è¡Œè¿ç®—åœ¨æ¯ä¸ªæ¢è½´ï¼ˆä¸€è¡Œä¸­çš„å‰å¯¼æ¡ç›®ï¼‰ä¸‹æ–¹åˆ›å»ºé›¶ã€‚
3.  Continue column by column until the matrix is in echelon form.
    ç»§ç»­é€åˆ—è¿›è¡Œï¼Œç›´åˆ°çŸ©é˜µå‘ˆé˜¶æ¢¯å½¢å¼ã€‚
4.  Solve by back substitution: starting from the last pivot equation and working upward.
    é€šè¿‡åå‘ä»£å…¥æ¥æ±‚è§£ï¼šä»æœ€åä¸€ä¸ªæ¢è½´æ–¹ç¨‹å¼€å§‹å‘ä¸Šæ±‚è§£ã€‚

If we continue to RREF, the solution can be read off directly.
å¦‚æœæˆ‘ä»¬ç»§ç»­ RREFï¼Œåˆ™å¯ä»¥ç›´æ¥è¯»å‡ºè§£å†³æ–¹æ¡ˆã€‚

### Example
ä¾‹å­

Example 3.2.1. Solve
ä¾‹ 3.2.1. æ±‚è§£

$$
\begin{cases}x + 2y - z = 3, \\2x + y + z = 7, \\3x - y + 2z = 4.\end{cases}
$$

Step 1. Augmented matrix
æ­¥éª¤1.å¢å¹¿çŸ©é˜µ

$$
\left[\begin{array}{ccc|c}1 & 2 & -1 & 3 \\2 & 1 & 1 & 7 \\3 & -1 & 2 & 4\end{array}\right].
$$

Step 2. Eliminate below the first pivot
æ­¥éª¤ 2. æ¶ˆé™¤ç¬¬ä¸€ä¸ªæ¢è½´ä»¥ä¸‹

Subtract 2 times row 1 from row 2, and 3 times row 1 from row 3:
ä»ç¬¬ 2 è¡Œå‡å»ç¬¬ 1 è¡Œçš„ 2 å€ï¼Œä»ç¬¬ 3 è¡Œå‡å»ç¬¬ 1 è¡Œçš„ 3 å€ï¼š

$$
\left[\begin{array}{ccc|c}1 & 2 & -1 & 3 \\0 & -3 & 3 & 1 \\0 & -7 & 5 & -5\end{array}\right].
$$

Step 3. Pivot in column 2
æ­¥éª¤ 3. åœ¨ç¬¬ 2 åˆ—ä¸­è¿›è¡Œé€è§†

Divide row 2 by -3:
å°†ç¬¬ 2 è¡Œé™¤ä»¥ -3ï¼š

$$
\left[\begin{array}{ccc|c}1 & 2 & -1 & 3 \\0 & 1 & -1 & -\tfrac{1}{3} \\0 & -7 & 5 & -5\end{array}\right].
$$

Add 7 times row 2 to row 3:
å°†ç¬¬ 2 è¡Œçš„ 7 å€åŠ åˆ°ç¬¬ 3 è¡Œï¼š

$$
\left[\begin{array}{ccc|c}1 & 2 & -1 & 3 \\0 & 1 & -1 & -\tfrac{1}{3} \\0 & 0 & -2 & -\tfrac{22}{3}\end{array}\right].
$$

Step 4. Pivot in column 3
æ­¥éª¤ 4. åœ¨ç¬¬ 3 åˆ—ä¸­è¿›è¡Œé€è§†

Divide row 3 by -2:
å°†ç¬¬ 3 è¡Œé™¤ä»¥ -2ï¼š

$$
\left[\begin{array}{ccc|c}1 & 2 & -1 & 3 \\0 & 1 & -1 & -\tfrac{1}{3} \\0 & 0 & 1 & \tfrac{11}{3}\end{array}\right].
$$

Step 5. Back substitution
æ­¥éª¤ 5. å›ä»£

From the last row:
ä»æœ€åä¸€è¡Œå¼€å§‹ï¼š

$$
z = \tfrac{11}{3}.
$$

Second row:
ç¬¬äºŒè¡Œï¼š

$$
y - z = -\tfrac{1}{3} \implies y = -\tfrac{1}{3} + \tfrac{11}{3} = \tfrac{10}{3}.
$$

First row:
ç¬¬ä¸€è¡Œï¼š

$$
x + 2y - z = 3 \implies x + 2\cdot\tfrac{10}{3} - \tfrac{11}{3} = 3.
$$

So
æ‰€ä»¥

$$
x + \tfrac{20}{3} - \tfrac{11}{3} = 3 \implies x + 3 = 3 \implies x = 0.
$$

Solution:
è§£å†³æ–¹æ¡ˆï¼š

$$
(x,y,z) = \big(0, \tfrac{10}{3}, \tfrac{11}{3}\big).
$$

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Gaussian elimination is the foundation of computational linear algebra. It reduces complex systems to a form where solutions are visible, and it forms the basis for algorithms used in numerical analysis, scientific computing, and machine learning.
é«˜æ–¯æ¶ˆå…ƒæ³•æ˜¯è®¡ç®—çº¿æ€§ä»£æ•°çš„åŸºç¡€ã€‚å®ƒå°†å¤æ‚ç³»ç»Ÿç®€åŒ–ä¸ºå¯è§è§£çš„å½¢å¼ï¼Œå¹¶æ„æˆæ•°å€¼åˆ†æã€ç§‘å­¦è®¡ç®—å’Œæœºå™¨å­¦ä¹ ä¸­ä½¿ç”¨çš„ç®—æ³•çš„åŸºç¡€ã€‚

### Exercises 3.2
ç»ƒä¹  3.2

1.  Solve by Gaussian elimination:
    é€šè¿‡é«˜æ–¯æ¶ˆå…ƒæ³•æ±‚è§£ï¼š

$$
\begin{cases}x + y = 2, \\2x - y = 0.\end{cases}
$$

2.  Reduce the following augmented matrix to REF:
    å°†ä»¥ä¸‹å¢å¹¿çŸ©é˜µç®€åŒ–ä¸º REFï¼š

$$
\left[\begin{array}{ccc|c}1 & 1 & 1 & 6 \\2 & -1 & 3 & 14 \\1 & 4 & -2 & -2\end{array}\right].
$$

3.  Show that Gaussian elimination always produces either:
    è¯æ˜é«˜æ–¯æ¶ˆå…ƒæ³•æ€»æ˜¯äº§ç”Ÿä»¥ä¸‹ç»“æœï¼š

*   a unique solution,
    ä¸€ä¸ªç‹¬ç‰¹çš„è§£å†³æ–¹æ¡ˆï¼Œ
*   infinitely many solutions, or
    æ— ç©·å¤šä¸ªè§£ï¼Œæˆ–è€…
*   a contradiction (no solution).
    çŸ›ç›¾ï¼ˆæ— è§£ï¼‰ã€‚

4.  Use Gaussian elimination to find all solutions of
    ä½¿ç”¨é«˜æ–¯æ¶ˆå…ƒæ³•æ‰¾åˆ°æ‰€æœ‰è§£

$$
\begin{cases}x + y + z = 0, \\2x + y + z = 1.\end{cases}
$$

5.  Explain why pivoting (choosing the largest available pivot element) is useful in numerical computation.
    è§£é‡Šä¸ºä»€ä¹ˆæ¢è½´æ—‹è½¬ï¼ˆé€‰æ‹©æœ€å¤§çš„å¯ç”¨æ¢è½´å…ƒç´ ï¼‰åœ¨æ•°å€¼è®¡ç®—ä¸­å¾ˆæœ‰ç”¨ã€‚

## 3.3 Rank and Consistency
3.3 ç­‰çº§å’Œä¸€è‡´æ€§

Gaussian elimination not only provides solutions but also reveals the structure of a linear system. Two key ideas are the rank of a matrix and the consistency of a system. Rank measures the amount of independent information in the equations, while consistency determines whether the system has at least one solution.
é«˜æ–¯æ¶ˆå…ƒæ³•ä¸ä»…èƒ½æä¾›è§£ï¼Œè¿˜èƒ½æ­ç¤ºçº¿æ€§ç³»ç»Ÿçš„ç»“æ„ã€‚ä¸¤ä¸ªå…³é”®æ¦‚å¿µæ˜¯çŸ©é˜µçš„ç§©å’Œç³»ç»Ÿçš„ä¸€è‡´æ€§ã€‚ç§©è¡¡é‡æ–¹ç¨‹ä¸­ç‹¬ç«‹ä¿¡æ¯çš„æ•°é‡ï¼Œè€Œä¸€è‡´æ€§åˆ™å†³å®šç³»ç»Ÿæ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªè§£ã€‚

### Rank of a Matrix
çŸ©é˜µçš„ç§©

The rank of a matrix is the number of leading pivots in its row echelon form. Equivalently, it is the maximum number of linearly independent rows or columns.
çŸ©é˜µçš„ç§©æ˜¯å…¶è¡Œé˜¶æ¢¯å½¢ä¸­å‰å¯¼ä¸»å…ƒçš„ä¸ªæ•°ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒæ˜¯çº¿æ€§æ— å…³çš„è¡Œæˆ–åˆ—çš„æœ€å¤§æ•°é‡ã€‚

Formally,
æ­£å¼åœ°ï¼Œ

$$
\text{rank}(A) = \dim(\text{row space of } A) = \dim(\text{column space of } A).
$$

The rank tells us the effective dimension of the space spanned by the rows (or columns).
ç§©å‘Šè¯‰æˆ‘ä»¬è¡Œï¼ˆæˆ–åˆ—ï¼‰æ‰€è·¨è¶Šçš„ç©ºé—´çš„æœ‰æ•ˆç»´åº¦ã€‚

Example 3.3.1. For
ä¾‹ 3.3.1. å¯¹äº

$$
A = \begin{bmatrix}1 & 2 & 3 \\2 & 4 & 6 \\3 & 6 & 9\end{bmatrix},
$$

row reduction gives
è¡Œå‡å°‘ç»™å‡º

$$
\begin{bmatrix}1 & 2 & 3 \\0 & 0 & 0 \\0 & 0 & 0\end{bmatrix}.
$$

Thus, $\text{rank}(A) = 1$, since all rows are multiples of the first.
å› æ­¤ï¼Œ $\text{rank}(A) = 1$ ï¼Œå› ä¸ºæ‰€æœ‰è¡Œéƒ½æ˜¯ç¬¬ä¸€è¡Œçš„å€æ•°ã€‚

### Consistency of Linear Systems
çº¿æ€§ç³»ç»Ÿçš„ä¸€è‡´æ€§

Consider the system $A\mathbf{x} = \mathbf{b}$. The system is consistent (has at least one solution) if and only if
è€ƒè™‘ç³»ç»Ÿ $A\mathbf{x} = \mathbf{b}$ ã€‚è¯¥ç³»ç»Ÿæ˜¯ä¸€è‡´çš„ï¼ˆè‡³å°‘æœ‰ä¸€ä¸ªè§£ï¼‰ï¼Œå½“ä¸”ä»…å½“

$$
\text{rank}(A) = \text{rank}(A|\mathbf{b}),
$$

where $(A|\mathbf{b})$ is the augmented matrix. If the ranks differ, the system is inconsistent.
å…¶ä¸­ $(A|\mathbf{b})$ æ˜¯å¢å¹¿çŸ©é˜µã€‚å¦‚æœç§©ä¸åŒï¼Œåˆ™ç³»ç»Ÿä¸ä¸€è‡´ã€‚

*   If $\text{rank}(A) = \text{rank}(A|\mathbf{b}) = n$ (number of unknowns), the system has a unique solution.
    å¦‚æœ $\text{rank}(A) = \text{rank}(A|\mathbf{b}) = n$ ï¼ˆæœªçŸ¥æ•°ï¼‰ï¼Œåˆ™ç³»ç»Ÿæœ‰ä¸€ä¸ªå”¯ä¸€çš„è§£ã€‚
*   If $\text{rank}(A) = \text{rank}(A|\mathbf{b}) < n$, the system has infinitely many solutions.
    å¦‚æœ $\text{rank}(A) = \text{rank}(A|\mathbf{b}) < n$ ï¼Œåˆ™ç³»ç»Ÿæœ‰æ— æ•°ä¸ªè§£ã€‚

### Example
ä¾‹å­

Example 3.3.2. Consider
ä¾‹ 3.3.2. è€ƒè™‘

$$
\begin{cases}x + y + z = 1, \\2x + 2y + 2z = 2, \\x + y + z = 3.\end{cases}
$$

The augmented matrix is
å¢å¹¿çŸ©é˜µæ˜¯

$$
\left[\begin{array}{ccc|c}1 & 1 & 1 & 1 \\2 & 2 & 2 & 2 \\1 & 1 & 1 & 3\end{array}\right].
$$

Row reduction gives
è¡Œå‡å°‘ç»™å‡º

$$
\left[\begin{array}{ccc|c}1 & 1 & 1 & 1 \\0 & 0 & 0 & 0 \\0 & 0 & 0 & 2\end{array}\right].
$$

Here, $\text{rank}(A) = 1$, but $\text{rank}(A|\mathbf{b}) = 2$. Since the ranks differ, the system is inconsistent: no solution exists.
è¿™é‡Œï¼Œ $\text{rank}(A) = 1$ ï¼Œä½† $\text{rank}(A|\mathbf{b}) = 2$ ã€‚ç”±äºç§©ä¸åŒï¼Œç³»ç»Ÿä¸ä¸€è‡´ï¼šä¸å­˜åœ¨è§£ã€‚

### Example with Infinite Solutions
æ— é™è§£çš„ä¾‹å­

Example 3.3.3. For
ä¾‹ 3.3.3. å¯¹äº

$$
\begin{cases}x + y = 2, \\2x + 2y = 4,\end{cases}
$$

the augmented matrix reduces to
å¢å¹¿çŸ©é˜µç®€åŒ–ä¸º

$$
\left[\begin{array}{cc|c}1 & 1 & 2 \\0 & 0 & 0\end{array}\right].
$$

Here, $\text{rank}(A) = \text{rank}(A|\mathbf{b}) = 1 < 2$. Thus, infinitely many solutions exist, forming a line.
è¿™é‡Œï¼Œ $\text{rank}(A) = \text{rank}(A|\mathbf{b}) = 1 < 2$ ã€‚å› æ­¤ï¼Œå­˜åœ¨æ— æ•°ä¸ªè§£ï¼Œå½¢æˆä¸€æ¡çº¿ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Rank is a measure of independence: it tells us how many truly distinct equations or directions are present. Consistency explains when equations align versus when they contradict. These concepts connect linear systems to vector spaces and prepare for the ideas of dimension, basis, and the Rankâ€“Nullity Theorem.
ç§©æ˜¯ç‹¬ç«‹æ€§çš„åº¦é‡ï¼šå®ƒå‘Šè¯‰æˆ‘ä»¬æœ‰å¤šå°‘ä¸ªçœŸæ­£ä¸åŒçš„æ–¹ç¨‹æˆ–æ–¹å‘ã€‚ä¸€è‡´æ€§è§£é‡Šäº†æ–¹ç¨‹ä½•æ—¶ä¸€è‡´ï¼Œä½•æ—¶çŸ›ç›¾ã€‚è¿™äº›æ¦‚å¿µå°†çº¿æ€§ç³»ç»Ÿä¸å‘é‡ç©ºé—´è”ç³»èµ·æ¥ï¼Œå¹¶ä¸ºç»´åº¦ã€åŸºå’Œç§©é›¶å®šç†çš„æ¦‚å¿µåšå¥½å‡†å¤‡ã€‚

### Exercises 3.3
ç»ƒä¹  3.3

1.  Compute the rank of
    è®¡ç®—

$$
A = \begin{bmatrix}1 & 2 & 1 \\0 & 1 & -1 \\2 & 5 & -1\end{bmatrix}.
$$

2.  Determine whether the system
    ç¡®å®šç³»ç»Ÿ

$$
\begin{cases}x + y + z = 1, \\2x + 3y + z = 2, \\3x + 5y + 2z = 3\end{cases}
$$

is consistent.
æ˜¯ä¸€è‡´çš„ã€‚

3.  Show that the rank of the identity matrix $I_n$ is $n$.
    è¯æ˜å•ä½çŸ©é˜µğ¼çš„ç§© ğ‘› I n â€‹ æ˜¯ $n$ ã€‚
    
4.  Give an example of a system in $\mathbb{R}^3$ with infinitely many solutions, and explain why it satisfies the rank condition.
    ç»™å‡º $\mathbb{R}^3$ ä¸­å…·æœ‰æ— ç©·å¤šä¸ªè§£çš„ç³»ç»Ÿçš„ä¾‹å­ï¼Œå¹¶è§£é‡Šå®ƒä¸ºä»€ä¹ˆæ»¡è¶³ç§©æ¡ä»¶ã€‚
    
5.  Prove that for any matrix $A \in \mathbb{R}^{m \times n}$, $\text{rank}(A) \leq \min(m,n).$
    è¯æ˜å¯¹äºä»»æ„çŸ©é˜µ $A \in \mathbb{R}^{m \times n}$ ï¼Œ $\text{rank}(A) \leq \min(m,n).$
    

## 3.4 Homogeneous Systems
3.4 å‡è´¨ç³»ç»Ÿ

A homogeneous system is a linear system in which all constant terms are zero:
é½æ¬¡ç³»ç»Ÿæ˜¯æ‰€æœ‰å¸¸æ•°é¡¹éƒ½ä¸ºé›¶çš„çº¿æ€§ç³»ç»Ÿï¼š

$$
A\mathbf{x} = \mathbf{0},
$$

where $A \in \mathbb{R}^{m \times n}$, and $\mathbf{0}$ is the zero vector in $\mathbb{R}^m$.
å…¶ä¸­ $A \in \mathbb{R}^{m \times n}$ ï¼Œä¸” $\mathbf{0}$ æ˜¯ $\mathbb{R}^m$ ä¸­çš„é›¶å‘é‡ã€‚

### The Trivial Solution
ç®€å•çš„è§£å†³æ–¹æ¡ˆ

Every homogeneous system has at least one solution:
æ¯ä¸ªåŒè´¨ç³»ç»Ÿè‡³å°‘æœ‰ä¸€ä¸ªè§£ï¼š

$$
\mathbf{x} = \mathbf{0}.
$$

This is called the trivial solution. The interesting question is whether *nontrivial solutions* (nonzero vectors) exist.
è¿™è¢«ç§°ä¸ºå¹³å‡¡è§£ã€‚æœ‰è¶£çš„é—®é¢˜æ˜¯æ˜¯å¦å­˜åœ¨*éå¹³å‡¡è§£* ï¼ˆéé›¶å‘é‡ï¼‰ã€‚

### Existence of Nontrivial Solutions
éå¹³å‡¡è§£çš„å­˜åœ¨æ€§

Nontrivial solutions exist precisely when the number of unknowns exceeds the rank of the coefficient matrix:
å½“æœªçŸ¥æ•°çš„æ•°é‡è¶…è¿‡ç³»æ•°çŸ©é˜µçš„ç§©æ—¶ï¼Œå°±ä¼šå­˜åœ¨éå¹³å‡¡è§£ï¼š

$$
\text{rank}(A) < n.
$$

In this case, there are infinitely many solutions, forming a subspace of $\mathbb{R}^n$. The dimension of this solution space is
åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ‰æ— ç©·å¤šä¸ªè§£ï¼Œå½¢æˆä¸€ä¸ª $\mathbb{R}^n$ çš„å­ç©ºé—´ã€‚è¿™ä¸ªè§£ç©ºé—´çš„ç»´åº¦æ˜¯

$$
\dim(\text{null}(A)) = n - \text{rank}(A),
$$

where null(A) is the set of all solutions to $A\mathbf{x} = 0$. This set is called the null space or kernel of $A$.
å…¶ä¸­ null(A) æ˜¯ $A\mathbf{x} = 0$ æ‰€æœ‰è§£çš„é›†åˆã€‚è¯¥é›†åˆç§°ä¸º $A$ çš„é›¶ç©ºé—´æˆ–é›¶æ ¸ã€‚

### Example
ä¾‹å­

Example 3.4.1. Consider
ä¾‹ 3.4.1. è€ƒè™‘

$$
\begin{cases}x + y + z = 0, \\2x + y - z = 0.\end{cases}
$$

The augmented matrix is
å¢å¹¿çŸ©é˜µæ˜¯

$$
\left[\begin{array}{ccc|c}1 & 1 & 1 & 0 \\2 & 1 & -1 & 0\end{array}\right].
$$

Row reduction:
è¡Œå‡å°‘ï¼š

$$
\left[\begin{array}{ccc|c}1 & 1 & 1 & 0 \\0 & -1 & -3 & 0\end{array}\right]\quad\to\quad\left[\begin{array}{ccc|c}1 & 1 & 1 & 0 \\0 & 1 & 3 & 0\end{array}\right].
$$

So the system is equivalent to:
å› æ­¤è¯¥ç³»ç»Ÿç­‰åŒäºï¼š

$$
\begin{cases}x + y + z = 0, \\y + 3z = 0.\end{cases}
$$

From the second equation, $y = -3z$. Substituting into the first: $x - 3z + z = 0 \implies x = 2z.$
ä»ç¬¬äºŒä¸ªæ–¹ç¨‹å¾—å‡º $y = -3z$ ã€‚ä»£å…¥ç¬¬ä¸€ä¸ªæ–¹ç¨‹ï¼š $x - 3z + z = 0 \implies x = 2z.$

Thus solutions are:
å› æ­¤è§£å†³æ–¹æ¡ˆæ˜¯ï¼š

$$
(x,y,z) = z(2, -3, 1), \quad z \in \mathbb{R}.
$$

The null space is the line spanned by the vector $(2, -3, 1)$.
é›¶ç©ºé—´æ˜¯å‘é‡ $(2, -3, 1)$ æ‰€è·¨è¶Šçš„çº¿ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

The solution set of a homogeneous system is always a subspace of $\mathbb{R}^n$.
åŒè´¨ç³»ç»Ÿçš„è§£é›†å§‹ç»ˆæ˜¯ $\mathbb{R}^n$ çš„å­ç©ºé—´ã€‚

*   If $\text{rank}(A) = n$, the only solution is the zero vector.
    å¦‚æœä¸º $\text{rank}(A) = n$ ï¼Œåˆ™å”¯ä¸€çš„è§£å°±æ˜¯é›¶å‘é‡ã€‚
*   If $\text{rank}(A) = n-1$, the solution set is a line through the origin.
    å¦‚æœä¸º $\text{rank}(A) = n-1$ ï¼Œåˆ™è§£é›†æ˜¯ä¸€æ¡è¿‡åŸç‚¹çš„çº¿ã€‚
*   If $\text{rank}(A) = n-2$, the solution set is a plane through the origin.
    å¦‚æœä¸º $\text{rank}(A) = n-2$ ï¼Œåˆ™è§£é›†æ˜¯é€šè¿‡åŸç‚¹çš„å¹³é¢ã€‚

More generally, the null space has dimension $n - \text{rank}(A)$, known as the nullity.
æ›´ä¸€èˆ¬åœ°ï¼Œé›¶ç©ºé—´çš„ç»´åº¦ä¸º $n - \text{rank}(A)$ ï¼Œç§°ä¸ºé›¶åº¦ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Homogeneous systems are central to understanding vector spaces, subspaces, and dimension. They lead directly to the concepts of kernel, null space, and linear dependence. In applications, homogeneous systems appear in equilibrium problems, eigenvalue equations, and computer graphics transformations.
é½æ¬¡ç³»ç»Ÿæ˜¯ç†è§£å‘é‡ç©ºé—´ã€å­ç©ºé—´å’Œç»´åº¦çš„æ ¸å¿ƒã€‚å®ƒä»¬ç›´æ¥å¼•å‡ºæ ¸ã€é›¶ç©ºé—´å’Œçº¿æ€§ç›¸å…³æ€§çš„æ¦‚å¿µã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé½æ¬¡ç³»ç»Ÿå‡ºç°åœ¨å¹³è¡¡é—®é¢˜ã€ç‰¹å¾å€¼æ–¹ç¨‹å’Œè®¡ç®—æœºå›¾å½¢å˜æ¢ä¸­ã€‚

### Exercises 3.4
ç»ƒä¹  3.4

1.  Solve the homogeneous system
    è§£å†³å‡è´¨ç³»ç»Ÿ

$$
\begin{cases}x + 2y - z = 0, \\2x + 4y - 2z = 0.\end{cases}
$$

What is the dimension of its solution space?
å…¶è§£ç©ºé—´çš„ç»´æ•°æ˜¯å¤šå°‘ï¼Ÿ

2.  Find all solutions of
    æ‰¾åˆ°æ‰€æœ‰è§£å†³æ–¹æ¡ˆ

$$
\begin{cases}x - y + z = 0, \\2x + y - z = 0.\end{cases}
$$

3.  Show that the solution set of any homogeneous system is a subspace of $\mathbb{R}^n$.
    è¯æ˜ä»»ä½•åŒè´¨ç³»ç»Ÿçš„è§£é›†éƒ½æ˜¯ $\mathbb{R}^n$ çš„å­ç©ºé—´ã€‚
    
4.  Suppose $A$ is a $3 \\times 3$matrix with$\\text{rank}(A) = 2$. What is the dimension of the null space of $A$?
    å‡è®¾ $A$ æ˜¯ $3 \\times 3 $matrix with$ \\text{rank}(A) = 2 $. What is the dimension of the null space of $ A$ï¼Ÿ
    
5.  For
    ä¸ºäº†
    

$$
A = \begin{bmatrix} 1 & 2 & -1 \\ 0 & 1 & 3 \end{bmatrix},
$$

compute a basis for the null space of $A$.
è®¡ç®— $A$ çš„é›¶ç©ºé—´çš„åŸºç¡€ã€‚

# Chapter 4. Vector Spaces
ç¬¬ 4 ç«  å‘é‡ç©ºé—´

## 4.1 Definition of a Vector Space
4.1 å‘é‡ç©ºé—´çš„å®šä¹‰

Up to now we have studied vectors and matrices concretely in $\mathbb{R}^n$. The next step is to move beyond coordinates and define vector spaces in full generality. A vector space is an abstract setting where the familiar rules of addition and scalar multiplication hold, regardless of whether the elements are geometric vectors, polynomials, functions, or other objects.
åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»åœ¨ $\mathbb{R}^n$ ä¸­å…·ä½“å­¦ä¹ äº†å‘é‡å’ŒçŸ©é˜µã€‚ä¸‹ä¸€æ­¥æ˜¯è¶…è¶Šåæ ‡ï¼Œå…¨é¢å®šä¹‰å‘é‡ç©ºé—´ã€‚å‘é‡ç©ºé—´æ˜¯ä¸€ä¸ªæŠ½è±¡çš„åœºæ™¯ï¼Œå…¶ä¸­ç†Ÿæ‚‰çš„åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•è§„åˆ™å§‹ç»ˆæˆç«‹ï¼Œæ— è®ºå…ƒç´ æ˜¯å‡ ä½•å‘é‡ã€å¤šé¡¹å¼ã€å‡½æ•°è¿˜æ˜¯å…¶ä»–å¯¹è±¡ã€‚

### Formal Definition
æ­£å¼å®šä¹‰

A vector space over the real numbers $\mathbb{R}$ is a set $V$ equipped with two operations:
å®æ•° $\mathbb{R}$ ä¸Šçš„å‘é‡ç©ºé—´æ˜¯å…·æœ‰ä¸¤ä¸ªè¿ç®—çš„é›†åˆ $V$ ï¼š

1.  Vector addition: For any $\mathbf{u}, \mathbf{v} \in V$, there is a vector $\mathbf{u} + \mathbf{v} \in V$.
    å‘é‡åŠ æ³•ï¼šå¯¹äºä»»ä½• $\mathbf{u}, \mathbf{v} \in V$ ï¼Œéƒ½æœ‰å‘é‡ $\mathbf{u} + \mathbf{v} \in V$ ã€‚
2.  Scalar multiplication: For any scalar $c \in \mathbb{R}$ and any $\mathbf{v} \in V$, there is a vector $c\mathbf{v} \in V$.
    æ ‡é‡ä¹˜æ³•ï¼šå¯¹äºä»»ä½•æ ‡é‡ $c \in \mathbb{R}$ å’Œä»»ä½• $\mathbf{v} \in V$ ï¼Œéƒ½æœ‰ä¸€ä¸ªå‘é‡ $c\mathbf{v} \in V$ ã€‚

These operations must satisfy the following axioms (for all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and all scalars $a,b \in \mathbb{R}$):
è¿™äº›è¿ç®—å¿…é¡»æ»¡è¶³ä»¥ä¸‹å…¬ç†ï¼ˆå¯¹äºæ‰€æœ‰ $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ å’Œæ‰€æœ‰æ ‡é‡ $a,b \in \mathbb{R}$ ï¼‰ï¼š

1.  Commutativity of addition: $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$.
    åŠ æ³•çš„äº¤æ¢æ€§ï¼š $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$ ã€‚
2.  Associativity of addition: $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$.
    åŠ æ³•çš„ç»“åˆæ€§ï¼š $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$ ã€‚
3.  Additive identity: There exists a zero vector $\mathbf{0} \in V$ such that $\mathbf{v} + \mathbf{0} = \mathbf{v}$.
    åŠ æ³•æ’ç­‰å¼ï¼šå­˜åœ¨é›¶å‘é‡ $\mathbf{0} \in V$ ä½¿å¾— $\mathbf{v} + \mathbf{0} = \mathbf{v}$ ã€‚
4.  Additive inverses: For each $\mathbf{v} \in V$, there exists $(-\mathbf{v} \in V$ such that $\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$.
    åŠ æ³•é€†å…ƒï¼šå¯¹äºæ¯ä¸ª $\mathbf{v} \in V$ ï¼Œå­˜åœ¨ $(-\mathbf{v} \in V$ ä½¿å¾— $\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$ ã€‚
5.  Compatibility of scalar multiplication: $a(b\mathbf{v}) = (ab)\mathbf{v}$.
    æ ‡é‡ä¹˜æ³•çš„å…¼å®¹æ€§ï¼š $a(b\mathbf{v}) = (ab)\mathbf{v}$ ã€‚
6.  Identity element of scalars: 1â‹…v\=v.
    æ ‡é‡çš„æ ‡è¯†å…ƒï¼š 1â‹…v\=v ã€‚
7.  Distributivity over vector addition: $a(\mathbf{u} + \mathbf{v}) = a\mathbf{u} + a\mathbf{v}$.
    å‘é‡åŠ æ³•çš„åˆ†é…å¾‹ï¼š $a(\mathbf{u} + \mathbf{v}) = a\mathbf{u} + a\mathbf{v}$ ã€‚
8.  Distributivity over scalar addition: $(a+b)\mathbf{v} = a\mathbf{v} + b\mathbf{v}$.
    æ ‡é‡åŠ æ³•çš„åˆ†é…å¾‹ï¼š $(a+b)\mathbf{v} = a\mathbf{v} + b\mathbf{v}$ ã€‚

If a set $V$ with operations satisfies all eight axioms, we call it a vector space.
å¦‚æœä¸€ä¸ªé›†åˆ $V$ æ»¡è¶³æ‰€æœ‰å…«ä¸ªå…¬ç†ï¼Œæˆ‘ä»¬ç§°å®ƒä¸ºå‘é‡ç©ºé—´ã€‚

### Examples
ç¤ºä¾‹

Example 4.1.1. Standard Euclidean space $\mathbb{R}^n$ with ordinary addition and scalar multiplication is a vector space. This is the model case from which the axioms are abstracted.
ä¾‹ 4.1.1. æ ‡å‡†æ¬§å‡ é‡Œå¾—ç©ºé—´ $\mathbb{R}^n$ è¿›è¡Œæ™®é€šçš„åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•è¿ç®—åï¼Œæ˜¯ä¸€ä¸ªå‘é‡ç©ºé—´ã€‚è¿™æ˜¯æŠ½è±¡å‡ºå…¬ç†çš„å…¸å‹ä¾‹å­ã€‚

Example 4.1.2. Polynomials The set of all polynomials with real coefficients, denoted $\mathbb{R}[x]$, forms a vector space. Addition and scalar multiplication are defined term by term.
ä¾‹ 4.1.2. å¤šé¡¹å¼ æ‰€æœ‰å®ç³»æ•°å¤šé¡¹å¼çš„é›†åˆï¼Œè®°ä¸º $\mathbb{R}[x]$ ï¼Œæ„æˆä¸€ä¸ªå‘é‡ç©ºé—´ã€‚åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•æ˜¯é€é¡¹å®šä¹‰çš„ã€‚

Example 4.1.3. Functions The set of all real-valued functions on an interval, e.g. $f: [0,1] \to \mathbb{R}$, forms a vector space, since functions can be added and scaled pointwise.
ä¾‹ 4.1.3. å‡½æ•° åŒºé—´ä¸Šçš„æ‰€æœ‰å®å€¼å‡½æ•°çš„é›†åˆï¼Œä¾‹å¦‚ $f: [0,1] \to \mathbb{R}$ ï¼Œå½¢æˆä¸€ä¸ªå‘é‡ç©ºé—´ï¼Œå› ä¸ºå‡½æ•°å¯ä»¥é€ç‚¹æ·»åŠ å’Œç¼©æ”¾ã€‚

### Non-Examples
éç¤ºä¾‹

Not every set with operations qualifies. For instance, the set of positive real numbers under usual addition is not a vector space, because additive inverses (negative numbers) are missing. The axioms must all hold.
å¹¶éæ‰€æœ‰åŒ…å«è¿ç®—çš„é›†åˆéƒ½ç¬¦åˆæ¡ä»¶ã€‚ä¾‹å¦‚ï¼Œé€šå¸¸åŠ æ³•è¿ç®—ä¸‹çš„æ­£å®æ•°é›†ä¸æ˜¯å‘é‡ç©ºé—´ï¼Œå› ä¸ºç¼ºå°‘åŠ æ³•é€†å…ƒï¼ˆè´Ÿæ•°ï¼‰ã€‚å…¬ç†å¿…é¡»å…¨éƒ¨æˆç«‹ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

In familiar cases like $\mathbb{R}^2$ or $\mathbb{R}^3$, vector spaces provide the stage for geometry: vectors can be added, scaled, and combined to form lines, planes, and higher-dimensional structures. In abstract settings like function spaces, the same algebraic rules let us apply geometric intuition to infinite-dimensional problems.
åœ¨åƒ $\mathbb{R}^2$ æˆ– $\mathbb{R}^3$ è¿™æ ·å¸¸è§çš„æƒ…å½¢ä¸‹ï¼Œå‘é‡ç©ºé—´ä¸ºå‡ ä½•å­¦æä¾›äº†èˆå°ï¼šå‘é‡å¯ä»¥ç›¸åŠ ã€ç¼©æ”¾å’Œç»„åˆï¼Œä»è€Œå½¢æˆçº¿ã€å¹³é¢å’Œæ›´é«˜ç»´åº¦çš„ç»“æ„ã€‚åœ¨åƒå‡½æ•°ç©ºé—´è¿™æ ·çš„æŠ½è±¡ç¯å¢ƒä¸­ï¼ŒåŒæ ·çš„ä»£æ•°è§„åˆ™è®©æˆ‘ä»¬èƒ½å¤Ÿå°†å‡ ä½•ç›´è§‰åº”ç”¨äºæ— é™ç»´é—®é¢˜ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

The concept of vector space unifies seemingly different mathematical objects under a single framework. Whether dealing with forces in physics, signals in engineering, or data in machine learning, the common language of vector spaces allows us to use the same techniques everywhere.
å‘é‡ç©ºé—´çš„æ¦‚å¿µå°†çœ‹ä¼¼ä¸åŒçš„æ•°å­¦å¯¹è±¡ç»Ÿä¸€åœ¨ä¸€ä¸ªæ¡†æ¶ä¸‹ã€‚æ— è®ºæ˜¯å¤„ç†ç‰©ç†å­¦ä¸­çš„åŠ›ã€å·¥ç¨‹å­¦ä¸­çš„ä¿¡å·ï¼Œè¿˜æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„æ•°æ®ï¼Œå‘é‡ç©ºé—´çš„é€šç”¨è¯­è¨€ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ä»»ä½•åœ°æ–¹ä½¿ç”¨ç›¸åŒçš„æŠ€æœ¯ã€‚

### Exercises 4.1
ç»ƒä¹ 4.1

1.  Verify that $\mathbb{R}^2$ with standard addition and scalar multiplication satisfies all eight vector space axioms.
    éªŒè¯ $\mathbb{R}^2$ é€šè¿‡æ ‡å‡†åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•æ»¡è¶³æ‰€æœ‰å…«ä¸ªå‘é‡ç©ºé—´å…¬ç†ã€‚
2.  Show that the set of integers $\mathbb{Z}$ with ordinary operations is not a vector space over $\mathbb{R}$. Which axiom fails?
    è¯æ˜ï¼šå…·æœ‰æ™®é€šè¿ç®—çš„æ•´æ•°é›† $\mathbb{Z}$ ä¸æ˜¯ $\mathbb{R}$ ä¸Šçš„å‘é‡ç©ºé—´ã€‚å“ªæ¡å…¬ç†ä¸æˆç«‹ï¼Ÿ
3.  Consider the set of all polynomials of degree at most 3. Show it forms a vector space over $\mathbb{R}$. What is its dimension?
    è€ƒè™‘æ‰€æœ‰æ¬¡æ•°æœ€å¤šä¸º3çš„å¤šé¡¹å¼çš„é›†åˆã€‚è¯æ˜å®ƒæ„æˆä¸€ä¸ª $\mathbb{R}$ ä¸Šçš„å‘é‡ç©ºé—´ã€‚å®ƒçš„ç»´åº¦æ˜¯å¤šå°‘ï¼Ÿ
4.  Give an example of a vector space where the vectors are not geometric objects.
    ç»™å‡ºä¸€ä¸ªå‘é‡ç©ºé—´çš„ä¾‹å­ï¼Œå…¶ä¸­çš„å‘é‡ä¸æ˜¯å‡ ä½•å¯¹è±¡ã€‚
5.  Prove that in any vector space, the zero vector is unique.
    è¯æ˜åœ¨ä»»ä½•å‘é‡ç©ºé—´ä¸­ï¼Œé›¶å‘é‡éƒ½æ˜¯å”¯ä¸€çš„ã€‚

## 4.2 Subspaces
4.2 å­ç©ºé—´

A subspace is a smaller vector space living inside a larger one. Just as lines and planes naturally sit inside three-dimensional space, subspaces generalize these ideas to higher dimensions and more abstract settings.
å­ç©ºé—´æ˜¯ä½äºè¾ƒå¤§å‘é‡ç©ºé—´ä¸­çš„è¾ƒå°å‘é‡ç©ºé—´ã€‚æ­£å¦‚çº¿å’Œå¹³é¢è‡ªç„¶åœ°å­˜åœ¨äºä¸‰ç»´ç©ºé—´ä¸­ä¸€æ ·ï¼Œå­ç©ºé—´å°†è¿™äº›æ¦‚å¿µæ¨å¹¿åˆ°æ›´é«˜ç»´åº¦å’Œæ›´æŠ½è±¡çš„åœºæ™¯ã€‚

### Definition
å®šä¹‰

Let $V$ be a vector space. A subset $W \subseteq V$ is called a subspace of $V$ if:
ä»¤ $V$ ä¸ºå‘é‡ç©ºé—´ã€‚è‹¥æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼Œåˆ™å­é›† $W \subseteq V$ ç§°ä¸º $V$ çš„å­ç©ºé—´ï¼š

1.  $\mathbf{0} \in W$ (contains the zero vector),
    $\mathbf{0} \in W$ ï¼ˆåŒ…å«é›¶å‘é‡ï¼‰ï¼Œ
2.  For all $\mathbf{u}, \mathbf{v} \in W$, the sum $\mathbf{u} + \mathbf{v} \in W$ (closed under addition),
    å¯¹äºæ‰€æœ‰ $\mathbf{u}, \mathbf{v} \in W$ ï¼Œæ€»å’Œä¸º $\mathbf{u} + \mathbf{v} \in W$ ï¼ˆåŠ æ³•é—­åŒ…ï¼‰ï¼Œ
3.  For all scalars $c \in \mathbb{R}$ and vectors $\mathbf{v} \in W$, the product $c\mathbf{v} \in W$ (closed under scalar multiplication).
    å¯¹äºæ‰€æœ‰æ ‡é‡ $c \in \mathbb{R}$ å’Œå‘é‡ $\mathbf{v} \in W$ ï¼Œä¹˜ç§¯ $c\mathbf{v} \in W$ ï¼ˆåœ¨æ ‡é‡ä¹˜æ³•ä¸‹å°é—­ï¼‰ã€‚

If these hold, then $W$ is itself a vector space with the inherited operations.
å¦‚æœè¿™äº›æˆç«‹ï¼Œé‚£ä¹ˆ $W$ æœ¬èº«å°±æ˜¯å…·æœ‰ç»§æ‰¿æ“ä½œçš„å‘é‡ç©ºé—´ã€‚

### Examples
ç¤ºä¾‹

Example 4.2.1. Line through the origin in $\mathbb{R}^2$ The set
ä¾‹ 4.2.1. ç©¿è¿‡ $\mathbb{R}^2$ ä¸­çš„åŸç‚¹çš„çº¿ è¯¥å¥—è£…

$$
W = \{ (t, 2t) \mid t \in \mathbb{R} \}
$$

is a subspace of $\mathbb{R}^2$. It contains the zero vector, is closed under addition, and is closed under scalar multiplication.
æ˜¯ $\mathbb{R}^2$ çš„ä¸€ä¸ªå­ç©ºé—´ã€‚å®ƒåŒ…å«é›¶å‘é‡ï¼Œåœ¨åŠ æ³•è¿ç®—ä¸‹å°é—­ï¼Œåœ¨æ ‡é‡ä¹˜æ³•è¿ç®—ä¸‹å°é—­ã€‚

Example 4.2.2. The xâ€“y plane in $\mathbb{R}^3$ The set
ä¾‹ 4.2.2. $\mathbb{R}^3$ ä¸­çš„ x-y å¹³é¢ è¯¥å¥—è£…

$$
W = \{ (x, y, 0) \mid x,y \in \mathbb{R} \}
$$

is a subspace of $\mathbb{R}^3$. It is the collection of all vectors lying in the plane through the origin parallel to the xâ€“y plane.
æ˜¯ $\mathbb{R}^3$ çš„ä¸€ä¸ªå­ç©ºé—´ã€‚å®ƒæ˜¯ä½äºé€šè¿‡åŸç‚¹å¹¶å¹³è¡Œäº x-y å¹³é¢çš„å¹³é¢å†…çš„æ‰€æœ‰å‘é‡çš„é›†åˆã€‚

Example 4.2.3. Null space of a matrix For a matrix $A \in \mathbb{R}^{m \times n}$, the null space
ä¾‹ 4.2.3. çŸ©é˜µçš„é›¶ç©ºé—´ å¯¹äºçŸ©é˜µ $A \in \mathbb{R}^{m \times n}$ ï¼Œé›¶ç©ºé—´

$$
\{ \mathbf{x} \in \mathbb{R}^n \mid A\mathbf{x} = \mathbf{0} \}
$$

is a subspace of $\mathbb{R}^n$. This subspace represents all solutions to the homogeneous system.
æ˜¯ $\mathbb{R}^n$ çš„ä¸€ä¸ªå­ç©ºé—´ã€‚è¯¥å­ç©ºé—´è¡¨ç¤ºé½æ¬¡ç³»ç»Ÿçš„æ‰€æœ‰è§£ã€‚

### Non-Examples
éç¤ºä¾‹

Not every subset is a subspace.
å¹¶éæ¯ä¸ªå­é›†éƒ½æ˜¯å­ç©ºé—´ã€‚

*   The set ${ (x,y) \in \mathbb{R}^2 \mid x \geq 0 }$ is not a subspace: it is not closed under scalar multiplication (a negative scalar breaks the condition).
    é›†åˆ ${ (x,y) \in \mathbb{R}^2 \mid x \geq 0 }$ ä¸æ˜¯å­ç©ºé—´ï¼šå®ƒåœ¨æ ‡é‡ä¹˜æ³•ä¸‹ä¸å°é—­ï¼ˆè´Ÿæ ‡é‡ä¼šç ´åè¯¥æ¡ä»¶ï¼‰ã€‚
*   Any line in $\mathbb{R}^2$ that does not pass through the origin is not a subspace, because it does not contain $\mathbf{0}$.
    $\mathbb{R}^2$ ä¸­ä»»ä½•ä¸ç»è¿‡åŸç‚¹çš„çº¿éƒ½ä¸æ˜¯å­ç©ºé—´ï¼Œå› ä¸ºå®ƒä¸åŒ…å« $\mathbf{0}$ ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

Subspaces are the linear structures inside vector spaces.
å­ç©ºé—´æ˜¯å‘é‡ç©ºé—´å†…çš„çº¿æ€§ç»“æ„ã€‚

*   In $\mathbb{R}^2$, the subspaces are: the zero vector, any line through the origin, or the entire plane.
    åœ¨ $\mathbb{R}^2$ ä¸­ï¼Œå­ç©ºé—´æ˜¯ï¼šé›¶å‘é‡ã€è¿‡åŸç‚¹çš„ä»»æ„ç›´çº¿æˆ–æ•´ä¸ªå¹³é¢ã€‚
*   In $\mathbb{R}^3$, the subspaces are: the zero vector, any line through the origin, any plane through the origin, or the entire space.
    åœ¨ $\mathbb{R}^3$ ä¸­ï¼Œå­ç©ºé—´æ˜¯ï¼šé›¶å‘é‡ã€è¿‡åŸç‚¹çš„ä»»æ„ç›´çº¿ã€è¿‡åŸç‚¹çš„ä»»æ„å¹³é¢æˆ–æ•´ä¸ªç©ºé—´ã€‚
*   In higher dimensions, the same principle applies: subspaces are the flat linear pieces through the origin.
    åœ¨æ›´é«˜çš„ç»´åº¦ä¸­ï¼ŒåŒæ ·çš„åŸç†é€‚ç”¨ï¼šå­ç©ºé—´æ˜¯é€šè¿‡åŸç‚¹çš„å¹³å¦çº¿æ€§éƒ¨åˆ†ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Subspaces capture the essential structure of linear problems. Column spaces, row spaces, and null spaces are all subspaces. Much of linear algebra consists of understanding how these subspaces intersect, span, and complement each other.
å­ç©ºé—´æ•æ‰äº†çº¿æ€§é—®é¢˜çš„æœ¬è´¨ç»“æ„ã€‚åˆ—ç©ºé—´ã€è¡Œç©ºé—´å’Œé›¶ç©ºé—´éƒ½æ˜¯å­ç©ºé—´ã€‚çº¿æ€§ä»£æ•°çš„å¤§éƒ¨åˆ†å†…å®¹éƒ½åœ¨äºç†è§£è¿™äº›å­ç©ºé—´å¦‚ä½•ç›¸äº’äº¤å‰ã€å»¶ä¼¸å’Œäº’è¡¥ã€‚

### Exercises 4.2
ç»ƒä¹  4.2

1.  Prove that the set $W = { (x,0) \mid x \in \mathbb{R} } \subseteq \mathbb{R}^2$ is a subspace.
    è¯æ˜é›†åˆ $W = { (x,0) \mid x \in \mathbb{R} } \subseteq \mathbb{R}^2$ æ˜¯ä¸€ä¸ªå­ç©ºé—´ã€‚
2.  Show that the line ${ (1+t, 2t) \mid t \in \mathbb{R} }$ is not a subspace of $\mathbb{R}^2$. Which condition fails?
    è¯æ˜è¡Œ ${ (1+t, 2t) \mid t \in \mathbb{R} }$ ä¸æ˜¯ $\mathbb{R}^2$ çš„å­ç©ºé—´ã€‚å“ªä¸ªæ¡ä»¶ä¸æˆç«‹ï¼Ÿ
3.  Determine whether the set of all vectors $(x,y,z) \in \mathbb{R}^3$ satisfying $x+y+z=0$ is a subspace.
    ç¡®å®šæ»¡è¶³ $x+y+z=0$ çš„æ‰€æœ‰å‘é‡ $(x,y,z) \in \mathbb{R}^3$ çš„é›†åˆæ˜¯å¦ä¸ºå­ç©ºé—´ã€‚
4.  For the matrix
    å¯¹äºçŸ©é˜µ

$$
A = \begin{bmatrix}1 & 2 & 3 \\4 & 5 & 6\end{bmatrix}
$$

Describe the null space of $A$ as a subspace of $\mathbb{R}^3$.
å°† $A$ çš„é›¶ç©ºé—´æè¿°ä¸º $\mathbb{R}^3$ çš„å­ç©ºé—´ã€‚

5.  List all possible subspaces of $\mathbb{R}^2$.
    åˆ—å‡º $\mathbb{R}^2$ æ‰€æœ‰å¯èƒ½çš„å­ç©ºé—´ã€‚

## 4.3 Span, Basis, Dimension
4.3 è·¨åº¦ã€åŸºã€ç»´åº¦

The ideas of span, basis, and dimension provide the language for describing the size and structure of subspaces. Together, they tell us how a vector space is generated, how many building blocks it requires, and how those blocks can be chosen.
è·¨åº¦ã€åŸºå’Œç»´æ•°çš„æ¦‚å¿µæä¾›äº†æè¿°å­ç©ºé—´å¤§å°å’Œç»“æ„çš„è¯­è¨€ã€‚å®ƒä»¬å…±åŒå‘Šè¯‰æˆ‘ä»¬å‘é‡ç©ºé—´æ˜¯å¦‚ä½•ç”Ÿæˆçš„ï¼Œå®ƒéœ€è¦å¤šå°‘ä¸ªæ„å»ºå—ï¼Œä»¥åŠå¦‚ä½•é€‰æ‹©è¿™äº›æ„å»ºå—ã€‚

### Span
è·¨åº¦

Given a set of vectors ${\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k} \subseteq V$, the span is the collection of all linear combinations:
ç»™å®šä¸€ç»„å‘é‡ ${\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k} \subseteq V$ ï¼Œè·¨åº¦æ˜¯æ‰€æœ‰çº¿æ€§ç»„åˆçš„é›†åˆï¼š

$$
\text{span}\{\mathbf{v}_1, \dots, \mathbf{v}_k\} = \{ c_1\mathbf{v}_1 + \cdots + c_k\mathbf{v}_k \mid c_i \in \mathbb{R} \}.
$$

The span is always a subspace of $V$, namely the smallest subspace containing those vectors.
è·¨åº¦å§‹ç»ˆæ˜¯ $V$ çš„å­ç©ºé—´ï¼Œå³åŒ…å«è¿™äº›å‘é‡çš„æœ€å°å­ç©ºé—´ã€‚

Example 4.3.1. In $\mathbb{R}^2$, $ \text{span}{(1,0)} = \{(x,0) \mid x \in \mathbb{R}\},$ the x-axis. Similarly, $\text{span}\{(1,0),(0,1)\} = \mathbb{R}^2.$
ä¾‹ 4.3.1ã€‚ åœ¨ $\mathbb{R}^2$ ä¸­ï¼Œ $ \text{span}{(1,0)} = \{(x,0) \mid x \in \mathbb{R}\},$ x è½´ã€‚åŒæ ·ï¼Œ $\text{span}\{(1,0),(0,1)\} = \mathbb{R}^2.$

### Basis
åŸºç¡€

A basis of a vector space $V$ is a set of vectors that:
å‘é‡ç©ºé—´ $V$ çš„åŸºæ˜¯ä¸€ç»„å‘é‡ï¼Œå…¶ï¼š

1.  Span $V$.
    è·¨åº¦ $V$ ã€‚
2.  Are linearly independent (no vector in the set is a linear combination of the others).
    æ˜¯çº¿æ€§ç‹¬ç«‹çš„ï¼ˆé›†åˆä¸­æ²¡æœ‰å‘é‡æ˜¯å…¶ä»–å‘é‡çš„çº¿æ€§ç»„åˆï¼‰ã€‚

If either condition fails, the set is not a basis.
å¦‚æœä»»ä¸€æ¡ä»¶ä¸æˆç«‹ï¼Œåˆ™è¯¥é›†åˆä¸ä½œä¸ºåŸºç¡€ã€‚

Example 4.3.2. In $\mathbb{R}^3$, the standard unit vectors
ä¾‹ 4.3.2ã€‚ åœ¨ $\mathbb{R}^3$ ä¸­ï¼Œæ ‡å‡†å•ä½å‘é‡

$$
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3 = (0,0,1)
$$

form a basis. Every vector $(x,y,z)$ can be uniquely written as
æ„æˆåŸºç¡€ã€‚æ¯ä¸ªå‘é‡ $(x,y,z)$ éƒ½å¯ä»¥å”¯ä¸€åœ°å†™æˆ

$$
x\mathbf{e}_1 + y\mathbf{e}_2 + z\mathbf{e}_3.
$$

### Dimension
æ–¹é¢

The dimension of a vector space $V$, written $\dim(V)$, is the number of vectors in any basis of $V$. This number is well-defined: all bases of a vector space have the same cardinality.
å‘é‡ç©ºé—´ $V$ çš„ç»´æ•°ï¼Œè®°ä½œ $\dim(V)$ ï¼Œæ˜¯ä»»æ„ $V$ çš„åŸºä¸­å‘é‡çš„æ•°é‡ã€‚è¿™ä¸ªç»´æ•°å®šä¹‰æ˜ç¡®ï¼šå‘é‡ç©ºé—´çš„æ‰€æœ‰åŸºéƒ½å…·æœ‰ç›¸åŒçš„åŸºæ•°ã€‚

Examples 4.3.3.
ç¤ºä¾‹ 4.3.3ã€‚

*   $\dim(\mathbb{R}^2) = 2$, with basis $(1,0), (0,1)$.
    $\dim(\mathbb{R}^2) = 2$ ï¼Œä¾æ®æ˜¯ $(1,0), (0,1)$ ã€‚
*   $\dim(\mathbb{R}^3) = 3$, with basis $(1,0,0), (0,1,0), (0,0,1)$.
    $\dim(\mathbb{R}^3) = 3$ ï¼Œä¾æ®æ˜¯ $(1,0,0), (0,1,0), (0,0,1)$ ã€‚
*   The set of polynomials of degree at most 3 has dimension 4, with basis $(1, x, x^2, x^3)$.
    æ¬¡æ•°æœ€å¤šä¸º 3 çš„å¤šé¡¹å¼é›†çš„ç»´åº¦ä¸º 4ï¼ŒåŸºä¸º $(1, x, x^2, x^3)$ ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

*   The span is like the reach of a set of vectors.
    è·¨åº¦å°±åƒä¸€ç»„å‘é‡çš„èŒƒå›´ã€‚
*   A basis is the minimal set of directions needed to reach everything in the space.
    åŸºç¡€æ˜¯åˆ°è¾¾ç©ºé—´ä¸­æ‰€æœ‰äº‹ç‰©æ‰€éœ€çš„æœ€å°æ–¹å‘é›†ã€‚
*   The dimension is the count of those independent directions.
    ç»´åº¦æ˜¯è¿™äº›ç‹¬ç«‹æ–¹å‘çš„æ•°é‡ã€‚

Lines, planes, and higher-dimensional flats can all be described in terms of span, basis, and dimension.
çº¿ã€å¹³é¢å’Œé«˜ç»´å¹³é¢éƒ½å¯ä»¥ç”¨è·¨åº¦ã€åŸºå’Œç»´åº¦æ¥æè¿°ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

These concepts classify vector spaces and subspaces in terms of size and structure. Many theorems in linear algebra-such as the Rankâ€“Nullity Theorem-are consequences of understanding span, basis, and dimension. In practical terms, bases are how we encode data in coordinates, and dimension tells us how much freedom a system truly has.
è¿™äº›æ¦‚å¿µæ ¹æ®å¤§å°å’Œç»“æ„å¯¹å‘é‡ç©ºé—´å’Œå­ç©ºé—´è¿›è¡Œåˆ†ç±»ã€‚çº¿æ€§ä»£æ•°ä¸­çš„è®¸å¤šå®šç†ï¼Œä¾‹å¦‚ç§©é›¶å®šç†ï¼Œéƒ½æ˜¯ç†è§£è·¨åº¦ã€åŸºå’Œç»´æ•°çš„ç»“æœã€‚å®é™…ä¸Šï¼ŒåŸºæ˜¯æˆ‘ä»¬åœ¨åæ ‡ç³»ä¸­ç¼–ç æ•°æ®çš„æ–¹å¼ï¼Œè€Œç»´æ•°åˆ™å‘Šè¯‰æˆ‘ä»¬ä¸€ä¸ªç³»ç»ŸçœŸæ­£æ‹¥æœ‰å¤šå°‘è‡ªç”±åº¦ã€‚

### Exercises 4.3
ç»ƒä¹  4.3

1.  Show that $(1,0,0)$, $(0,1,0)$, $(1,1,0)$ span the $xy$\-plane in $\mathbb{R}^3$. Are they a basis?
    è¯æ˜ $(1,0,0)$ , $(0,1,0)$ , $(1,1,0)$ åœ¨ $\mathbb{R}^3$ ä¸­è·¨è¶Š $xy$ -å¹³é¢ã€‚å®ƒä»¬æ˜¯åŸºå—ï¼Ÿ
2.  Find a basis for the line $\{(2t,-3t,t) : t \in \mathbb{R}\}$ in $\mathbb{R}^3$.
    æ‰¾å‡º $\mathbb{R}^3$ ä¸­ç¬¬ $\{(2t,-3t,t) : t \in \mathbb{R}\}$ è¡Œçš„ä¾æ®ã€‚
3.  Determine the dimension of the subspace of $\mathbb{R}^3$ defined by $x+y+z=0$.
    ç¡®å®šç”± $x+y+z=0$ å®šä¹‰çš„ $\mathbb{R}^3$ å­ç©ºé—´çš„ç»´æ•°ã€‚
4.  Prove that any two different bases of $\mathbb{R}^n$ must contain exactly $n$ vectors.
    è¯æ˜ $\mathbb{R}^n$ çš„ä»»æ„ä¸¤ä¸ªä¸åŒåŸºå¿…å®šåŒ…å«æ°å¥½ $n$ ä¸ªå‘é‡ã€‚
5.  Give a basis for the set of polynomials of degree $\leq 2$. What is its dimension?
    ç»™å‡ºæ¬¡æ•°ä¸º $\leq 2$ çš„å¤šé¡¹å¼é›†çš„åŸºã€‚å®ƒçš„ç»´æ•°æ˜¯å¤šå°‘ï¼Ÿ

## 4.4 Coordinates
4.4 åæ ‡

Once a basis for a vector space is chosen, every vector can be expressed uniquely as a linear combination of the basis vectors. The coefficients in this combination are called the coordinates of the vector relative to that basis. Coordinates allow us to move between the abstract world of vector spaces and the concrete world of numbers.
ä¸€æ—¦é€‰å®šäº†å‘é‡ç©ºé—´çš„åŸºï¼Œæ¯ä¸ªå‘é‡éƒ½å¯ä»¥å”¯ä¸€åœ°è¡¨ç¤ºä¸ºåŸºå‘é‡çš„çº¿æ€§ç»„åˆã€‚è¯¥ç»„åˆä¸­çš„ç³»æ•°ç§°ä¸ºå‘é‡ç›¸å¯¹äºè¯¥åŸºçš„åæ ‡ã€‚åæ ‡ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å‘é‡ç©ºé—´çš„æŠ½è±¡ä¸–ç•Œå’Œå…·ä½“çš„æ•°å­—ä¸–ç•Œä¹‹é—´ç§»åŠ¨ã€‚

### Coordinates Relative to a Basis
ç›¸å¯¹äºåŸºåæ ‡

Let $V$ be a vector space, and let
ä»¤ $V$ ä¸ºå‘é‡ç©ºé—´ï¼Œ

$$
\mathcal{B} = \{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\}
$$

be an ordered basis for $V$. Every vector $\mathbf{u} \in V$ can be written uniquely as
æ˜¯ $V$ çš„æœ‰åºåŸºã€‚æ¯ä¸ªå‘é‡ $\mathbf{u} \in V$ éƒ½å¯ä»¥å”¯ä¸€åœ°å†™æˆ

$$
\mathbf{u} = c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_n \mathbf{v}_n.
$$

The scalars $(c_1, c_2, \dots, c_n)$ are the coordinates of $\mathbf{u}$ relative to $\mathcal{B}$, written
æ ‡é‡ $(c_1, c_2, \dots, c_n)$ æ˜¯ $\mathbf{u}$ ç›¸å¯¹äº $\mathcal{B}$ çš„åæ ‡ï¼Œå†™ä¸º

$$
[\mathbf{u}]_{\mathcal{B}} = \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{bmatrix}.
$$

### Example in $\mathbb{R}^2$
$\mathbb{R}^2$ ä¸­çš„ç¤ºä¾‹

Example 4.4.1. Let the basis be
ä¾‹ 4.4.1. è®¾åŸºç¡€ä¸º

$$
\mathcal{B} = \{ (1,1), (1,-1) \}.
$$

To find the coordinates of $\mathbf{u} = (3,1)$ relative to $\mathcal{B}$, solve
è¦æŸ¥æ‰¾ $\mathbf{u} = (3,1)$ ç›¸å¯¹äº $\mathcal{B}$ çš„åæ ‡ï¼Œè¯·æ±‚è§£

$$
(3,1) = c_1(1,1) + c_2(1,-1).
$$

This gives the system
è¿™ä½¿å¾—ç³»ç»Ÿ

$$
\begin{cases}c_1 + c_2 = 3, \\c_1 - c_2 = 1.\end{cases}
$$

Adding: $2c\_1 = 4 \\implies c\_1 = 2$. Then $c\_2 = 1$.
æ·»åŠ ï¼š$2c\_1 = 4 \\implies c\_1 = 2 $. Then $ c\_2 = 1$ã€‚

So,
æ‰€ä»¥ï¼Œ

$$
[\mathbf{u}]_{\mathcal{B}} = \begin{bmatrix} 2 \\ 1 \end{bmatrix}.
$$

### Standard Coordinates
æ ‡å‡†åæ ‡

In $\mathbb{R}^n$, the standard basis is
åœ¨ $\mathbb{R}^n$ ä¸­ï¼Œæ ‡å‡†ä¾æ®æ˜¯

$$
\mathbf{e}_1 = (1,0,\dots,0), \quad \mathbf{e}_2 = (0,1,0,\dots,0), \dots, \mathbf{e}_n = (0,\dots,0,1).
$$

Relative to this basis, the coordinates of a vector are simply its entries. Thus, column vectors are coordinate representations by default.
ç›¸å¯¹äºæ­¤åŸºï¼Œå‘é‡çš„åæ ‡ä»…ä»…æ˜¯å®ƒçš„å…ƒç´ ã€‚å› æ­¤ï¼Œåˆ—å‘é‡é»˜è®¤ä¸ºåæ ‡è¡¨ç¤ºã€‚

### Change of Basis
åŸºç¡€å˜æ›´

If $\mathcal{B} = {\mathbf{v}_1, \dots, \mathbf{v}_n}$ is a basis of $\mathbb{R}^n$, the change of basis matrix is
å¦‚æœğµ = ğ‘£ 1 , â€¦ , ğ‘£ ğ‘› B=v 1 â€‹ ï¼Œâ€¦ï¼Œv n â€‹ æ˜¯ $\mathbb{R}^n$ çš„åŸºï¼ŒåŸºçŸ©é˜µçš„å˜åŒ–æ˜¯

$$
P = \begin{bmatrix} \mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n \end{bmatrix},
$$

with basis vectors as columns. For any vector $\mathbf{u}$,
ä»¥åŸºå‘é‡ä¸ºåˆ—ã€‚å¯¹äºä»»æ„å‘é‡ $\mathbf{u}$ ï¼Œ

$$
\mathbf{u} = P [\mathbf{u}]_{\mathcal{B}}, \qquad [\mathbf{u}]_{\mathcal{B}} = P^{-1}\mathbf{u}.
$$

Thus, switching between bases reduces to matrix multiplication.
å› æ­¤ï¼ŒåŸºæ•°ä¹‹é—´çš„åˆ‡æ¢å°±ç®€åŒ–ä¸ºçŸ©é˜µä¹˜æ³•ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

Coordinates are the address of a vector relative to a chosen set of directions. Different bases are like different coordinate systems: Cartesian, rotated, skewed, or scaled. The same vector may look very different numerically depending on the basis, but its geometric identity is unchanged.
åæ ‡æ˜¯å‘é‡ç›¸å¯¹äºä¸€ç»„é€‰å®šæ–¹å‘çš„åœ°å€ã€‚ä¸åŒçš„åŸºå°±åƒä¸åŒçš„åæ ‡ç³»ï¼šç¬›å¡å°”åæ ‡ç³»ã€æ—‹è½¬åæ ‡ç³»ã€å€¾æ–œåæ ‡ç³»æˆ–ç¼©æ”¾åæ ‡ç³»ã€‚åŒä¸€ä¸ªå‘é‡åœ¨ä¸åŒåŸºä¸Šå¯èƒ½å‘ˆç°å‡ºæˆªç„¶ä¸åŒçš„æ•°å€¼ï¼Œä½†å…¶å‡ ä½•æ’ç­‰å¼ä¿æŒä¸å˜ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Coordinates turn abstract vectors into concrete numerical data. Changing basis is the algebraic language for rotations of axes, diagonalization of matrices, and principal component analysis in data science. Mastery of coordinates is essential for moving fluidly between geometry, algebra, and computation.
åæ ‡å°†æŠ½è±¡å‘é‡è½¬åŒ–ä¸ºå…·ä½“çš„æ•°å€¼æ•°æ®ã€‚å˜æ¢åŸºæ˜¯æ•°æ®ç§‘å­¦ä¸­è½´æ—‹è½¬ã€çŸ©é˜µå¯¹è§’åŒ–å’Œä¸»æˆåˆ†åˆ†æçš„ä»£æ•°è¯­è¨€ã€‚æŒæ¡åæ ‡ç³»å¯¹äºåœ¨å‡ ä½•ã€ä»£æ•°å’Œè®¡ç®—ä¹‹é—´æµç•…åˆ‡æ¢è‡³å…³é‡è¦ã€‚

### Exercises 4.4
ç»ƒä¹  4.4

1.  Express $(4,2)$ in terms of the basis $(1,1), (1,-1)$.
    æ ¹æ®åŸºç¡€ $(1,1), (1,-1)$ è¡¨è¾¾ $(4,2)$ ã€‚
2.  Find the coordinates of $(1,2,3)$ relative to the standard basis of $\mathbb{R}^3$.
    æ‰¾å‡º $(1,2,3)$ ç›¸å¯¹äº $\mathbb{R}^3$ æ ‡å‡†åŸºçš„åæ ‡ã€‚
3.  If $\mathcal{B} = \{(2,0), (0,3)\}$, compute $[ (4,6) ]_{\mathcal{B}}$.
    å¦‚æœ $\mathcal{B} = \{(2,0), (0,3)\}$ ï¼Œåˆ™è®¡ç®— \[ ( 4 , 6 ) \] ğµ \[(4,6)\] B â€‹ .
4.  Construct the change of basis matrix from the standard basis of $\mathbb{R}^2$ to $\mathcal{B} = \{(1,1), (1,-1)\}$.
    æ„å»ºä»æ ‡å‡†åŸº $\mathbb{R}^2$ åˆ° $\mathcal{B} = \{(1,1), (1,-1)\}$ çš„åŸºå˜æ¢çŸ©é˜µã€‚
5.  Prove that coordinate representation with respect to a basis is unique.
    è¯æ˜å…³äºåŸºçš„åæ ‡è¡¨ç¤ºæ˜¯å”¯ä¸€çš„ã€‚

# Chapter 5. Linear Transformations
ç¬¬äº”ç« çº¿æ€§å˜æ¢

## 5.1 Functions that Preserve Linearity
5.1 ä¿æŒçº¿æ€§çš„å‡½æ•°

A central theme of linear algebra is understanding linear transformations: functions between vector spaces that preserve their algebraic structure. These transformations generalize the idea of matrix multiplication and capture the essence of linear behavior.
çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒä¸»é¢˜æ˜¯ç†è§£çº¿æ€§å˜æ¢ï¼šå‘é‡ç©ºé—´ä¹‹é—´ä¿æŒå…¶ä»£æ•°ç»“æ„çš„å‡½æ•°ã€‚è¿™äº›å˜æ¢æ¨å¹¿äº†çŸ©é˜µä¹˜æ³•çš„æ¦‚å¿µï¼Œå¹¶æŠ“ä½äº†çº¿æ€§è¡Œä¸ºçš„æœ¬è´¨ã€‚

### Definition
å®šä¹‰

Let $V$ and $W$ be vector spaces over $\mathbb{R}$. A function
ä»¤ $V$ å’Œ $W$ ä¸º $\mathbb{R}$ ä¸Šçš„å‘é‡ç©ºé—´ã€‚å‡½æ•°

$$
T : V \to W
$$

is called a linear transformation (or linear map) if for all vectors $\mathbf{u}, \mathbf{v} \in V$ and all scalars $c \in \mathbb{R}$:
å¦‚æœå¯¹äºæ‰€æœ‰å‘é‡ $\mathbf{u}, \mathbf{v} \in V$ å’Œæ‰€æœ‰æ ‡é‡ $c \in \mathbb{R}$ ï¼Œåˆ™ç§°ä¸ºçº¿æ€§å˜æ¢ï¼ˆæˆ–çº¿æ€§æ˜ å°„ï¼‰ï¼š

1.  Additivity:
    åŠ æ€§ï¼š

$$
T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v}),
$$

2.  Homogeneity:
    åŒè´¨æ€§ï¼š

$$
T(c\mathbf{u}) = cT(\mathbf{u}).
$$

If both conditions hold, then $T$ automatically respects linear combinations:
å¦‚æœä¸¤ä¸ªæ¡ä»¶éƒ½æˆç«‹ï¼Œåˆ™ $T$ è‡ªåŠ¨éµå¾ªçº¿æ€§ç»„åˆï¼š

$$
T(c_1\mathbf{v}_1 + \cdots + c_k\mathbf{v}_k) = c_1 T(\mathbf{v}_1) + \cdots + c_k T(\mathbf{v}_k).
$$

### Examples
ç¤ºä¾‹

Example 5.1.1. Scaling in $\mathbb{R}^2$. Let $T:\mathbb{R}^2 \to \mathbb{R}^2$ be defined by
ä¾‹ 5.1.1. ç¼©æ”¾ $\mathbb{R}^2$ ã€‚ä»¤ $T:\mathbb{R}^2 \to \mathbb{R}^2$ å®šä¹‰ä¸º

$$
T(x,y) = (2x, 2y).
$$

This doubles the length of every vector, preserving direction. It is linear.
è¿™ä¼šä½¿æ¯ä¸ªå‘é‡çš„é•¿åº¦åŠ å€ï¼ŒåŒæ—¶ä¿æŒæ–¹å‘ä¸å˜ã€‚å®ƒæ˜¯çº¿æ€§çš„ã€‚

Example 5.1.2. Rotation.
ä¾‹ 5.1.2. æ—‹è½¬ã€‚

Let $R_\theta: \mathbb{R}^2 \to \mathbb{R}^2$ be
ä»¤ $R_\theta: \mathbb{R}^2 \to \mathbb{R}^2$ ä¸º

$$
R_\theta(x,y) = (x\cos\theta - y\sin\theta, \; x\sin\theta + y\cos\theta).
$$

This rotates vectors by angle $\theta$. It satisfies additivity and homogeneity, hence is linear.
è¿™å°†å‘é‡æ—‹è½¬è§’åº¦ $\theta$ ã€‚å®ƒæ»¡è¶³å¯åŠ æ€§å’Œé½æ¬¡æ€§ï¼Œå› æ­¤æ˜¯çº¿æ€§çš„ã€‚

Example 5.1.3. Differentiation.
ä¾‹ 5.1.3. åŒºåˆ†ã€‚

Let $D: \mathbb{R}[x] \to \mathbb{R}[x]$ be differentiation: $D(p(x)) = p'(x)$.
ä»¤ $D: \mathbb{R}[x] \to \mathbb{R}[x]$ ä¸ºå¾®åˆ†ï¼š $D(p(x)) = p'(x)$ ã€‚

Since derivatives respect addition and scalar multiples, differentiation is a linear transformation.
ç”±äºå¯¼æ•°å°Šé‡åŠ æ³•å’Œæ ‡é‡å€æ•°ï¼Œå› æ­¤å¾®åˆ†æ˜¯ä¸€ç§çº¿æ€§å˜æ¢ã€‚

### Non-Example
éç¤ºä¾‹

The map $S:\mathbb{R}^2 \to \mathbb{R}^2$ defined by
åœ°å›¾ $S:\mathbb{R}^2 \to \mathbb{R}^2$ å®šä¹‰ä¸º

$$
S(x,y) = (x^2, y^2)
$$

is not linear, because $S(\mathbf{u} + \mathbf{v}) \neq S(\mathbf{u}) + S(\mathbf{v})$ in general.
ä¸æ˜¯çº¿æ€§çš„ï¼Œå› ä¸ºä¸€èˆ¬æ¥è¯´ $S(\mathbf{u} + \mathbf{v}) \neq S(\mathbf{u}) + S(\mathbf{v})$ ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

Linear transformations are exactly those that preserve the origin, lines through the origin, and proportions along those lines. They include familiar operations: scaling, rotations, reflections, shears, and projections. Nonlinear transformations bend or curve space, breaking these properties.
çº¿æ€§å˜æ¢æ­£æ˜¯é‚£äº›ä¿ç•™åŸç‚¹ã€è¿‡åŸç‚¹çš„ç›´çº¿ä»¥åŠæ²¿è¿™äº›ç›´çº¿çš„æ¯”ä¾‹çš„å˜æ¢ã€‚å®ƒä»¬åŒ…æ‹¬æˆ‘ä»¬ç†Ÿæ‚‰çš„æ“ä½œï¼šç¼©æ”¾ã€æ—‹è½¬ã€åå°„ã€å‰ªåˆ‡å’ŒæŠ•å½±ã€‚éçº¿æ€§å˜æ¢ä¼šå¼¯æ›²ç©ºé—´ï¼Œä»è€Œç ´åè¿™äº›å±æ€§ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Linear transformations unify geometry, algebra, and computation. They explain how matrices act on vectors, how data can be rotated or projected, and how systems evolve under linear rules. Much of linear algebra is devoted to understanding these transformations, their representations, and their invariants.
çº¿æ€§å˜æ¢ç»Ÿä¸€äº†å‡ ä½•ã€ä»£æ•°å’Œè®¡ç®—ã€‚å®ƒè§£é‡Šäº†çŸ©é˜µå¦‚ä½•ä½œç”¨äºå‘é‡ï¼Œæ•°æ®å¦‚ä½•æ—‹è½¬æˆ–æŠ•å½±ï¼Œä»¥åŠç³»ç»Ÿå¦‚ä½•åœ¨çº¿æ€§è§„åˆ™ä¸‹æ¼”åŒ–ã€‚çº¿æ€§ä»£æ•°çš„å¤§éƒ¨åˆ†å†…å®¹è‡´åŠ›äºç†è§£è¿™äº›å˜æ¢ã€å®ƒä»¬çš„è¡¨ç¤ºåŠå…¶ä¸å˜é‡ã€‚

### Exercises 5.1
ç»ƒä¹  5.1

1.  Verify that $T(x,y) = (3x-y, 2y)$ is a linear transformation on $\mathbb{R}^2$.
    éªŒè¯ $T(x,y) = (3x-y, 2y)$ æ˜¯å¦æ˜¯ $\mathbb{R}^2$ çš„çº¿æ€§å˜æ¢ã€‚
2.  Show that $T(x,y) = (x+1, y)$ is not linear. Which axiom fails?
    è¯æ˜ $T(x,y) = (x+1, y)$ ä¸æ˜¯çº¿æ€§çš„ã€‚å“ªæ¡å…¬ç†ä¸æˆç«‹ï¼Ÿ
3.  Prove that if $T$ and $S$ are linear transformations, then so is $T+S$.
    è¯æ˜å¦‚æœ $T$ å’Œ $S$ æ˜¯çº¿æ€§å˜æ¢ï¼Œé‚£ä¹ˆ $T+S$ ä¹Ÿæ˜¯çº¿æ€§å˜æ¢ã€‚
4.  Give an example of a linear transformation from $\mathbb{R}^3$ to $\mathbb{R}^2$.
    ç»™å‡ºä¸€ä¸ªä» $\mathbb{R}^3$ åˆ° $\mathbb{R}^2$ çš„çº¿æ€§å˜æ¢çš„ä¾‹å­ã€‚
5.  Let $T:\mathbb{R}[x] \to \mathbb{R}[x]$ be integration:
    ä»¤ $T:\mathbb{R}[x] \to \mathbb{R}[x]$ ä¸ºç§¯åˆ†ï¼š

$$
T(p(x)) = \int_0^x p(t)\\,dt.
$$

Prove that $T$ is a linear transformation.
è¯æ˜ $T$ æ˜¯çº¿æ€§å˜æ¢ã€‚

## 5.2 Matrix Representation of Linear Maps
5.2 çº¿æ€§æ˜ å°„çš„çŸ©é˜µè¡¨ç¤º

Every linear transformation between finite-dimensional vector spaces can be represented by a matrix. This correspondence is one of the central insights of linear algebra: it lets us use the tools of matrix arithmetic to study abstract transformations.
æœ‰é™ç»´å‘é‡ç©ºé—´ä¹‹é—´çš„æ‰€æœ‰çº¿æ€§å˜æ¢éƒ½å¯ä»¥ç”¨çŸ©é˜µè¡¨ç¤ºã€‚è¿™ç§å¯¹åº”å…³ç³»æ˜¯çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒæ´è§ä¹‹ä¸€ï¼šå®ƒè®©æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨çŸ©é˜µè¿ç®—å·¥å…·æ¥ç ”ç©¶æŠ½è±¡çš„å˜æ¢ã€‚

### From Linear Map to Matrix
ä»çº¿æ€§æ˜ å°„åˆ°çŸ©é˜µ

Let $T: \mathbb{R}^n \to \mathbb{R}^m$ be a linear transformation. Choose the standard basis $\{ \mathbf{e}_1, \dots, \mathbf{e}_n \}$ of $\mathbb{R}^n$, where $\mathbf{e}_i$ has a 1 in the $i$\-th position and 0 elsewhere.
ä»¤ $T: \mathbb{R}^n \to \mathbb{R}^m$ ä¸ºçº¿æ€§å˜æ¢ã€‚é€‰å– $\mathbb{R}^n$ çš„æ ‡å‡†åŸº $\{ \mathbf{e}_1, \dots, \mathbf{e}_n \}$ ï¼Œå…¶ä¸­ ğ‘’ ğ‘– e i â€‹ ç¬¬ $i$ ä¸ªä½ç½®ä¸º 1ï¼Œå…¶ä»–åœ°æ–¹ä¸º 0ã€‚

The action of $T$ on each basis vector determines the entire transformation:
$T$ å¯¹æ¯ä¸ªåŸºå‘é‡çš„ä½œç”¨å†³å®šäº†æ•´ä¸ªå˜æ¢ï¼š

$$
T(\mathbf{e}\_j) = \begin{bmatrix}a_{1j} \\a_{2j} \\\vdots \\a_{mj} \end{bmatrix}.
$$

Placing these outputs as columns gives the matrix of $T$:
å°†è¿™äº›è¾“å‡ºä½œä¸ºåˆ—æ”¾ç½®ï¼Œå¾—åˆ°çŸ©é˜µ $T$ ï¼š

$$
[T] = A = \begin{bmatrix}a_{11} & a_{12} & \cdots & a_{1n} \\a_{21} & a_{22} & \cdots & a_{2n} \\\vdots & \vdots & \ddots & \vdots \\a_{m1} & a_{m2} & \cdots & a_{mn}\end{bmatrix}.
$$

Then for any vector $\mathbf{x} \in \mathbb{R}^n$:
ç„¶åå¯¹äºä»»æ„å‘é‡ $\mathbf{x} \in \mathbb{R}^n$ ï¼š

$$
T(\mathbf{x}) = A\mathbf{x}.
$$

### Examples
ç¤ºä¾‹

Example 5.2.1. Scaling in $\mathbb{R}^2$. Let $T(x,y) = (2x, 3y)$. Then
ä¾‹ 5.2.1. ç¼©æ”¾ $\mathbb{R}^2$ ã€‚è®¾ $T(x,y) = (2x, 3y)$ ã€‚ç„¶å

$$
T(\mathbf{e}_1) = (2,0), \quad T(\mathbf{e}_2) = (0,3).
$$

So the matrix is
æ‰€ä»¥çŸ©é˜µæ˜¯

$$
[T] = \begin{bmatrix}2 & 0 \\0 & 3\end{bmatrix}.
$$

Example 5.2.2. Rotation in the plane. The rotation transformation $R_\theta(x,y) = (x\cos\theta - y\sin\theta, \; x\sin\theta + y\cos\theta)$ has matrix
ä¾‹5.2.2. å¹³é¢æ—‹è½¬ã€‚ æ—‹è½¬å˜æ¢ $R_\theta(x,y) = (x\cos\theta - y\sin\theta, \; x\sin\theta + y\cos\theta)$ å…·æœ‰çŸ©é˜µ

$$
[R_\theta] = \begin{bmatrix}\cos\theta & -\sin\theta \\\sin\theta & \cos\theta\end{bmatrix}.
$$

Example 5.2.3. Projection onto the x-axis. The map $P(x,y) = (x,0)$ corresponds to
ä¾‹ 5.2.3. æŠ•å½±åˆ° x è½´ã€‚ åœ°å›¾ $P(x,y) = (x,0)$ å¯¹åº”äº

$$
[P] = \begin{bmatrix}1 & 0 \\0 & 0\end{bmatrix}.
$$

### Change of Basis
åŸºç¡€å˜æ›´

Matrix representations depend on the chosen basis. If $\mathcal{B}$ and $\mathcal{C}$ are bases of $\mathbb{R}^n$ and $\mathbb{R}^m$, then the matrix of $T: \mathbb{R}^n \to \mathbb{R}^m$ with respect to these bases is obtained by expressing $T(\mathbf{v}_j)$ in terms of $\mathcal{C}$ for each $\mathbf{v}_j \in \mathcal{B}$. Changing bases corresponds to conjugating the matrix by the appropriate change-of-basis matrices.
çŸ©é˜µè¡¨ç¤ºå–å†³äºæ‰€é€‰çš„åŸºã€‚å¦‚æœ $\mathcal{B}$ å’Œ $\mathcal{C}$ æ˜¯ $\mathbb{R}^n$ çš„åŸº å’Œ $\mathbb{R}^m$ ï¼Œåˆ™ $T: \mathbb{R}^n \to \mathbb{R}^m$ å…³äºè¿™äº›åŸºçš„çŸ©é˜µï¼Œå¯ä»¥é€šè¿‡å°† $T(\mathbf{v}_j)$ è¡¨ç¤ºä¸º $\mathcal{C}$ æ¥è·å¾—ï¼Œå…¶ä¸­ $\mathbf{v}_j \in \mathcal{B}$ è¡¨ç¤ºä¸º $T(\mathbf{v}_j)$ã€‚æ”¹å˜åŸºç›¸å½“äºå°†çŸ©é˜µä¸é€‚å½“çš„åŸºå˜æ¢çŸ©é˜µå…±è½­ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

Matrices are not just convenient notation-they *are* linear maps once a basis is fixed. Every rotation, reflection, projection, shear, or scaling corresponds to multiplying by a specific matrix. Thus, studying linear transformations reduces to studying their matrices.
çŸ©é˜µä¸ä»…ä»…æ˜¯æ–¹ä¾¿çš„ç¬¦å·â€”â€”ä¸€æ—¦åŸºç¡®å®šï¼Œå®ƒä»¬*å°±æ˜¯*çº¿æ€§æ˜ å°„ã€‚æ‰€æœ‰æ—‹è½¬ã€åå°„ã€æŠ•å½±ã€å‰ªåˆ‡æˆ–ç¼©æ”¾éƒ½å¯¹åº”äºä¹˜ä»¥ä¸€ä¸ªç‰¹å®šçš„çŸ©é˜µã€‚å› æ­¤ï¼Œç ”ç©¶çº¿æ€§å˜æ¢å¯ä»¥å½’ç»“ä¸ºç ”ç©¶å®ƒä»¬çš„çŸ©é˜µã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Matrix representations make linear transformations computable. They connect abstract definitions to explicit calculations, enabling algorithms for solving systems, finding eigenvalues, and performing decompositions. Applications from graphics to machine learning depend on this translation.
çŸ©é˜µè¡¨ç¤ºä½¿çº¿æ€§å˜æ¢å¯è®¡ç®—ã€‚å®ƒä»¬å°†æŠ½è±¡å®šä¹‰ä¸æ˜ç¡®çš„è®¡ç®—è”ç³»èµ·æ¥ï¼Œä»è€Œæ”¯æŒæ±‚è§£ç³»ç»Ÿã€æŸ¥æ‰¾ç‰¹å¾å€¼å’Œæ‰§è¡Œåˆ†è§£çš„ç®—æ³•ã€‚ä»å›¾å½¢åˆ°æœºå™¨å­¦ä¹ ç­‰å„ç§åº”ç”¨éƒ½ä¾èµ–äºè¿™ç§è½¬æ¢ã€‚

### Exercises 5.2
ç»ƒä¹  5.2

1.  Find the matrix representation of $T:\mathbb{R}^2 \to \mathbb{R}^2$, $T(x,y) = (x+y, x-y)$.
    æ‰¾åˆ° $T:\mathbb{R}^2 \to \mathbb{R}^2$ , $T(x,y) = (x+y, x-y)$ çš„çŸ©é˜µè¡¨ç¤ºã€‚
2.  Determine the matrix of the linear transformation $T:\mathbb{R}^3 \to \mathbb{R}^2$, $T(x,y,z) = (x+z, y-2z)$.
    ç¡®å®šçº¿æ€§å˜æ¢çŸ©é˜µ $T:\mathbb{R}^3 \to \mathbb{R}^2$ ï¼Œ $T(x,y,z) = (x+z, y-2z)$ ã€‚
3.  What matrix represents reflection across the line $y=x$ in $\mathbb{R}^2$?
    å“ªä¸ªçŸ©é˜µè¡¨ç¤º $\mathbb{R}^2$ ä¸­æ²¿çº¿ $y=x$ çš„åå°„ï¼Ÿ
4.  Show that the matrix of the identity transformation on $\mathbb{R}^n$ is $I_n$.
    è¯æ˜ $\mathbb{R}^n$ ä¸Šçš„æ’ç­‰å˜æ¢çŸ©é˜µæ˜¯ ğ¼ ğ‘› I n â€‹ .
5.  For the differentiation map $D:\mathbb{R}_2[x] \to \mathbb{R}_1[x]$, where $\mathbb{R}_k[x]$ is the space of polynomials of degree at most $k$, find the matrix of $D$ relative to the bases $\{1,x,x^2\}$ and $\{1,x\}$.
    å¯¹äºå¾®åˆ†æ˜ å°„ $D:\mathbb{R}_2[x] \to \mathbb{R}_1[x]$ ï¼Œå…¶ä¸­ $\mathbb{R}_k[x]$ æ˜¯æ¬¡æ•°æœ€å¤šä¸º $k$ çš„å¤šé¡¹å¼ç©ºé—´ï¼Œæ±‚å‡º $D$ ç›¸å¯¹äºåŸºæ•° $\{1,x,x^2\}$ å’Œ $\{1,x\}$ çš„çŸ©é˜µã€‚

## 5.3 Kernel and Image
5.3 å†…æ ¸å’Œé•œåƒ

To understand a linear transformation deeply, we must examine what it kills and what it produces. These ideas are captured by the kernel and the image, two fundamental subspaces associated with any linear map.
è¦æ·±å…¥ç†è§£çº¿æ€§å˜æ¢ï¼Œæˆ‘ä»¬å¿…é¡»è€ƒå¯Ÿå®ƒæ¶ˆé™¤äº†ä»€ä¹ˆï¼Œåˆäº§ç”Ÿäº†ä»€ä¹ˆã€‚è¿™äº›æ¦‚å¿µå¯ä»¥é€šè¿‡æ ¸å’Œåƒæ¥ç†è§£ï¼Œå®ƒä»¬æ˜¯ä»»ä½•çº¿æ€§æ˜ å°„éƒ½ç›¸å…³çš„ä¸¤ä¸ªåŸºæœ¬å­ç©ºé—´ã€‚

### The Kernel
å†…æ ¸

The kernel (or null space) of a linear transformation $T: V \to W$ is the set of all vectors in $V$ that map to the zero vector in $W$:
çº¿æ€§å˜æ¢ $T: V \to W$ çš„æ ¸ï¼ˆæˆ–é›¶ç©ºé—´ï¼‰æ˜¯ $V$ ä¸­æ˜ å°„åˆ° $W$ ä¸­çš„é›¶å‘é‡çš„æ‰€æœ‰å‘é‡çš„é›†åˆï¼š

$$
\ker(T) = \{ \mathbf{v} \in V \mid T(\mathbf{v}) = \mathbf{0} \}.
$$

The kernel is always a subspace of $V$. It measures the degeneracy of the transformation-directions that collapse to nothing.
æ ¸å§‹ç»ˆæ˜¯ $V$ çš„å­ç©ºé—´ã€‚å®ƒè¡¡é‡çš„æ˜¯åç¼©ä¸ºé›¶çš„å˜æ¢æ–¹å‘çš„é€€åŒ–ç¨‹åº¦ã€‚

Example 5.3.1. Let $T:\mathbb{R}^3 \to \mathbb{R}^2$ be defined by
ä¾‹ 5.3.1ã€‚ è®© $T:\mathbb{R}^3 \to \mathbb{R}^2$ å®šä¹‰ä¸º

$$
T(x,y,z) = (x+y, y+z).
$$

In matrix form,
ä»¥çŸ©é˜µå½¢å¼ï¼Œ

$$
[T] = \begin{bmatrix}1 & 1 & 0 \\0 & 1 & 1\end{bmatrix}.
$$

To find the kernel, solve
è¦æ‰¾åˆ°å†…æ ¸ï¼Œè¯·è§£å†³

$$
\begin{bmatrix}1 & 1 & 0 \\0 & 1 & 1\end{bmatrix}\begin{bmatrix} x \\ y \\ z \end{bmatrix}= \begin{bmatrix} 0 \\ 0 \end{bmatrix}.
$$

This gives the equations $x + y = 0$, $y + z = 0$. Hence $x = -y, z = -y$. The kernel is
ç”±æ­¤å¾—åˆ°æ–¹ç¨‹ $x + y = 0$ ï¼Œ $y + z = 0$ ã€‚å› æ­¤ $x = -y, z = -y$ ã€‚æ ¸å‡½æ•°ä¸º

$$
\ker(T) = \{ (-t, t, -t) \mid t \in \mathbb{R} \},
$$

a line in $\mathbb{R}^3$.
$\mathbb{R}^3$ ä¸­çš„ä¸€è¡Œã€‚

### The Image
å›¾åƒ

The image (or range) of a linear transformation $T: V \to W$ is the set of all outputs:
çº¿æ€§å˜æ¢ $T: V \to W$ çš„å›¾åƒï¼ˆæˆ–èŒƒå›´ï¼‰æ˜¯æ‰€æœ‰è¾“å‡ºçš„é›†åˆï¼š

$$
\text{im}(T) = \{ T(\mathbf{v}) \mid \mathbf{v} \in V \} \subseteq W.
$$

Equivalently, it is the span of the columns of the representing matrix. The image is always a subspace of $W$.
ç­‰æ•ˆåœ°ï¼Œå®ƒæ˜¯è¡¨ç¤ºçŸ©é˜µçš„åˆ—çš„è·¨åº¦ã€‚å›¾åƒå§‹ç»ˆæ˜¯ $W$ çš„å­ç©ºé—´ã€‚

Example 5.3.2. For the same transformation as above,
ä¾‹ 5.3.2. å¯¹äºä¸ä¸Šè¿°ç›¸åŒçš„å˜æ¢ï¼Œ

$$
[T] = \begin{bmatrix}1 & 1 & 0 \\0 & 1 & 1\end{bmatrix},
$$

the columns are $(1,0)$, $(1,1)$, and $(0,1)$. Since $(1,1) = (1,0) + (0,1)$, the image is
åˆ—ä¸º $(1,0)$ ã€ $(1,1)$ å’Œ $(0,1)$ ã€‚ç”±äº $(1,1) = (1,0) + (0,1)$ ï¼Œå› æ­¤å›¾åƒä¸º

$$
\text{im}(T) = \text{span}\{ (1,0), (0,1) \} = \mathbb{R}^2.
$$

### Dimension Formula (Rankâ€“Nullity Theorem)
ç»´åº¦å…¬å¼ï¼ˆç§©-é›¶åº¦å®šç†ï¼‰

For a linear transformation $T: V \to W$ with $V$ finite-dimensional,
å¯¹äºçº¿æ€§å˜æ¢ $T: V \to W$ ä¸” $V$ ä¸ºæœ‰é™ç»´ï¼Œ

$$
\dim(\ker(T)) + \dim(\text{im}(T)) = \dim(V).
$$

This fundamental result connects the lost directions (kernel) with the achieved directions (image).
è¿™ä¸ªåŸºæœ¬ç»“æœå°†ä¸¢å¤±çš„æ–¹å‘ï¼ˆå†…æ ¸ï¼‰ä¸å®ç°çš„æ–¹å‘ï¼ˆå›¾åƒï¼‰è”ç³»èµ·æ¥ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

*   The kernel describes how the transformation flattens space (e.g., projecting a 3D object onto a plane).
    å†…æ ¸æè¿°äº†å˜æ¢å¦‚ä½•ä½¿ç©ºé—´å˜å¹³å¦ï¼ˆä¾‹å¦‚ï¼Œå°† 3D å¯¹è±¡æŠ•å½±åˆ°å¹³é¢ä¸Šï¼‰ã€‚
*   The image describes the target subspace reached by the transformation.
    è¯¥å›¾åƒæè¿°äº†å˜æ¢æ‰€è¾¾åˆ°çš„ç›®æ ‡å­ç©ºé—´ã€‚
*   The rankâ€“nullity theorem quantifies the tradeoff: the more dimensions collapse, the fewer remain in the image.
    ç§©é›¶å®šç†é‡åŒ–äº†è¿™ç§æƒè¡¡ï¼šç»´åº¦å´©æºƒå¾—è¶Šå¤šï¼Œå›¾åƒä¸­å‰©ä½™çš„ç»´åº¦å°±è¶Šå°‘ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Kernel and image capture the essence of a linear map. They classify transformations, explain when systems have unique or infinite solutions, and form the backbone of important results like the Rankâ€“Nullity Theorem, diagonalization, and spectral theory.
æ ¸å’Œå›¾åƒæ•æ‰äº†çº¿æ€§æ˜ å°„çš„æœ¬è´¨ã€‚å®ƒä»¬å¯¹å˜æ¢è¿›è¡Œåˆ†ç±»ï¼Œè§£é‡Šç³»ç»Ÿä½•æ—¶å…·æœ‰å”¯ä¸€æˆ–æ— é™è§£ï¼Œå¹¶æ„æˆç§©é›¶å®šç†ã€å¯¹è§’åŒ–å’Œè°±ç†è®ºç­‰é‡è¦ç»“æœçš„æ”¯æŸ±ã€‚

### Exercises 5.3
ç»ƒä¹  5.3

1.  Find the kernel and image of $T:\mathbb{R}^2 \to \mathbb{R}^2$, $T(x,y) = (x-y, x+y)$.
    æŸ¥æ‰¾ $T:\mathbb{R}^2 \to \mathbb{R}^2$ ã€ $T(x,y) = (x-y, x+y)$ çš„æ ¸å’Œå›¾åƒã€‚
2.  Let
    è®©

$$
A = \begin{bmatrix} 1 & 2 & 3 \\ 0 & 1 & 4 \end{bmatrix}
$$

Find bases for $\ker(A)$ and $\text{im}(A)$. 3. For the projection map $P(x,y,z) = (x,y,0)$, describe the kernel and image. 4. Prove that $\ker(T)$ and $\text{im}(T)$ are always subspaces. 5. Verify the Rankâ€“Nullity Theorem for the transformation in Example 5.3.1.
æ‰¾åˆ° $\ker(A)$ å’Œ $\text{im}(A)$ çš„åŸºã€‚3. å¯¹äºæŠ•å½±å›¾ $P(x,y,z) = (x,y,0)$ ï¼Œæè¿°å…¶æ ¸å’Œå›¾åƒã€‚4. è¯æ˜ $\ker(T)$ å’Œ $\text{im}(T)$ å§‹ç»ˆæ˜¯å­ç©ºé—´ã€‚5. éªŒè¯ç¤ºä¾‹5.3.1ä¸­å˜æ¢çš„ç§©é›¶æ€§å®šç†ã€‚

## 5.4 Change of Basis
5.4 åŸºç¡€å˜æ›´

Linear transformations can look very different depending on the coordinate system we use. The process of rewriting vectors and transformations relative to a new basis is called a change of basis. This concept lies at the heart of diagonalization, orthogonalization, and many computational techniques.
æ ¹æ®æˆ‘ä»¬ä½¿ç”¨çš„åæ ‡ç³»ï¼Œçº¿æ€§å˜æ¢çœ‹èµ·æ¥å¯èƒ½éå¸¸ä¸åŒã€‚ç›¸å¯¹äºæ–°çš„åŸºé‡å†™å‘é‡å’Œå˜æ¢çš„è¿‡ç¨‹ç§°ä¸ºåŸºå˜æ¢ã€‚è¿™ä¸ªæ¦‚å¿µæ˜¯å¯¹è§’åŒ–ã€æ­£äº¤åŒ–ä»¥åŠè®¸å¤šè®¡ç®—æŠ€æœ¯çš„æ ¸å¿ƒã€‚

### Coordinate Change
åæ ‡å˜æ¢

Suppose $V$ is an $n$\-dimensional vector space, and let $\mathcal{B} = \{\mathbf{v}_1, \dots, \mathbf{v}_n\}$ be a basis. Every vector $\mathbf{x} \in V$ has a coordinate vector $[\mathbf{x}]_{\mathcal{B}} \in \mathbb{R}^n$.
å‡è®¾ $V$ æ˜¯ä¸€ä¸ª $n$ ç»´å‘é‡ç©ºé—´ï¼Œè®¾ $\mathcal{B} = \{\mathbf{v}_1, \dots, \mathbf{v}_n\}$ ä¸ºåŸºã€‚æ¯ä¸ªå‘é‡ $\mathbf{x} \in V$ éƒ½æœ‰ä¸€ä¸ªåæ ‡å‘é‡ $[\mathbf{x}]_{\mathcal{B}} \in \mathbb{R}^n$ ã€‚

If $P$ is the change-of-basis matrix from $\mathcal{B}$ to the standard basis, then
å¦‚æœ $P$ æ˜¯ä» $\mathcal{B}$ åˆ°æ ‡å‡†åŸºçš„åŸºå˜æ¢çŸ©é˜µï¼Œåˆ™

$$
\mathbf{x} = P [\mathbf{x}]_{\mathcal{B}}.
$$

Equivalently,
ç­‰ä»·åœ°ï¼Œ

$$
[\mathbf{x}]_{\mathcal{B}} = P^{-1} \mathbf{x}.
$$

Here, $P$ has the basis vectors of $\mathcal{B}$ as its columns:
è¿™é‡Œï¼Œ $P$ ä»¥ $\mathcal{B}$ çš„åŸºå‘é‡ä½œä¸ºå…¶åˆ—ï¼š

$$
P = \begin{bmatrix}\mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n\end{bmatrix}.
$$

### Transformation of Matrices
çŸ©é˜µå˜æ¢

Let $T: V \to V$ be a linear transformation. Suppose its matrix in the standard basis is $A$. In the basis $\mathcal{B}$, the representing matrix becomes
ä»¤ $T: V \to V$ ä¸ºçº¿æ€§å˜æ¢ã€‚å‡è®¾å…¶åœ¨æ ‡å‡†åŸºä¸­çš„çŸ©é˜µä¸º $A$ ã€‚åœ¨åŸº $\mathcal{B}$ ä¸­ï¼Œè¡¨ç¤ºçŸ©é˜µå˜ä¸º

$$
[T]_{\mathcal{B}} = P^{-1} A P.
$$

Thus, changing basis corresponds to a similarity transformation of the matrix.
å› æ­¤ï¼Œæ”¹å˜åŸºç¡€å¯¹åº”äºçŸ©é˜µçš„ç›¸ä¼¼å˜æ¢ã€‚

### Example
ä¾‹å­

Example 5.4.1. Let $T:\mathbb{R}^2 \to \mathbb{R}^2$ be given by
ä¾‹ 5.4.1ã€‚ ä»¤ $T:\mathbb{R}^2 \to \mathbb{R}^2$ ä¸º

$$
T(x,y) = (3x + y, x + y).
$$

In the standard basis, its matrix is
åœ¨æ ‡å‡†åŸºç¡€ä¸Šï¼Œå…¶çŸ©é˜µä¸º

$$
A = \begin{bmatrix}3 & 1 \\1 & 1\end{bmatrix}.
$$

Now consider the basis $\mathcal{B} = \{ (1,1), (1,-1) \}$. The change-of-basis matrix is
ç°åœ¨è€ƒè™‘åŸº $\mathcal{B} = \{ (1,1), (1,-1) \}$ ã€‚åŸºå˜æ¢çŸ©é˜µä¸º

$$
P = \begin{bmatrix}1 & 1 \\1 & -1\end{bmatrix}.
$$

Then
ç„¶å

$$
[T]_{\mathcal{B}} = P^{-1} A P.
$$

Computing gives
è®¡ç®—å¾—å‡º

$$
[T]_{\mathcal{B}} =\begin{bmatrix}4 & 0 \\0 & 0\end{bmatrix}.
$$

In this new basis, the transformation is diagonal: one direction is scaled by 4, the other collapsed to 0.
åœ¨è¿™ä¸ªæ–°çš„åŸºç¡€ä¸Šï¼Œå˜æ¢æ˜¯å¯¹è§’çš„ï¼šä¸€ä¸ªæ–¹å‘ç¼©æ”¾ 4ï¼Œå¦ä¸€ä¸ªæ–¹å‘æŠ˜å ä¸º 0ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

Change of basis is like rotating or skewing your coordinate grid. The underlying transformation does not change, but its description in numbers becomes simpler or more complicated depending on the basis. Finding a basis that simplifies a transformation (often a diagonal basis) is a key theme in linear algebra.
åŸºå˜æ¢å°±åƒæ—‹è½¬æˆ–å€¾æ–œåæ ‡ç½‘æ ¼ã€‚åº•å±‚å˜æ¢æœ¬èº«ä¸ä¼šæ”¹å˜ï¼Œä½†å…¶æ•°å€¼æè¿°ä¼šæ ¹æ®åŸºçš„å˜åŒ–è€Œå˜å¾—æ›´ç®€å•æˆ–æ›´å¤æ‚ã€‚å¯»æ‰¾èƒ½å¤Ÿç®€åŒ–å˜æ¢çš„åŸºï¼ˆé€šå¸¸æ˜¯å¯¹è§’åŸºï¼‰æ˜¯çº¿æ€§ä»£æ•°çš„ä¸€ä¸ªå…³é”®ä¸»é¢˜ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Change of basis connects the abstract notion of similarity to practical computation. It is the tool that allows us to diagonalize matrices, compute eigenvalues, and simplify complex transformations. In applications, it corresponds to choosing a more natural coordinate system-whether in geometry, physics, or machine learning.
åŸºå˜æ¢å°†ç›¸ä¼¼æ€§çš„æŠ½è±¡æ¦‚å¿µä¸å®é™…è®¡ç®—è”ç³»èµ·æ¥ã€‚å®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿå¯¹çŸ©é˜µè¿›è¡Œå¯¹è§’åŒ–ã€è®¡ç®—ç‰¹å¾å€¼å¹¶ç®€åŒ–å¤æ‚çš„å˜æ¢ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå®ƒç›¸å½“äºé€‰æ‹©ä¸€ä¸ªæ›´è‡ªç„¶çš„åæ ‡ç³»â€”â€”æ— è®ºæ˜¯åœ¨å‡ ä½•ã€ç‰©ç†è¿˜æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸã€‚

### Exercises 5.4
ç»ƒä¹  5.4

1.  Let
    è®©

$$
A = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}
$$

Compute its representation in the basis $\{(1,0),(1,1)\}$. 2. Find the change-of-basis matrix from the standard basis of $\mathbb{R}^2$ to $\{(2,1),(1,1)\}$. 3. Prove that similar matrices (related by $P^{-1}AP$) represent the same linear transformation under different bases. 4. Diagonalize the matrix
è®¡ç®—å…¶åœ¨åŸº $\{(1,0),(1,1)\}$ ä¸­çš„è¡¨ç¤ºã€‚2. æ±‚å‡ºä» $\mathbb{R}^2$ åˆ° $\{(2,1),(1,1)\}$ çš„æ ‡å‡†åŸºå˜æ¢çŸ©é˜µã€‚3. è¯æ˜ç›¸ä¼¼çš„çŸ©é˜µï¼ˆç”± $P^{-1}AP$ å…³è”ï¼‰åœ¨ä¸åŒåŸºä¸‹è¡¨ç¤ºç›¸åŒçš„çº¿æ€§å˜æ¢ã€‚4. å¯¹è§’åŒ–çŸ©é˜µ

$$
A = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}
$$

in the basis $\{(1,1),(1,-1)\}$. 5. In $\mathbb{R}^3$, let $\mathcal{B} = \{(1,0,0),(1,1,0),(1,1,1)\}$. Construct the change-of-basis matrix $P$ and compute $P^{-1}$.
åœ¨åŸº $\{(1,1),(1,-1)\}$ ä¸­ã€‚5. åœ¨ $\mathbb{R}^3$ ä¸­ï¼Œä»¤ $\mathcal{B} = \{(1,0,0),(1,1,0),(1,1,1)\}$ ã€‚æ„é€ åŸºå˜æ¢çŸ©é˜µ $P$ å¹¶è®¡ç®— $P^{-1}$ ã€‚

# Chapter 6. Determinants
ç¬¬å…­ç«  å†³å®šå› ç´ 

## 6.1 Motivation and Geometric Meaning
6.1 åŠ¨æœºå’Œå‡ ä½•æ„ä¹‰

Determinants are numerical values associated with square matrices. At first they may appear as a complicated formula, but their importance comes from what they measure: determinants encode scaling, orientation, and invertibility of linear transformations. They bridge algebra and geometry.
è¡Œåˆ—å¼æ˜¯ä¸æ–¹é˜µç›¸å…³çš„æ•°å€¼ã€‚ä¹ä¸€çœ‹ï¼Œå®ƒä»¬å¯èƒ½çœ‹èµ·æ¥åƒä¸€ä¸ªå¤æ‚çš„å…¬å¼ï¼Œä½†å®ƒä»¬çš„é‡è¦æ€§åœ¨äºå®ƒä»¬æ‰€æµ‹é‡çš„å†…å®¹ï¼šè¡Œåˆ—å¼ç¼–ç äº†çº¿æ€§å˜æ¢çš„ç¼©æ”¾ã€æ–¹å‘å’Œå¯é€†æ€§ã€‚å®ƒä»¬è¿æ¥äº†ä»£æ•°å’Œå‡ ä½•ã€‚

### Determinants of 2Ã—2 Matrices
2Ã—2 çŸ©é˜µçš„è¡Œåˆ—å¼

For a 2Ã—2 matrix
å¯¹äº 2Ã—2 çŸ©é˜µ

$$
A = \begin{bmatrix} a & b \\ c & d \end{bmatrix},
$$

the determinant is defined as
è¡Œåˆ—å¼å®šä¹‰ä¸º

$$
\det(A) = ad - bc.
$$

Geometric meaning: If $A$ represents a linear transformation of the plane, then $|\det(A)|$ is the area scaling factor. For example, if $\det(A) = 2$, areas of shapes are doubled. If $\det(A) = 0$, the transformation collapses the plane to a line: all area is lost.
å‡ ä½•å«ä¹‰ï¼šå¦‚æœ $A$ è¡¨ç¤ºå¹³é¢çš„çº¿æ€§å˜æ¢ï¼Œåˆ™ $|\det(A)|$ æ˜¯é¢ç§¯ç¼©æ”¾å› å­ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ $\det(A) = 2$ ï¼Œå½¢çŠ¶çš„é¢ç§¯å°†åŠ å€ã€‚å¦‚æœ $\det(A) = 0$ ï¼Œå˜æ¢å°†å¹³é¢æŠ˜å æˆä¸€æ¡çº¿ï¼šæ‰€æœ‰é¢ç§¯éƒ½å°†ä¸¢å¤±ã€‚

### Determinants of 3Ã—3 Matrices
3Ã—3 çŸ©é˜µçš„è¡Œåˆ—å¼

For
ä¸ºäº†

$$
A = \begin{bmatrix}a & b & c \\d & e & f \\g & h & i\end{bmatrix},
$$

the determinant can be computed as
è¡Œåˆ—å¼å¯ä»¥è®¡ç®—ä¸º

$$
\det(A) = a(ei - fh) - b(di - fg) + c(dh - eg).
$$

Geometric meaning: In $\mathbb{R}^3$, $|\det(A)|$ is the volume scaling factor. If $\det(A) < 0$, orientation is reversed (a handedness flip), such as turning a right-handed coordinate system into a left-handed one.
å‡ ä½•å«ä¹‰ï¼šåœ¨ $\mathbb{R}^3$ ä¸­ï¼Œ $|\det(A)|$ æ˜¯ä½“ç§¯ç¼©æ”¾å› å­ã€‚å¦‚æœä¸º $\det(A) < 0$ ï¼Œåˆ™æ–¹å‘åè½¬ï¼ˆå³æ‰‹æ€§ç¿»è½¬ï¼‰ï¼Œä¾‹å¦‚å°†å³æ‰‹åæ ‡ç³»è½¬æ¢ä¸ºå·¦æ‰‹åæ ‡ç³»ã€‚

### General Case
ä¸€èˆ¬æƒ…å†µ

For $A \in \mathbb{R}^{n \times n}$, the determinant is a scalar that measures how the linear transformation given by $A$ scales n-dimensional volume.
å¯¹äº $A \in \mathbb{R}^{n \times n}$ ï¼Œè¡Œåˆ—å¼æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œå®ƒè¡¡é‡ $A$ ç»™å‡ºçš„çº¿æ€§å˜æ¢å¦‚ä½•ç¼©æ”¾ n ç»´ä½“ç§¯ã€‚

*   If $\det(A) = 0$: the transformation squashes space into a lower dimension, so $A$ is not invertible.
    å¦‚æœ $\det(A) = 0$ ï¼šå˜æ¢å°†ç©ºé—´å‹ç¼©åˆ°è¾ƒä½ç»´åº¦ï¼Œå› æ­¤ $A$ ä¸å¯é€†ã€‚
*   If $\det(A) > 0$: volume is scaled by $\det(A)$, orientation preserved.
    å¦‚æœæ˜¯ $\det(A) > 0$ ï¼šä½“ç§¯æŒ‰ $\det(A)$ ç¼©æ”¾ï¼Œæ–¹å‘ä¿æŒä¸å˜ã€‚
*   If $\det(A) < 0$: volume is scaled by $|\det(A)|$, orientation reversed.
    å¦‚æœæ˜¯ $\det(A) < 0$ ï¼šä½“ç§¯æŒ‰ $|\det(A)|$ ç¼©æ”¾ï¼Œæ–¹å‘åè½¬ã€‚

### Visual Examples
è§†è§‰ç¤ºä¾‹

1.  Shear in $\mathbb{R}^2$: $A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$. Then $\det(A) = 1$. The transformation slants the unit square into a parallelogram but preserves area.
    $\mathbb{R}^2$ å¤„çš„å‰ªåˆ‡ï¼š $A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$ ã€‚ç„¶åæ˜¯ $\det(A) = 1$ ã€‚å˜æ¢å°†å•ä½æ­£æ–¹å½¢å€¾æ–œä¸ºå¹³è¡Œå››è¾¹å½¢ï¼Œä½†ä¿ç•™é¢ç§¯ã€‚
    
2.  Projection in $\mathbb{R}^2$: $A = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$. Then $\det(A) = 0$. The unit square collapses into a line segment: area vanishes.
    $\mathbb{R}^2$ ä¸­çš„æŠ•å½±ï¼š $A = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$ ã€‚ç„¶å $\det(A) = 0$ ã€‚å•ä½æ­£æ–¹å½¢åç¼©æˆä¸€æ¡çº¿æ®µï¼šé¢ç§¯æ¶ˆå¤±ã€‚
    
3.  Rotation in $\mathbb{R}^2$: $R_\theta = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}$. Then $\det(R_\theta) = 1$. Rotations preserve area and orientation.
    $\mathbb{R}^2$ ä¸­çš„æ—‹è½¬ï¼š $R_\theta = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}$ ã€‚ç„¶å $\det(R_\theta) = 1$ ã€‚æ—‹è½¬ä¿ç•™é¢ç§¯å’Œæ–¹å‘ã€‚
    

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

The determinant is not just a formula-it is a measure of transformation. It tells us whether a matrix is invertible, how it distorts space, and whether it flips orientation. This geometric insight makes the determinant indispensable in analysis, geometry, and applied mathematics.
è¡Œåˆ—å¼ä¸ä»…ä»…æ˜¯ä¸€ä¸ªå…¬å¼ï¼Œå®ƒè¿˜æ˜¯ä¸€ç§å˜æ¢çš„åº¦é‡ã€‚å®ƒå‘Šè¯‰æˆ‘ä»¬ä¸€ä¸ªçŸ©é˜µæ˜¯å¦å¯é€†ï¼Œå®ƒå¦‚ä½•æ‰­æ›²ç©ºé—´ï¼Œä»¥åŠå®ƒæ˜¯å¦ä¼šç¿»è½¬æ–¹å‘ã€‚è¿™ç§å‡ ä½•å­¦ä¸Šçš„æ´å¯ŸåŠ›ä½¿å¾—è¡Œåˆ—å¼åœ¨åˆ†æã€å‡ ä½•å’Œåº”ç”¨æ•°å­¦ä¸­ä¸å¯æˆ–ç¼ºã€‚

### Exercises 6.1
ç»ƒä¹  6.1

1.  Compute the determinant of
    è®¡ç®—è¡Œåˆ—å¼

$$
\begin{bmatrix} 2 & 3 \\ 1 & 4 \end{bmatrix}
$$

What area scaling factor does it represent? 2. Find the determinant of the shear matrix
å®ƒä»£è¡¨ä»€ä¹ˆé¢ç§¯æ¯”ä¾‹å› å­ï¼Ÿ2. æ±‚å‰ªåˆ‡çŸ©é˜µçš„è¡Œåˆ—å¼

$$
\begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix}
$$

What happens to the area of the unit square? 3. For the $3 \\times 3matrix \[100020003\] Compute the determinant. How does it scale volume in\\mathbb{R}^3$?4. Show that any rotation matrix in $\\mathbb{R}^2 has determinant \\1. 5. Give an example of a \\2 \\times 2$matrix with determinant$\-1$. What geometric action does it represent?
å•ä½æ­£æ–¹å½¢çš„é¢ç§¯ä¼šå‘ç”Ÿä»€ä¹ˆå˜åŒ–ï¼Ÿ 3. å¯¹äº $3 \\times 3 çŸ©é˜µ \[100020003\] è®¡ç®—è¡Œåˆ—å¼ã€‚\\mathbb{R}^3 $?4. Show that any rotation matrix in $ \\mathbb{R}^2 çš„è¡Œåˆ—å¼ä¸º \\ 1 ï¼Œå®ƒå¦‚ä½•ç¼©æ”¾ä½“ç§¯ ï¼Ÿ5. ä¸¾ä¸€ä¸ª \\ 2 \\times 2 $matrix with determinant$ -1$ çš„ä¾‹å­ ã€‚å®ƒä»£è¡¨ä»€ä¹ˆå‡ ä½•ä½œç”¨ï¼Ÿ

## 6.2 Properties of Determinants
6.2 è¡Œåˆ—å¼çš„æ€§è´¨

Beyond their geometric meaning, determinants satisfy a collection of algebraic rules that make them powerful tools in linear algebra. These properties allow us to compute efficiently, test invertibility, and understand how determinants behave under matrix operations.
é™¤äº†å‡ ä½•æ„ä¹‰ä¹‹å¤–ï¼Œè¡Œåˆ—å¼è¿˜æ»¡è¶³ä¸€ç³»åˆ—ä»£æ•°è§„åˆ™ï¼Œä½¿å…¶æˆä¸ºçº¿æ€§ä»£æ•°ä¸­å¼ºå¤§çš„å·¥å…·ã€‚è¿™äº›æ€§è´¨ä½¿æˆ‘ä»¬èƒ½å¤Ÿé«˜æ•ˆè®¡ç®—ã€æµ‹è¯•å¯é€†æ€§ï¼Œå¹¶ç†è§£è¡Œåˆ—å¼åœ¨çŸ©â€‹â€‹é˜µè¿ç®—ä¸‹çš„è¡Œä¸ºã€‚

### Basic Properties
åŸºæœ¬å±æ€§

Let $A, B \in \mathbb{R}^{n \times n}$, and let $c \in \mathbb{R}$. Then:
ä»¤ $A, B \in \mathbb{R}^{n \times n}$ ï¼Œä»¤ $c \in \mathbb{R}$ ã€‚ç„¶åï¼š

1.  Identity:
    èº«ä»½ï¼š

$$
\det(I_n) = 1.
$$

2.  Triangular matrices: If $A$ is upper or lower triangular, then
    ä¸‰è§’çŸ©é˜µï¼š å¦‚æœ $A$ æ˜¯ä¸Šä¸‰è§’æˆ–ä¸‹ä¸‰è§’ï¼Œåˆ™

$$
\det(A) = a_{11} a_{22} \cdots a_{nn}.
$$

3.  Row/column swap: Interchanging two rows (or columns) multiplies the determinant by $-1$.
    è¡Œ/åˆ—äº¤æ¢ï¼š äº¤æ¢ä¸¤è¡Œï¼ˆæˆ–åˆ—ï¼‰å°†è¡Œåˆ—å¼ä¹˜ä»¥ $-1$ ã€‚
    
4.  Row/column scaling: Multiplying a row (or column) by a scalar $c$ multiplies the determinant by $c$.
    è¡Œ/åˆ—ç¼©æ”¾ï¼š å°†è¡Œï¼ˆæˆ–åˆ—ï¼‰ä¹˜ä»¥æ ‡é‡ $c$ ä¼šå°†è¡Œåˆ—å¼ä¹˜ä»¥ $c$ ã€‚
    
5.  Row/column addition: Adding a multiple of one row to another does not change the determinant.
    è¡Œ/åˆ—åŠ æ³•ï¼šå°†ä¸€è¡Œçš„å€æ•°æ·»åŠ åˆ°å¦ä¸€è¡Œä¸ä¼šæ”¹å˜è¡Œåˆ—å¼ã€‚
    
6.  Transpose:
    è½¬ç½®ï¼š
    

$$
\det(A^T) = \det(A).
$$

7.  Multiplicativity:
    ä¹˜æ³•æ€§ï¼š

$$
\det(AB) = \det(A)\det(B).
$$

8.  Invertibility: $A$ is invertible if and only if $\det(A) \neq 0$.
    å¯é€†æ€§ï¼š å½“ä¸”ä»…å½“ $\det(A) \neq 0$ æ—¶ï¼Œ $A$ æ‰æ˜¯å¯é€†çš„ã€‚

### Example Computations
è®¡ç®—ç¤ºä¾‹

Example 6.2.1. For
ä¾‹ 6.2.1. å¯¹äº

$$
A = \begin{bmatrix}2 & 0 & 0 \\1 & 3 & 0 \\-1 & 4 & 5\end{bmatrix},
$$

$A$ is lower triangular, so
$A$ æ˜¯ä¸‹ä¸‰è§’ï¼Œæ‰€ä»¥

$$
\det(A) = 2 \cdot 3 \cdot 5 = 30.
$$

Example 6.2.2. Let
ä¾‹ 6.2.2. è®¾

$$
B = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}, \quadC = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}.
$$

Then
ç„¶å

$$
\det(B) = 1\cdot 4 - 2\cdot 3 = -2, \quad \det(C) = -1.
$$

Since $CB$ is obtained by swapping rows of $B$,
ç”±äº $CB$ æ˜¯é€šè¿‡äº¤æ¢ $B$ çš„è¡Œè·å¾—çš„ï¼Œ

$$
\det(CB) = -\det(B) = 2.
$$

This matches the multiplicativity rule: $\det(CB) = \det(C)\det(B) = (-1)(-2) = 2.$
è¿™ç¬¦åˆä¹˜æ³•è§„åˆ™ï¼š $\det(CB) = \det(C)\det(B) = (-1)(-2) = 2.$

### Geometric Insights
å‡ ä½•æ´å¯Ÿ

*   Row swaps: flipping orientation of space.
    è¡Œäº¤æ¢ï¼šç¿»è½¬ç©ºé—´çš„æ–¹å‘ã€‚
*   Scaling a row: stretching space in one direction.
    ç¼©æ”¾ä¸€è¡Œï¼šæœä¸€ä¸ªæ–¹å‘æ‹‰ä¼¸ç©ºé—´ã€‚
*   Row replacement: sliding hyperplanes without altering volume.
    è¡Œæ›¿æ¢ï¼šæ»‘åŠ¨è¶…å¹³é¢è€Œä¸æ”¹å˜ä½“ç§¯ã€‚
*   Multiplicativity: performing two transformations multiplies their scaling factors.
    ä¹˜æ³•æ€§ï¼šæ‰§è¡Œä¸¤ä¸ªå˜æ¢ä¼šå°†å®ƒä»¬çš„æ¯”ä¾‹å› å­ç›¸ä¹˜ã€‚

These properties make determinants both computationally manageable and geometrically interpretable.
è¿™äº›æ€§è´¨ä½¿å¾—è¡Œåˆ—å¼æ—¢æ˜“äºè®¡ç®—ç®¡ç†ï¼Œåˆæ˜“äºå‡ ä½•è§£é‡Šã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Determinant properties connect computation with geometry and theory. They explain why Gaussian elimination works, why invertibility is equivalent to nonzero determinant, and why determinants naturally arise in areas like volume computation, eigenvalue theory, and differential equations.
è¡Œåˆ—å¼çš„æ€§è´¨å°†è®¡ç®—ä¸å‡ ä½•å’Œç†è®ºè”ç³»èµ·æ¥ã€‚å®ƒä»¬è§£é‡Šäº†é«˜æ–¯æ¶ˆå…ƒæ³•ä¸ºä½•æœ‰æ•ˆï¼Œå¯é€†æ€§ä¸ºä½•ç­‰ä»·äºéé›¶è¡Œåˆ—å¼ï¼Œä»¥åŠè¡Œåˆ—å¼ä¸ºä½•è‡ªç„¶åœ°å‡ºç°åœ¨ä½“ç§¯è®¡ç®—ã€ç‰¹å¾å€¼ç†è®ºå’Œå¾®åˆ†æ–¹ç¨‹ç­‰é¢†åŸŸã€‚

### Exercises 6.2
ç»ƒä¹  6.2

1.  Compute the determinant of
    è®¡ç®—è¡Œåˆ—å¼

$$
A = \begin{bmatrix} 1 & 2 & 3 \\ 0 & 1 & 4 \\ 0 & 0 & 2 \end{bmatrix}.
$$

2.  Show that if two rows of a square matrix are identical, then its determinant is zero.
    è¯æ˜å¦‚æœæ–¹é˜µçš„ä¸¤è¡Œç›¸åŒï¼Œåˆ™å…¶è¡Œåˆ—å¼ä¸ºé›¶ã€‚
    
3.  Verify $\det(A^T) = \det(A)$ for
    éªŒè¯ $\det(A^T) = \det(A)$
    

$$
A = \begin{bmatrix} 2 & -1 \\ 3 & 4 \end{bmatrix}.
$$

4.  If $A$ is invertible, prove that
    å¦‚æœ $A$ å¯é€†ï¼Œåˆ™è¯æ˜

$$
\det(A^{-1}) = \frac{1}{\det(A)}.
$$

5.  Suppose $A$ is a $3\\times 3$matrix with$\\det(A) = 5$. What is $\\det(2A)$?
    å‡è®¾ $A$ æ˜¯ $3\\times 3 $matrix with$ \\det(A) = 5 $. What is $ \\det(2A)$ï¼Ÿ

## 6.3 Cofactor Expansion
6.3 è¾…å› å­å±•å¼€

While determinants of small matrices can be computed directly from formulas, larger matrices require a systematic method. The cofactor expansion (also known as Laplace expansion) provides a recursive way to compute determinants by breaking them into smaller ones.
è™½ç„¶å°çŸ©é˜µçš„è¡Œåˆ—å¼å¯ä»¥ç›´æ¥é€šè¿‡å…¬å¼è®¡ç®—ï¼Œä½†è¾ƒå¤§çš„çŸ©é˜µåˆ™éœ€è¦ç³»ç»Ÿçš„æ–¹æ³•ã€‚ä½™å› å­å±•å¼€å¼ï¼ˆä¹Ÿç§°ä¸ºæ‹‰æ™®æ‹‰æ–¯å±•å¼€å¼ï¼‰é€šè¿‡å°†è¡Œåˆ—å¼åˆ†è§£ä¸ºæ›´å°çš„çŸ©é˜µï¼Œæä¾›äº†ä¸€ç§é€’å½’è®¡ç®—è¡Œåˆ—å¼çš„æ–¹æ³•ã€‚

### Minors and Cofactors
å°å¼å’Œè¾…å› å­

For an $n \times n$ matrix $A = [a_{ij}]$:
å¯¹äº $n \times n$ çŸ©é˜µ $A = [a_{ij}]$ ï¼š

*   The minor $M_{ij}$ is the determinant of the $(n-1) \times (n-1)$ matrix obtained by deleting the $i$\-th row and $j$ -th column of $A$.
    å°è°ƒğ‘€ ğ‘– ğ‘— M ä¼Šå¥‡ â€‹ æ˜¯åˆ é™¤ç¬¬ $i$ è¡Œå’Œ $j$ åå¾—åˆ°çš„ $(n-1) \times (n-1)$ çŸ©é˜µçš„è¡Œåˆ—å¼ $A$ çš„ç¬¬åˆ—ã€‚
*   The cofactor $C_{ij}$ is defined by
    è¾…å› å­ğ¶ ğ‘– ğ‘— C ä¼Šå¥‡ â€‹ å®šä¹‰ä¸º

$$
C_{ij} = (-1)^{i+j} M_{ij}.
$$

The sign factor $(-1)^{i+j}$ alternates in a checkerboard pattern:
ç¬¦å·å› å­ $(-1)^{i+j}$ ä»¥æ£‹ç›˜æ ¼å›¾æ¡ˆäº¤æ›¿å‡ºç°ï¼š

$$
\begin{bmatrix}+ & - & + & - & \cdots \\- & + & - & + & \cdots \\+ & - & + & - & \cdots \\\vdots & \vdots & \vdots & \vdots & \ddots\end{bmatrix}.
$$

### Cofactor Expansion Formula
è¾…å› å¼å±•å¼€å…¬å¼

The determinant of $A$ can be computed by expanding along any row or any column:
$A$ çš„è¡Œåˆ—å¼å¯ä»¥é€šè¿‡æ²¿ä»»æ„è¡Œæˆ–ä»»æ„åˆ—å±•å¼€æ¥è®¡ç®—ï¼š

$$
\det(A) = \sum_{j=1}^n a_{ij} C_{ij} \quad \text{(expansion along row \(i\))},
$$

 

$$
\det(A) = \sum_{i=1}^n a_{ij} C_{ij} \quad \text{(expansion along column \(j\))}.
$$

### Example
ä¾‹å­

Example 6.3.1. Compute
ä¾‹ 6.3.1. è®¡ç®—

$$
A = \begin{bmatrix}1 & 2 & 3 \\0 & 4 & 5 \\1 & 0 & 6\end{bmatrix}.
$$

Expand along the first row:
æ²¿ç¬¬ä¸€è¡Œå±•å¼€ï¼š

$$
\det(A) = 1 \cdot C_{11} + 2 \cdot C_{12} + 3 \cdot C_{13}.
$$

*   For $C_{11}$:
    å¯¹äºğ¶ 11 C 11 â€‹ :

$$
M_{11} = \det \begin{bmatrix} 4 & 5 \\ 0 & 6 \end{bmatrix} = 24
$$

so $C_{11} = (+1)(24) = 24$.
æ‰€ä»¥ $C_{11} = (+1)(24) = 24$ ã€‚

*   For $C_{12}$:
    å¯¹äºğ¶ 12 C 12 â€‹ :

$$
M_{12} = \det \begin{bmatrix} 0 & 5 \\ 1 & 6 \end{bmatrix} = 0 - 5 = -5
$$

so $C_{12} = (-1)(-5) = 5$.
æ‰€ä»¥ $C_{12} = (-1)(-5) = 5$ ã€‚

*   For $C_{13}$:
    å¯¹äºğ¶ 13 C 13 â€‹ :

$$
M_{13} = \det \begin{bmatrix} 0 & 4 \\ 1 & 0 \end{bmatrix} = 0 - 4 = -4
$$

so $C_{13} = (+1)(-4) = -4$.
æ‰€ä»¥ $C_{13} = (+1)(-4) = -4$ ã€‚

Thus,
å› æ­¤ï¼Œ

$$
\det(A) = 1(24) + 2(5) + 3(-4) = 24 + 10 - 12 = 22.
$$

### Properties of Cofactor Expansion
è¾…å› å­å±•å¼€çš„æ€§è´¨

1.  Expansion along any row or column yields the same result.
    æ²¿ä»»æ„è¡Œæˆ–åˆ—æ‰©å±•éƒ½ä¼šäº§ç”Ÿç›¸åŒçš„ç»“æœã€‚
2.  The cofactor expansion provides a recursive definition of determinant: a determinant of size $n$ is expressed in terms of determinants of size $n-1$.
    ä½™å› å­å±•å¼€æä¾›äº†è¡Œåˆ—å¼çš„é€’å½’å®šä¹‰ï¼šå¤§å°ä¸º $n$ çš„è¡Œåˆ—å¼å¯ä»¥ç”¨å¤§å°ä¸º $n-1$ çš„è¡Œåˆ—å¼æ¥è¡¨ç¤ºã€‚
3.  Cofactors are fundamental in constructing the adjugate matrix, which gives a formula for inverses:
    ä½™å› å­æ˜¯æ„é€ ä¼´éšçŸ©é˜µçš„åŸºç¡€ï¼Œå®ƒç»™å‡ºäº†é€†çš„å…¬å¼ï¼š

$$
A^{-1} = \frac{1}{\det(A)} \, \text{adj}(A), \quad \text{where adj}(A) = [C_{ji}].
$$

### Geometric Interpretation
å‡ ä½•è§£é‡Š

Cofactor expansion breaks down the determinant into contributions from sub-volumes defined by fixing one row or column at a time. Each cofactor measures how that row/column influences the overall volume scaling.
ä½™å› å­å±•å¼€å°†è¡Œåˆ—å¼åˆ†è§£ä¸ºç”±æ¯æ¬¡å›ºå®šä¸€è¡Œæˆ–ä¸€åˆ—å®šä¹‰çš„å­ä½“ç§¯çš„è´¡çŒ®ã€‚æ¯ä¸ªä½™å› å­è¡¡é‡è¯¥è¡Œ/åˆ—å¯¹æ•´ä½“ä½“ç§¯ç¼©æ”¾çš„å½±å“ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Cofactor expansion generalizes the small-matrix formulas and provides a conceptual definition of determinants. While not the most efficient way to compute determinants for large matrices, it is essential for theory, proofs, and connections to adjugates, Cramerâ€™s rule, and classical geometry.
ä½™å› å­å±•å¼€å¼æ¨å¹¿äº†å°çŸ©é˜µå…¬å¼ï¼Œå¹¶æä¾›äº†è¡Œåˆ—å¼çš„æ¦‚å¿µå®šä¹‰ã€‚è™½ç„¶å®ƒå¹¶éè®¡ç®—å¤§çŸ©é˜µè¡Œåˆ—å¼çš„æœ€æœ‰æ•ˆæ–¹æ³•ï¼Œä½†å®ƒå¯¹äºç†è®ºã€è¯æ˜ä»¥åŠä¸ä¼´éšé¡¹ã€å…‹è±å§†è§„åˆ™å’Œå¤å…¸å‡ ä½•çš„è”ç³»è‡³å…³é‡è¦ã€‚

### Exercises 6.3
ç»ƒä¹  6.3

1.  Compute the determinant of
    è®¡ç®—è¡Œåˆ—å¼

$$
\begin{bmatrix}2 & 0 & 1 \\3 & -1 & 4 \\1 & 2 & 0\end{bmatrix}
$$

by cofactor expansion along the first column.
é€šè¿‡æ²¿ç¬¬ä¸€åˆ—çš„ä½™å› å­å±•å¼€ã€‚

2.  Verify that expanding along the second row of Example 6.3.1 gives the same determinant.
    éªŒè¯æ²¿ç¤ºä¾‹ 6.3.1 çš„ç¬¬äºŒè¡Œå±•å¼€æ˜¯å¦ç»™å‡ºç›¸åŒçš„è¡Œåˆ—å¼ã€‚
    
3.  Prove that expansion along any row gives the same value.
    è¯æ˜æ²¿ä»»ä½•è¡Œå±•å¼€éƒ½ä¼šç»™å‡ºç›¸åŒçš„å€¼ã€‚
    
4.  Show that if a row of a matrix is zero, then its determinant is zero.
    è¯æ˜å¦‚æœçŸ©é˜µçš„æŸä¸€è¡Œæ˜¯é›¶ï¼Œé‚£ä¹ˆå®ƒçš„è¡Œåˆ—å¼ä¹Ÿæ˜¯é›¶ã€‚
    
5.  Use cofactor expansion to prove that $\det(A) = \det(A^T)$.
    ä½¿ç”¨ä½™å› å­å±•å¼€æ¥è¯æ˜ $\det(A) = \det(A^T)$ ã€‚
    

## 6.4 Applications (Volume, Invertibility Test)
6.4 åº”ç”¨ï¼ˆä½“ç§¯ã€å¯é€†æ€§æµ‹è¯•ï¼‰

Determinants are not merely algebraic curiosities; they have concrete geometric and computational uses. Two of the most important applications are measuring volumes and testing invertibility of matrices.
è¡Œåˆ—å¼ä¸ä»…ä»…æ˜¯ä»£æ•°ä¸Šçš„å¥‡é—»ï¼›å®ƒä»¬æœ‰ç€å…·ä½“çš„å‡ ä½•å’Œè®¡ç®—ç”¨é€”ã€‚å…¶ä¸­æœ€é‡è¦çš„ä¸¤ä¸ªåº”ç”¨æ˜¯æµ‹é‡ä½“ç§¯å’Œæ£€éªŒçŸ©é˜µçš„å¯é€†æ€§ã€‚

### Determinants as Volume Scalers
å†³å®šå› ç´ ä½œä¸ºä½“ç§¯æ ‡é‡

Given vectors $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \in \mathbb{R}^n$, arrange them as columns of a matrix:
ç»™å®šå‘é‡ $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \in \mathbb{R}^n$ ï¼Œå°†å®ƒä»¬æ’åˆ—ä¸ºçŸ©é˜µçš„åˆ—ï¼š

$$
A = \begin{bmatrix}| & | & & | \\\mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n \\| & | & & |\end{bmatrix}.
$$

Then $|\det(A)|$ equals the volume of the parallelepiped spanned by these vectors.
é‚£ä¹ˆ $|\det(A)|$ ç­‰äºè¿™äº›å‘é‡æ‰€è·¨è¶Šçš„å¹³è¡Œå…­é¢ä½“çš„ä½“ç§¯ã€‚

*   In $\mathbb{R}^2$, $|\det(A)|$ gives the area of the parallelogram spanned by $\mathbf{v}_1, \mathbf{v}_2$.
    åœ¨ $\mathbb{R}^2$ ä¸­ï¼Œ $|\det(A)|$ ç»™å‡ºç”± ğ‘£ æ„æˆçš„å¹³è¡Œå››è¾¹å½¢çš„é¢ç§¯ 1 , ğ‘£ 2 v 1 â€‹ ï¼Œv 2 â€‹ .
*   In $\mathbb{R}^3$, $|\det(A)|$ gives the volume of the parallelepiped spanned by $\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3$.
    åœ¨ $\mathbb{R}^3$ ä¸­ï¼Œ $|\det(A)|$ ç»™å‡ºå¹³è¡Œå…­é¢ä½“çš„ä½“ç§¯ï¼Œè·¨åº¦ä¸º ğ‘£ 1 , ğ‘£ 2 , ğ‘£ 3 v 1 â€‹ ï¼Œv 2 â€‹ ï¼Œv 3 â€‹ .
*   In higher dimensions, it generalizes to $n$\-dimensional volume (hypervolume).
    åœ¨æ›´é«˜ç»´åº¦ä¸­ï¼Œå®ƒå¯ä»¥æ¨å¹¿åˆ° $n$ ç»´ä½“ç§¯ï¼ˆè¶…ä½“ç§¯ï¼‰ã€‚

Example 6.4.1. Let
ä¾‹ 6.4.1. è®¾

$$
\mathbf{v}_1 = (1,0,0), \quad \mathbf{v}_2 = (1,1,0), \quad \mathbf{v}_3 = (1,1,1).
$$

Then
ç„¶å

$$
A = \begin{bmatrix}1 & 1 & 1 \\0 & 1 & 1 \\0 & 0 & 1\end{bmatrix}, \quad \det(A) = 1.
$$

So the parallelepiped has volume 1, even though the vectors are not orthogonal.
å› æ­¤ï¼Œå³ä½¿å‘é‡ä¸æ­£äº¤ï¼Œå¹³è¡Œå…­é¢ä½“çš„ä½“ç§¯ä¹Ÿæ˜¯ 1 ã€‚

### Invertibility Test
å¯é€†æ€§æµ‹è¯•

A square matrix $A$ is invertible if and only if $\det(A) \neq 0$.
æ–¹é˜µ $A$ å¯é€†å½“ä¸”ä»…å½“ $\det(A) \neq 0$ ã€‚

*   If $\det(A) = 0$: the transformation collapses space into a lower dimension (area/volume is zero). No inverse exists.
    å¦‚æœ $\det(A) = 0$ ï¼šå˜æ¢å°†ç©ºé—´å¡Œç¼©è‡³è¾ƒä½ç»´åº¦ï¼ˆé¢ç§¯/ä½“ç§¯ä¸ºé›¶ï¼‰ã€‚ä¸å­˜åœ¨é€†å˜æ¢ã€‚
*   If $\det(A) \neq 0$: the transformation scales volume by $|\det(A)|$, and is reversible.
    å¦‚æœ $\det(A) \neq 0$ ï¼šå˜æ¢å°†ä½“ç§¯ç¼©æ”¾ $|\det(A)|$ ï¼Œå¹¶ä¸”æ˜¯å¯é€†çš„ã€‚

Example 6.4.2. The matrix
ä¾‹ 6.4.2. çŸ©é˜µ

$$
B = \begin{bmatrix} 2 & 4 \\ 1 & 2 \end{bmatrix}
$$

has determinant $\det(B) = 2 \cdot 2 - 4 \cdot 1 = 0$. Thus, $B$ is not invertible. Geometrically, the two column vectors are collinear, spanning only a line in $\mathbb{R}^2$.
è¡Œåˆ—å¼ä¸º $\det(B) = 2 \cdot 2 - 4 \cdot 1 = 0$ ã€‚å› æ­¤ï¼Œ $B$ ä¸å¯é€†ã€‚å‡ ä½•ä¸Šï¼Œè¿™ä¸¤ä¸ªåˆ—å‘é‡å…±çº¿ï¼Œåœ¨ $\mathbb{R}^2$ ä¸­ä»…å»¶ä¼¸ä¸€æ¡çº¿ã€‚

### Cramerâ€™s Rule
å…‹è±é»˜è§„åˆ™

Determinants also provide an explicit formula for solving systems of linear equations when the matrix is invertible. For $A\mathbf{x} = \mathbf{b}$ with $A \in \mathbb{R}^{n \times n}$:
å½“çŸ©é˜µå¯é€†æ—¶ï¼Œè¡Œåˆ—å¼è¿˜æä¾›äº†æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„çš„æ˜ç¡®å…¬å¼ã€‚ å¯¹äºå¸¦æœ‰ $A \in \mathbb{R}^{n \times n}$ çš„ $A\mathbf{x} = \mathbf{b}$ ï¼š

$$
x_i = \frac{\det(A_i)}{\det(A)},
$$

where $A_i$ is obtained by replacing the $i$\-th column of $A$ with $\mathbf{b}$. While inefficient computationally, Cramerâ€™s rule highlights the determinantâ€™s role in solutions and uniqueness.
å…¶ä¸­ğ´ ğ‘– A i â€‹ é€šè¿‡å°† $A$ çš„ç¬¬ $i$ åˆ—æ›¿æ¢ä¸º $\mathbf{b}$ å¾—åˆ°ã€‚å…‹è±å§†è§„åˆ™è™½ç„¶è®¡ç®—æ•ˆç‡ä½ä¸‹ï¼Œä½†å®ƒå‡¸æ˜¾äº†è¡Œåˆ—å¼åœ¨è§£å’Œå”¯ä¸€æ€§æ–¹é¢çš„ä½œç”¨ã€‚

### Orientation
æ–¹å‘

The sign of $\det(A)$ indicates whether a transformation preserves or reverses orientation. For example, a reflection in the plane has determinant $-1$, flipping handedness.
$\det(A)$ çš„ç¬¦å·è¡¨ç¤ºå˜æ¢æ˜¯ä¿æŒæ–¹å‘è¿˜æ˜¯åè½¬æ–¹å‘ã€‚ä¾‹å¦‚ï¼Œå¹³é¢ä¸Šçš„åå°„å…·æœ‰è¡Œåˆ—å¼ $-1$ ï¼Œå³ç¿»è½¬æ—‹å‘æ€§ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Determinants condense key information: they measure scaling, test invertibility, and track orientation. These insights are indispensable in geometry (areas and volumes), analysis (Jacobian determinants in calculus), and computation ( solving systems and checking singularity).
è¡Œåˆ—å¼æµ“ç¼©äº†å…³é”®ä¿¡æ¯ï¼šå®ƒä»¬æµ‹é‡ç¼©æ”¾æ¯”ä¾‹ã€æ£€éªŒå¯é€†æ€§å¹¶è¿½è¸ªæ–¹å‘ã€‚è¿™äº›æ´è§åœ¨å‡ ä½•å­¦ï¼ˆé¢ç§¯å’Œä½“ç§¯ï¼‰ã€åˆ†æå­¦ï¼ˆå¾®ç§¯åˆ†ä¸­çš„é›…å¯æ¯”è¡Œåˆ—å¼ï¼‰å’Œè®¡ç®—å­¦ï¼ˆæ±‚è§£ç³»ç»Ÿå’Œæ£€æŸ¥å¥‡ç‚¹ï¼‰ä¸­éƒ½ä¸å¯æˆ–ç¼ºã€‚

### Exercises 6.4
ç»ƒä¹  6.4

1.  Compute the area of the parallelogram spanned by $(2,1)$ and $(1,3)$.
    è®¡ç®— $(2,1)$ å’Œ $(1,3)$ æ‰€æ„æˆçš„å¹³è¡Œå››è¾¹å½¢çš„é¢ç§¯ã€‚
    
2.  Find the volume of the parallelepiped spanned by $(1,0,0), (1,1,0), (1,1,1)$.
    æ±‚å‡º $(1,0,0), (1,1,0), (1,1,1)$ æ‰€è·¨åº¦çš„å¹³è¡Œå…­é¢ä½“çš„ä½“ç§¯ã€‚
    
3.  Determine whether the matrix
    ç¡®å®šçŸ©é˜µ
    

$$
\begin{bmatrix} 1 & 2 \\ 3 & 6 \end{bmatrix}
$$

is invertible. Justify using determinants. 4. Use Cramerâ€™s rule to solve
æ˜¯å¯é€†çš„ã€‚ç”¨è¡Œåˆ—å¼è¯æ˜ã€‚4. ä½¿ç”¨å…‹è±å§†è§„åˆ™æ±‚è§£

$$
\begin{cases}x + y = 3, \\2x - y = 0.\end{cases}
$$

5.  Explain geometrically why a determinant of zero implies no inverse exists.
    ä»å‡ ä½•è§’åº¦è§£é‡Šä¸ºä»€ä¹ˆè¡Œåˆ—å¼ä¸ºé›¶æ„å‘³ç€ä¸å­˜åœ¨é€†å…ƒã€‚

# Chapter 7. Inner Product Spaces
ç¬¬ä¸ƒç« å†…ç§¯ç©ºé—´

## 7.1 Inner Products and Norms
7.1 å†…ç§¯å’ŒèŒƒæ•°

To extend the geometric ideas of length, distance, and angle beyond $\mathbb{R}^2$ and $\mathbb{R}^3$, we introduce inner products. Inner products provide a way of measuring similarity between vectors, while norms derived from them measure length. These concepts are the foundation of geometry inside vector spaces.
ä¸ºäº†å°†é•¿åº¦ã€è·ç¦»å’Œè§’åº¦çš„å‡ ä½•æ¦‚å¿µæ‰©å±•åˆ° $\mathbb{R}^2$ å’Œ $\mathbb{R}^3$ ä¹‹å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å†…ç§¯ã€‚å†…ç§¯æä¾›äº†ä¸€ç§åº¦é‡å‘é‡ä¹‹é—´ç›¸ä¼¼æ€§çš„æ–¹æ³•ï¼Œè€Œç”±å†…ç§¯å¯¼å‡ºçš„èŒƒæ•°åˆ™ç”¨äºåº¦é‡é•¿åº¦ã€‚è¿™äº›æ¦‚å¿µæ˜¯å‘é‡ç©ºé—´å‡ ä½•çš„åŸºç¡€ã€‚

### Inner Product
å†…ç§¯

An inner product on a real vector space $V$ is a function
å®å‘é‡ç©ºé—´ $V$ ä¸Šçš„å†…ç§¯æ˜¯ä¸€ä¸ªå‡½æ•°

$$
\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}
$$

that assigns to each pair of vectors $(\mathbf{u}, \mathbf{v})$ a real number, subject to the following properties:
ä¸ºæ¯å¯¹å‘é‡ $(\mathbf{u}, \mathbf{v})$ åˆ†é…ä¸€ä¸ªå®æ•°ï¼Œå¹¶éµå¾ªä»¥ä¸‹å±æ€§ï¼š

1.  Symmetry: $\langle \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle.$
    å¯¹ç§°ï¼š $\langle \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle.$
    
2.  Linearity in the first argument: $\langle a\mathbf{u} + b\mathbf{w}, \mathbf{v} \rangle = a \langle \mathbf{u}, \mathbf{v} \rangle + b \langle \mathbf{w}, \mathbf{v} \rangle.$
    ç¬¬ä¸€ä¸ªå‚æ•°çš„çº¿æ€§ï¼š $\langle a\mathbf{u} + b\mathbf{w}, \mathbf{v} \rangle = a \langle \mathbf{u}, \mathbf{v} \rangle + b \langle \mathbf{w}, \mathbf{v} \rangle.$
    
3.  Positive-definiteness: $\langle \mathbf{v}, \mathbf{v} \rangle \geq 0$, and equality holds if and only if $\mathbf{v} = \mathbf{0}$.
    æ­£å®šæ€§ï¼š $\langle \mathbf{v}, \mathbf{v} \rangle \geq 0$ ï¼Œä¸”ä»…å½“ $\mathbf{v} = \mathbf{0}$ æ—¶ç­‰å¼æˆç«‹ã€‚
    

The standard inner product on $\mathbb{R}^n$ is the dot product:
$\mathbb{R}^n$ ä¸Šçš„æ ‡å‡†å†…ç§¯æ˜¯ç‚¹ç§¯ï¼š

$$
\langle \mathbf{u}, \mathbf{v} \rangle = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n.
$$

### Norms
è§„èŒƒ

The norm of a vector is its length, defined in terms of the inner product:
å‘é‡çš„èŒƒæ•°æ˜¯å…¶é•¿åº¦ï¼Œæ ¹æ®å†…ç§¯å®šä¹‰ï¼š

$$
\|\mathbf{v}\| = \sqrt{\langle \mathbf{v}, \mathbf{v} \rangle}.
$$

For the dot product in $\mathbb{R}^n$:
å¯¹äº $\mathbb{R}^n$ ä¸­çš„ç‚¹ç§¯ï¼š

$$
\|(x_1, x_2, \dots, x_n)\| = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}.
$$

### Angles Between Vectors
å‘é‡ä¹‹é—´çš„è§’åº¦

The inner product allows us to define the angle $\theta$ between two nonzero vectors $\mathbf{u}, \mathbf{v}$ by
é€šè¿‡å†…ç§¯ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸¤ä¸ªéé›¶å‘é‡ $\mathbf{u}, \mathbf{v}$ ä¹‹é—´çš„è§’åº¦ $\theta$ ï¼Œå³

$$
\cos \theta = \frac{\langle \mathbf{u}, \mathbf{v} \rangle}{\|\mathbf{u}\| \, \|\mathbf{v}\|}.
$$

Thus, two vectors are orthogonal if $\langle \mathbf{u}, \mathbf{v} \rangle = 0$.
å› æ­¤ï¼Œè‹¥ $\langle \mathbf{u}, \mathbf{v} \rangle = 0$ ï¼Œåˆ™ä¸¤ä¸ªå‘é‡æ­£äº¤ã€‚

### Examples
ç¤ºä¾‹

Example 7.1.1. In $\mathbb{R}^2$, with $\mathbf{u} = (1,2)$, $\mathbf{v} = (3,4)$:
ä¾‹ 7.1.1ã€‚ åœ¨ $\mathbb{R}^2$ ä¸­ï¼Œä½¿ç”¨ $\mathbf{u} = (1,2)$ ã€ $\mathbf{v} = (3,4)$ ï¼š

$$
\langle \mathbf{u}, \mathbf{v} \rangle = 1\cdot 3 + 2\cdot 4 = 11.
$$

 

$$
\|\mathbf{u}\| = \sqrt{1^2 + 2^2} = \sqrt{5}, \quad \|\mathbf{v}\| = \sqrt{3^2 + 4^2} = 5.
$$

So,
æ‰€ä»¥ï¼Œ

$$
\cos \theta = \frac{11}{\sqrt{5}\cdot 5}.
$$

Example 7.1.2. In the function space $C[0,1]$, the inner product
ä¾‹ 7.1.2ã€‚ åœ¨å‡½æ•°ç©ºé—´ $C[0,1]$ ä¸­ï¼Œå†…ç§¯

$$
\langle f, g \rangle = \int_0^1 f(x) g(x)\, dx
$$

defines a length
å®šä¹‰é•¿åº¦

$$
\|f\| = \sqrt{\int_0^1 f(x)^2 dx}.
$$

This generalizes geometry to infinite-dimensional spaces.
è¿™å°†å‡ ä½•å­¦æ¨å¹¿åˆ°æ— é™ç»´ç©ºé—´ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

*   Inner product: measures similarity between vectors.
    å†…ç§¯ï¼šæµ‹é‡å‘é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚
*   Norm: length of a vector.
    èŒƒæ•°ï¼šå‘é‡çš„é•¿åº¦ã€‚
*   Angle: measure of alignment between two directions.
    è§’åº¦ï¼šä¸¤ä¸ªæ–¹å‘ä¹‹é—´çš„å¯¹é½åº¦é‡ã€‚

These concepts unify algebraic operations with geometric intuition.
è¿™äº›æ¦‚å¿µå°†ä»£æ•°è¿ç®—ä¸å‡ ä½•ç›´è§‰ç»Ÿä¸€èµ·æ¥ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Inner products and norms allow us to extend geometry into abstract vector spaces. They form the basis of orthogonality, projections, Fourier series, least squares approximation, and many applications in physics and machine learning.
å†…ç§¯å’ŒèŒƒæ•°ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†å‡ ä½•æ‰©å±•åˆ°æŠ½è±¡å‘é‡ç©ºé—´ã€‚å®ƒä»¬æ„æˆäº†æ­£äº¤æ€§ã€æŠ•å½±ã€å‚…é‡Œå¶çº§æ•°ã€æœ€å°äºŒä¹˜è¿‘ä¼¼ä»¥åŠç‰©ç†å­¦å’Œæœºå™¨å­¦ä¹ ä¸­è®¸å¤šåº”ç”¨çš„åŸºç¡€ã€‚

### Exercises 7.1
ç»ƒä¹  7.1

1.  Compute $\langle (2,-1,3), (1,4,0) \rangle$. Then find the angle between them.
    è®¡ç®— $\langle (2,-1,3), (1,4,0) \rangle$ ã€‚ç„¶åæ±‚å‡ºå®ƒä»¬ä¹‹é—´çš„è§’åº¦ã€‚
    
2.  Show that $\|(x,y)\| = \sqrt{x^2+y^2}$ satisfies the properties of a norm.
    è¯æ˜âˆ¥ ( ğ‘¥ , ğ‘¦ ) âˆ¥ = ğ‘¥ 2 + ğ‘¦ 2 âˆ¥(x,y)âˆ¥= x 2 +y 2 â€‹ æ»¡è¶³èŒƒæ•°çš„æ€§è´¨ã€‚
    
3.  In $\mathbb{R}^3$, verify that $(1,1,0)$ and $(1,-1,0)$ are orthogonal.
    åœ¨ $\mathbb{R}^3$ ä¸­ï¼ŒéªŒè¯ $(1,1,0)$ å’Œ $(1,-1,0)$ æ˜¯å¦æ­£äº¤ã€‚
    
4.  In $C[0,1]$, compute $\langle f,g \rangle$ for $f(x)=x$, $g(x)=1$.
    åœ¨ $C[0,1]$ ä¸­ï¼Œè®¡ç®— $f(x)=x$ ã€ $g(x)=1$ çš„ $\langle f,g \rangle$ ã€‚
    
5.  Prove the Cauchyâ€“Schwarz inequality:
    è¯æ˜æŸ¯è¥¿-æ–½ç“¦èŒ¨ä¸ç­‰å¼ï¼š
    
    $$
    |\langle \mathbf{u}, \mathbf{v} \rangle| \leq \|\mathbf{u}\| \, \|\mathbf{v}\|.
    $$
    

## 7.2 Orthogonal Projections
7.2 æ­£äº¤æŠ•å½±

One of the most useful applications of inner products is the notion of orthogonal projection. Projection allows us to approximate a vector by another lying in a subspace, minimizing error in the sense of distance. This idea underpins geometry, statistics, and numerical analysis.
å†…ç§¯æœ€æœ‰ç”¨çš„åº”ç”¨ä¹‹ä¸€æ˜¯æ­£äº¤æŠ•å½±çš„æ¦‚å¿µã€‚æŠ•å½±ä½¿æˆ‘ä»¬èƒ½å¤Ÿç”¨å­ç©ºé—´ä¸­çš„å¦ä¸€ä¸ªå‘é‡æ¥è¿‘ä¼¼ä¸€ä¸ªå‘é‡ï¼Œä»è€Œæœ€å°åŒ–è·ç¦»æ–¹å‘ä¸Šçš„è¯¯å·®ã€‚è¿™ä¸€æ€æƒ³æ˜¯å‡ ä½•ã€ç»Ÿè®¡å­¦å’Œæ•°å€¼åˆ†æçš„åŸºç¡€ã€‚

### Projection onto a Line
æŠ•å½±åˆ°çº¿ä¸Š

Let $\mathbf{u} \in \mathbb{R}^n$ be a nonzero vector. The line spanned by $\mathbf{u}$ is
ä»¤ $\mathbf{u} \in \mathbb{R}^n$ ä¸ºéé›¶å‘é‡ã€‚ $\mathbf{u}$ æ‰€æ„æˆçš„çº¿æ®µä¸º

$$
L = \{ c\mathbf{u} \mid c \in \mathbb{R} \}.
$$

Given a vector $\mathbf{v}$, the projection of $\mathbf{v}$ onto $\mathbf{u}$ is the vector in $L$ closest to $\mathbf{v}$. Geometrically, it is the shadow of $\mathbf{v}$ on the line.
ç»™å®šå‘é‡ $\mathbf{v}$ ï¼Œ $\mathbf{v}$ åœ¨ $\mathbf{u}$ ä¸Šçš„æŠ•å½±æ˜¯ $L$ ä¸­è·ç¦» $\mathbf{v}$ æœ€è¿‘çš„å‘é‡ã€‚ä»å‡ ä½•å­¦ä¸Šè®²ï¼Œå®ƒæ˜¯ $\mathbf{v}$ åœ¨çº¿ä¸Šçš„é˜´å½±ã€‚

The formula is
å…¬å¼æ˜¯

$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{\langle \mathbf{v}, \mathbf{u} \rangle}{\langle \mathbf{u}, \mathbf{u} \rangle} \, \mathbf{u}.
$$

The error vector $\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})$ is orthogonal to $\mathbf{u}$.
è¯¯å·®å‘é‡ $\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})$ ä¸ $\mathbf{u}$ æ­£äº¤ã€‚

### Example 7.2.1
ä¾‹ 7.2.1

Let $\mathbf{u} = (1,2)$, $\mathbf{v} = (3,1)$.
ä»¤ $\mathbf{u} = (1,2)$ ï¼Œ $\mathbf{v} = (3,1)$ ã€‚

$$
\langle \mathbf{v}, \mathbf{u} \rangle = 3\cdot 1 + 1\cdot 2 = 5, \quad\langle \mathbf{u}, \mathbf{u} \rangle = 1^2 + 2^2 = 5.
$$

So
æ‰€ä»¥

$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{5}{5}(1,2) = (1,2).
$$

The error vector is $(3,1) - (1,2) = (2,-1)$, which is orthogonal to $(1,2)$.
è¯¯å·®å‘é‡ä¸º $(3,1) - (1,2) = (2,-1)$ ï¼Œä¸ $(1,2)$ æ­£äº¤ã€‚

### Projection onto a Subspace
æŠ•å½±åˆ°å­ç©ºé—´

Suppose $W \subseteq \mathbb{R}^n$ is a subspace with orthonormal basis $\{ \mathbf{w}_1, \dots, \mathbf{w}_k \}$. The projection of a vector $\mathbf{v}$ onto $W$ is
å‡è®¾ $W \subseteq \mathbb{R}^n$ æ˜¯ä¸€ä¸ªå…·æœ‰æ­£äº¤åŸº $\{ \mathbf{w}_1, \dots, \mathbf{w}_k \}$ çš„å­ç©ºé—´ã€‚å‘é‡ $\mathbf{v}$ åœ¨ $W$ ä¸Šçš„æŠ•å½±ä¸º

$$
\text{proj}_{W}(\mathbf{v}) = \langle \mathbf{v}, \mathbf{w}_1 \rangle \mathbf{w}_1 + \cdots + \langle \mathbf{v}, \mathbf{w}_k \rangle \mathbf{w}_k.
$$

This is the unique vector in $W$ closest to $\mathbf{v}$. The difference $\mathbf{v} - \text{proj}_{W}(\mathbf{v})$ is orthogonal to all of $W$.
è¿™æ˜¯ $W$ ä¸­ä¸ $\mathbf{v}$ æœ€æ¥è¿‘çš„å”¯ä¸€å‘é‡ã€‚å·®å€¼ $\mathbf{v} - \text{proj}_{W}(\mathbf{v})$ ä¸æ‰€æœ‰ $W$ æ­£äº¤ã€‚

### Least Squares Approximation
æœ€å°äºŒä¹˜è¿‘ä¼¼

Orthogonal projection explains the method of least squares. To solve an overdetermined system $A\mathbf{x} \approx \mathbf{b}$, we seek the $\mathbf{x}$ that makes $A\mathbf{x}$ the projection of $\mathbf{b}$ onto the column space of $A$. This gives the normal equations
æ­£äº¤æŠ•å½±è§£é‡Šäº†æœ€å°äºŒä¹˜æ³•ã€‚ä¸ºäº†è§£å†³è¶…å®šé—®é¢˜ ç³»ç»Ÿ $A\mathbf{x} \approx \mathbf{b}$ ï¼Œæˆ‘ä»¬å¯»æ‰¾ $\mathbf{x}$ ï¼Œä½¿å¾— $A\mathbf{x}$ æˆä¸º $\mathbf{b}$ åœ¨ $A$ çš„åˆ—ç©ºé—´ä¸Šçš„æŠ•å½±ã€‚è¿™ç»™å‡ºäº†æ­£åˆ™æ–¹ç¨‹

$$
A^T A \mathbf{x} = A^T \mathbf{b}.
$$

Thus, least squares is just projection in disguise.
å› æ­¤ï¼Œæœ€å°äºŒä¹˜æ³•åªæ˜¯ä¼ªè£…çš„æŠ•å½±ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

*   Projection finds the closest point in a subspace to a given vector.
    æŠ•å½±æ‰¾åˆ°å­ç©ºé—´ä¸­è·ç¦»ç»™å®šå‘é‡æœ€è¿‘çš„ç‚¹ã€‚
*   It minimizes distance (error) in the sense of Euclidean norm.
    å®ƒæŒ‰ç…§æ¬§å‡ é‡Œå¾—èŒƒæ•°çš„æ„ä¹‰æœ€å°åŒ–è·ç¦»ï¼ˆè¯¯å·®ï¼‰ã€‚
*   Orthogonality ensures the error vector points directly away from the subspace.
    æ­£äº¤æ€§ç¡®ä¿è¯¯å·®å‘é‡ç›´æ¥æŒ‡å‘è¿œç¦»å­ç©ºé—´çš„æ–¹å‘ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Orthogonal projection is central in both pure and applied mathematics. It underlies the geometry of subspaces, the theory of Fourier series, regression in statistics, and approximation methods in numerical linear algebra. Whenever we fit data with a simpler model, projection is at work.
æ­£äº¤æŠ•å½±åœ¨çº¯æ•°å­¦å’Œåº”ç”¨æ•°å­¦ä¸­éƒ½è‡³å…³é‡è¦ã€‚å®ƒæ˜¯å­ç©ºé—´å‡ ä½•ã€å‚…é‡Œå¶çº§æ•°ç†è®ºã€ç»Ÿè®¡å­¦ä¸­çš„å›å½’ä»¥åŠæ•°å€¼çº¿æ€§ä»£æ•°ä¸­çš„è¿‘ä¼¼æ–¹æ³•çš„åŸºç¡€ã€‚æ¯å½“æˆ‘ä»¬ç”¨æ›´ç®€å•çš„æ¨¡å‹æ‹Ÿåˆæ•°æ®æ—¶ï¼ŒæŠ•å½±å°±ä¼šå‘æŒ¥ä½œç”¨ã€‚

### Exercises 7.2
ç»ƒä¹  7.2

1.  Compute the projection of $(2,3)$ onto the vector $(1,1)$.
    è®¡ç®— $(2,3)$ åˆ°å‘é‡ $(1,1)$ çš„æŠ•å½±ã€‚
2.  Show that $\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})$ is orthogonal to $\mathbf{u}$.
    è¯æ˜ $\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})$ ä¸ $\mathbf{u}$ æ­£äº¤ã€‚
3.  Let $W = \text{span}\{(1,0,0), (0,1,0)\} \subseteq \mathbb{R}^3$. Find the projection of $(1,2,3)$ onto $W$.
    ä»¤ $W = \text{span}\{(1,0,0), (0,1,0)\} \subseteq \mathbb{R}^3$ ã€‚æ±‚ $(1,2,3)$ åˆ° $W$ çš„æŠ•å½±ã€‚
4.  Explain why least squares fitting corresponds to projection onto the column space of $A$.
    è§£é‡Šä¸ºä»€ä¹ˆæœ€å°äºŒä¹˜æ‹Ÿåˆå¯¹åº”äº $A$ çš„åˆ—ç©ºé—´ä¸Šçš„æŠ•å½±ã€‚
5.  Prove that projection onto a subspace $W$ is unique: there is exactly one closest vector in $W$ to a given $\mathbf{v}$.
    è¯æ˜æŠ•å½±åˆ°å­ç©ºé—´ $W$ æ˜¯å”¯ä¸€çš„ï¼šåœ¨ $W$ ä¸­ï¼Œæœ‰ä¸”ä»…æœ‰ä¸€ä¸ªä¸ç»™å®š $\mathbf{v}$ æœ€æ¥è¿‘çš„å‘é‡ã€‚

## 7.3 Gramâ€“Schmidt Process
7.3 æ ¼æ‹‰å§†-æ–½å¯†ç‰¹è¿‡ç¨‹

The Gramâ€“Schmidt process is a systematic way to turn any linearly independent set of vectors into an orthonormal basis. This is especially useful because orthonormal bases simplify computations: inner products become simple coordinate comparisons, and projections take clean forms.
æ ¼æ‹‰å§†-æ–½å¯†ç‰¹è¿‡ç¨‹æ˜¯ä¸€ç§å°†ä»»æ„çº¿æ€§æ— å…³çš„å‘é‡é›†è½¬åŒ–ä¸ºæ­£äº¤åŸºçš„ç³»ç»Ÿæ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å°¤å…¶æœ‰ç”¨ï¼Œå› ä¸ºæ­£äº¤åŸºå¯ä»¥ç®€åŒ–è®¡ç®—ï¼šå†…ç§¯å˜æˆäº†ç®€å•çš„åæ ‡æ¯”è¾ƒï¼Œå¹¶ä¸”æŠ•å½±å‘ˆç°å‡ºæ¸…æ™°çš„å½¢å¼ã€‚

### The Idea
ç†å¿µ

Given a linearly independent set of vectors $\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\}$ in an inner product space, we want to construct an orthonormal set $\{\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n\}$ that spans the same subspace.
ç»™å®šå†…ç§¯ç©ºé—´ä¸­ä¸€ç»„çº¿æ€§æ— å…³çš„å‘é‡ $\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\}$ ï¼Œæˆ‘ä»¬æƒ³è¦æ„å»ºä¸€ä¸ªè·¨è¶ŠåŒä¸€å­ç©ºé—´çš„æ­£äº¤é›† $\{\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n\}$ ã€‚

We proceed step by step:
æˆ‘ä»¬ä¸€æ­¥æ­¥æ¥ï¼š

1.  Start with $\mathbf{v}_1$, normalize it to get $\mathbf{u}_1$.
    ä»ğ‘£å¼€å§‹ 1 v 1 â€‹ ï¼Œå°†å…¶æ ‡å‡†åŒ–å¾—åˆ°ğ‘¢ 1 u 1 â€‹ .
2.  Subtract from $\mathbf{v}_2$ its projection onto $\mathbf{u}_1$, leaving a vector orthogonal to $\mathbf{u}_1$. Normalize to get $\mathbf{u}_2$.
    ä»ğ‘£ä¸­å‡å» 2 v 2 â€‹ å®ƒåœ¨ğ‘¢ä¸Šçš„æŠ•å½± 1 u 1 â€‹ ï¼Œç•™ä¸‹ä¸€ä¸ªä¸ğ‘¢æ­£äº¤çš„å‘é‡ 1 u 1 â€‹ . æ ‡å‡†åŒ–å¾—åˆ°ğ‘¢ 2 u 2 â€‹ .
3.  For each $\mathbf{v}_k$, subtract projections onto all previously constructed $\mathbf{u}_1, \dots, \mathbf{u}_{k-1}$, then normalize.
    å¯¹äºæ¯ä¸ªğ‘£ ğ‘˜ v k â€‹ ï¼Œå‡å»æ‰€æœ‰å…ˆå‰æ„å»ºçš„ğ‘¢ä¸Šçš„æŠ•å½± 1 , â€¦ , ğ‘¢ ğ‘˜ âˆ’ 1 u 1 â€‹ ï¼Œâ€¦ï¼Œä½  kâˆ’1 â€‹ ï¼Œç„¶åæ ‡å‡†åŒ–ã€‚

### The Algorithm
ç®—æ³•

For $k = 1, 2, \dots, n$:
å¯¹äº $k = 1, 2, \dots, n$ ï¼š

$$
\mathbf{w}_k = \mathbf{v}_k - \sum_{j=1}^{k-1} \langle \mathbf{v}_k, \mathbf{u}_j \rangle \mathbf{u}_j,
$$

 

$$
\mathbf{u}_k = \frac{\mathbf{w}_k}{\|\mathbf{w}_k\|}.
$$

The result $\{\mathbf{u}_1, \dots, \mathbf{u}_n\}$ is an orthonormal basis of the span of the original vectors.
ç»“æœ $\{\mathbf{u}_1, \dots, \mathbf{u}_n\}$ æ˜¯åŸå§‹å‘é‡è·¨åº¦çš„æ­£äº¤åŸºã€‚

### Example 7.3.1
ä¾‹ 7.3.1

Take $\mathbf{v}_1 = (1,1,0), \ \mathbf{v}_2 = (1,0,1), \ \mathbf{v}_3 = (0,1,1)$ in $\mathbb{R}^3$.
åœ¨ $\mathbb{R}^3$ ä¸­ä¹˜å $\mathbf{v}_1 = (1,1,0), \ \mathbf{v}_2 = (1,0,1), \ \mathbf{v}_3 = (0,1,1)$ ã€‚

1.  Normalize $\mathbf{v}_1$:
    æ ‡å‡†åŒ–ğ‘£ 1 v 1 â€‹ :

$$
\mathbf{u}_1 = \frac{1}{\sqrt{2}}(1,1,0).
$$

2.  Subtract projection of $\mathbf{v}_2$ on $\mathbf{u}_1$:
    å‡å»ğ‘£çš„æŠ•å½± 2 v 2 â€‹ åœ¨ğ‘¢ 1 u 1 â€‹ :

$$
\mathbf{w}_2 = \mathbf{v}_2 - \langle \mathbf{v}_2,\mathbf{u}_1 \rangle \mathbf{u}_1.
$$

 

$$
\langle \mathbf{v}_2,\mathbf{u}_1 \rangle = \frac{1}{\sqrt{2}}(1\cdot 1 + 0\cdot 1 + 1\cdot 0) = \tfrac{1}{\sqrt{2}}.
$$

So
æ‰€ä»¥

$$
\mathbf{w}_2 = (1,0,1) - \tfrac{1}{\sqrt{2}}\cdot \tfrac{1}{\sqrt{2}}(1,1,0)= (1,0,1) - \tfrac{1}{2}(1,1,0)= \left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right).
$$

Normalize:
è§„èŒƒåŒ–ï¼š

$$
\mathbf{u}_2 = \frac{1}{\sqrt{\tfrac{1}{4}+\tfrac{1}{4}+1}} \left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right)= \frac{1}{\sqrt{\tfrac{3}{2}}}\left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right).
$$

3.  Subtract projections from $\mathbf{v}_3$:
    ä»ğ‘£ä¸­å‡å»æŠ•å½± 3 v 3 â€‹ :

$$
\mathbf{w}_3 = \mathbf{v}_3 - \langle \mathbf{v}_3,\mathbf{u}_1 \rangle \mathbf{u}_1 - \langle \mathbf{v}_3,\mathbf{u}_2 \rangle \mathbf{u}_2.
$$

After computing, normalize to obtain $\mathbf{u}_3$.
è®¡ç®—åï¼Œå½’ä¸€åŒ–å¾—åˆ°ğ‘¢ 3 u 3 â€‹ .

The result is an orthonormal basis of the span of $\{\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3\}$.
ç»“æœæ˜¯ $\{\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3\}$ è·¨åº¦çš„æ­£äº¤åŸºã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

Gramâ€“Schmidt is like straightening out a set of vectors: you start with the original directions and adjust each new vector to be perpendicular to all previous ones. Then you scale to unit length. The process ensures orthogonality while preserving the span.
æ ¼æ‹‰å§†-æ–½å¯†ç‰¹å˜æ¢å°±åƒæ‹‰ç›´ä¸€ç»„å‘é‡ï¼šä»åŸå§‹æ–¹å‘å¼€å§‹ï¼Œè°ƒæ•´æ¯ä¸ªæ–°å‘é‡ä½¿å…¶ä¸æ‰€æœ‰å…ˆå‰çš„å‘é‡å‚ç›´ã€‚ç„¶åç¼©æ”¾åˆ°å•ä½é•¿åº¦ã€‚è¿™ä¸ªè¿‡ç¨‹ç¡®ä¿äº†æ­£äº¤æ€§ï¼ŒåŒæ—¶ä¿ç•™äº†è·¨åº¦ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Orthonormal bases simplify inner products, projections, and computations in general. They make coordinate systems easier to work with and are crucial in numerical methods, QR decomposition, Fourier analysis, and statistics (orthogonal polynomials, principal component analysis).
æ­£äº¤åŸºå¯ä»¥ç®€åŒ–å†…ç§¯ã€æŠ•å½±å’Œä¸€èˆ¬è®¡ç®—ã€‚å®ƒä»¬ä½¿åæ ‡ç³»æ›´æ˜“äºä½¿ç”¨ï¼Œå¹¶ä¸”åœ¨æ•°å€¼æ–¹æ³•ã€QR åˆ†è§£ã€å‚…é‡Œå¶åˆ†æå’Œç»Ÿè®¡å­¦ï¼ˆæ­£äº¤å¤šé¡¹å¼ã€ä¸»æˆåˆ†åˆ†æï¼‰ä¸­è‡³å…³é‡è¦ã€‚

### Exercises 7.3
ç»ƒä¹  7.3

1.  Apply Gramâ€“Schmidt to $(1,0), (1,1)$ in $\mathbb{R}^2$.
    å¯¹ $\mathbb{R}^2$ ä¸­çš„ $(1,0), (1,1)$ åº”ç”¨ Gramâ€“Schmidt å…¬å¼ã€‚
2.  Orthogonalize $(1,1,1), (1,0,1)$ in $\mathbb{R}^3$.
    åœ¨ $\mathbb{R}^3$ ä¸­å¯¹ $(1,1,1), (1,0,1)$ è¿›è¡Œæ­£äº¤åŒ–ã€‚
3.  Prove that each step of Gramâ€“Schmidt yields a vector orthogonal to all previous ones.
    è¯æ˜ Gram-Schmidt çš„æ¯ä¸€æ­¥éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªä¸æ‰€æœ‰å‰é¢çš„å‘é‡æ­£äº¤çš„å‘é‡ã€‚
4.  Show that Gramâ€“Schmidt preserves the span of the original vectors.
    è¯æ˜ Gramâ€“Schmidt ä¿ç•™äº†åŸå§‹å‘é‡çš„è·¨åº¦ã€‚
5.  Explain how Gramâ€“Schmidt leads to the QR decomposition of a matrix.
    è§£é‡Š Gram-Schmidt å¦‚ä½•å¯¼è‡´çŸ©é˜µçš„ QR åˆ†è§£ã€‚

## 7.4 Orthonormal Bases
7.4 æ­£äº¤åŸº

An orthonormal basis is a basis of a vector space in which all vectors are both orthogonal to each other and have unit length. Such bases are the most convenient possible coordinate systems: computations involving inner products, projections, and norms become exceptionally simple.
æ­£äº¤åŸºæ˜¯å‘é‡ç©ºé—´ä¸­çš„ä¸€ç§åŸºï¼Œå…¶ä¸­æ‰€æœ‰å‘é‡å½¼æ­¤æ­£äº¤ä¸”å…·æœ‰å•ä½é•¿åº¦ã€‚è¿™æ ·çš„åŸºæ˜¯æœ€æ–¹ä¾¿çš„åæ ‡ç³»ï¼šæ¶‰åŠå†…ç§¯ã€æŠ•å½±å’ŒèŒƒæ•°çš„è®¡ç®—å˜å¾—å¼‚å¸¸ç®€å•ã€‚

### Definition
å®šä¹‰

A set of vectors $\{\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n\}$ in an inner product space $V$ is called an orthonormal basis if
å†…ç§¯ç©ºé—´ $V$ ä¸­çš„ä¸€ç»„å‘é‡ $\{\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n\}$ ç§°ä¸ºæ­£äº¤åŸºï¼Œè‹¥

1.  $\langle \mathbf{u}_i, \mathbf{u}_j \rangle = 0$ whenever $i \neq j$ (orthogonality),
    $\langle \mathbf{u}_i, \mathbf{u}_j \rangle = 0$ æ¯å½“ $i \neq j$ ï¼ˆæ­£äº¤æ€§ï¼‰
2.  $\|\mathbf{u}_i\| = 1$ for all $i$ (normalization),
    å¯¹æ‰€æœ‰ $i$ è¿›è¡Œ $\|\mathbf{u}_i\| = 1$ ï¼ˆè§„èŒƒåŒ–ï¼‰ï¼Œ
3.  The set spans $V$.
    è¯¥é›†åˆè·¨è¶Š $V$ ã€‚

### Examples
ç¤ºä¾‹

Example 7.4.1. In $\mathbb{R}^2$, the standard basis
ä¾‹ 7.4.1. åœ¨ $\mathbb{R}^2$ ä¸­ï¼Œæ ‡å‡†åŸºç¡€

$$
\mathbf{e}_1 = (1,0), \quad \mathbf{e}_2 = (0,1)
$$

is orthonormal under the dot product.
åœ¨ç‚¹ç§¯ä¸‹æ˜¯æ­£äº¤çš„ã€‚

Example 7.4.2. In $\mathbb{R}^3$, the standard basis
ä¾‹ 7.4.2. åœ¨ $\mathbb{R}^3$ ä¸­ï¼Œæ ‡å‡†åŸºç¡€

$$
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3 = (0,0,1)
$$

is orthonormal.
æ˜¯æ­£äº¤çš„ã€‚

Example 7.4.3. Fourier basis on functions:
ä¾‹ 7.4.3. å‡½æ•°çš„å‚…é‡Œå¶åŸºï¼š

$$
\{1, \cos x, \sin x, \cos 2x, \sin 2x, \dots\}
$$

is an orthogonal set in the space of square-integrable functions on $[-\pi,\pi]$ with inner product
æ˜¯ $[-\pi,\pi]$ ä¸Šå¹³æ–¹å¯ç§¯å‡½æ•°ç©ºé—´ä¸­çš„æ­£äº¤é›†ï¼Œå…·æœ‰å†…ç§¯

$$
\langle f,g \rangle = \int_{-\pi}^{\pi} f(x) g(x)\, dx.
$$

After normalization, it becomes an orthonormal basis.
ç»è¿‡å½’ä¸€åŒ–ä¹‹åï¼Œå®ƒå°±å˜æˆäº†æ­£äº¤åŸºã€‚

### Properties
ç‰¹æ€§

1.  Coordinate simplicity: If $\{\mathbf{u}_1,\dots,\mathbf{u}_n\}$ is an orthonormal basis of $V$, then any vector $\mathbf{v}\in V$ has coordinates
    åæ ‡ç®€å•æ€§ï¼šå¦‚æœ $\{\mathbf{u}_1,\dots,\mathbf{u}_n\}$ æ˜¯ $V$ çš„æ­£äº¤åŸºï¼Œåˆ™ä»»ä½•å‘é‡ $\mathbf{v}\in V$ éƒ½æœ‰åæ ‡
    
    $$
    [\mathbf{v}] = \begin{bmatrix} \langle \mathbf{v}, \mathbf{u}_1 \rangle \\ \vdots \\ \langle \mathbf{v}, \mathbf{u}_n \rangle \end{bmatrix}.
    $$
    
    That is, coordinates are just inner products.
    ä¹Ÿå°±æ˜¯è¯´ï¼Œåæ ‡åªæ˜¯å†…ç§¯ã€‚
    
2.  Parsevalâ€™s identity: For any $\mathbf{v} \in V$,
    å¸•å¡ç“¦å°”çš„èº«ä»½ï¼š å¯¹äºä»»æ„çš„ $\mathbf{v} \in V$ ï¼Œ
    
    $$
    \|\mathbf{v}\|^2 = \sum_{i=1}^n |\langle \mathbf{v}, \mathbf{u}_i \rangle|^2.
    $$
    
3.  Projections: The orthogonal projection onto the span of $\\{\mathbf{u}_1,\dots,\mathbf{u}_k\\}$ is
    é¢„æµ‹ï¼š ğ‘¢ è·¨åº¦ä¸Šçš„æ­£äº¤æŠ•å½± 1 , â€¦ , ğ‘¢ ğ‘˜ u 1 â€‹ ï¼Œâ€¦ï¼Œä½  k â€‹ æ˜¯
    
    $$
    \text{proj}(\mathbf{v}) = \sum_{i=1}^k \langle \mathbf{v}, \mathbf{u}_i \rangle \mathbf{u}_i.
    $$
    

### Constructing Orthonormal Bases
æ„é€ æ­£äº¤åŸº

*   Start with any linearly independent set, then apply the Gramâ€“Schmidt process to obtain an orthonormal set spanning the same subspace.
    ä»ä»»æ„çº¿æ€§æ— å…³é›†å¼€å§‹ï¼Œç„¶ååº”ç”¨ Gram-Schmidt è¿‡ç¨‹æ¥è·å–è·¨è¶Šç›¸åŒå­ç©ºé—´çš„æ­£äº¤é›†ã€‚
*   In practice, orthonormal bases are often chosen for numerical stability and simplicity of computation.
    åœ¨å®è·µä¸­ï¼Œé€šå¸¸é€‰æ‹©æ­£äº¤åŸºæ¥å®ç°æ•°å€¼ç¨³å®šæ€§å’Œè®¡ç®—ç®€å•æ€§ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

An orthonormal basis is like a perfectly aligned and equally scaled coordinate system. Distances and angles are computed directly using coordinates without correction factors. They are the ideal rulers of linear algebra.
æ­£äº¤åŸºå°±åƒä¸€ä¸ªå®Œç¾å¯¹é½ä¸”ç­‰æ¯”ä¾‹ç¼©æ”¾çš„åæ ‡ç³»ã€‚è·ç¦»å’Œè§’åº¦ç›´æ¥ä½¿ç”¨åæ ‡è®¡ç®—ï¼Œæ— éœ€æ ¡æ­£å› å­ã€‚å®ƒä»¬æ˜¯çº¿æ€§ä»£æ•°çš„ç†æƒ³æ ‡å°ºã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Orthonormal bases simplify every aspect of linear algebra: solving systems, computing projections, expanding functions, diagonalizing symmetric matrices, and working with Fourier series. In data science, principal component analysis produces orthonormal directions capturing maximum variance.
æ­£äº¤åŸºç®€åŒ–äº†çº¿æ€§ä»£æ•°çš„å„ä¸ªæ–¹é¢ï¼šæ±‚è§£ç³»ç»Ÿã€è®¡ç®—æŠ•å½±ã€å±•å¼€å‡½æ•°ã€å¯¹è§’åŒ–å¯¹ç§°çŸ©é˜µä»¥åŠå¤„ç†å‚…é‡Œå¶çº§æ•°ã€‚åœ¨æ•°æ®ç§‘å­¦ä¸­ï¼Œä¸»æˆåˆ†åˆ†æå¯ä»¥ç”Ÿæˆæ­£äº¤æ–¹å‘ï¼Œä»è€Œæ•æ‰æœ€å¤§æ–¹å·®ã€‚

### Exercises 7.4
ç»ƒä¹  7.4

1.  Verify that $(1/\\sqrt{2})(1,1)$ and $(1/\\sqrt{2})(1,-1)$ form an orthonormal basis of $\mathbb{R}^2$.
    éªŒè¯ $(1/\\sqrt{2})(1,1)$ å’Œ $(1/\\sqrt{2})(1,-1)$ æ˜¯å¦æ„æˆ $\mathbb{R}^2$ çš„æ­£äº¤åŸºã€‚
2.  Express $(3,4)$ in terms of the orthonormal basis $\{(1/\\sqrt{2})(1,1), (1/\\sqrt{2})(1,-1)\}$.
    ç”¨æ­£äº¤åŸº $\{(1/\\sqrt{2})(1,1), (1/\\sqrt{2})(1,-1)\}$ è¡¨ç¤º $(3,4)$ ã€‚
3.  Prove Parsevalâ€™s identity for $\\mathbb{R}^n$ with the dot product.
    ä½¿ç”¨ç‚¹ç§¯è¯æ˜ $\\mathbb{R}^n$ çš„å¸•å¡ç“¦å°”æ’ç­‰å¼ã€‚
4.  Find an orthonormal basis for the plane $x+y+z=0$ in $\\mathbb{R}^3$.
    åœ¨ $\\mathbb{R}^3$ ä¸­æ‰¾å‡ºå¹³é¢ $x+y+z=0$ çš„æ­£äº¤åŸºã€‚
5.  Explain why orthonormal bases are numerically more stable than arbitrary bases in computations.
    è§£é‡Šä¸ºä»€ä¹ˆæ­£äº¤åŸºåœ¨è®¡ç®—ä¸­æ¯”ä»»æ„åŸºåœ¨æ•°å€¼ä¸Šæ›´ç¨³å®šã€‚

# Chapter 8. Eigenvalues and eigenvectors
ç¬¬ 8 ç«  ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡

## 8.1 Definitions and Intuition
8.1 å®šä¹‰å’Œç›´è§‰

The concepts of eigenvalues and eigenvectors reveal the most fundamental behavior of linear transformations. They identify the special directions in which a transformation acts by simple stretching or compressing, without rotation or distortion.
ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡çš„æ¦‚å¿µæ­ç¤ºäº†çº¿æ€§å˜æ¢æœ€åŸºæœ¬çš„è¡Œä¸ºã€‚å®ƒä»¬é€šè¿‡ç®€å•çš„æ‹‰ä¼¸æˆ–å‹ç¼©ï¼ˆä¸è¿›è¡Œæ—‹è½¬æˆ–å˜å½¢ï¼‰æ¥è¯†åˆ«å˜æ¢æ‰€ä½œç”¨çš„ç‰¹å®šæ–¹å‘ã€‚

### Definition
å®šä¹‰

Let $T: V \to V$ be a linear transformation on a vector space $V$. A nonzero vector $\mathbf{v} \in V$ is called an eigenvector of $T$ if
ä»¤ $T: V \to V$ ä¸ºå‘é‡ç©ºé—´ $V$ ä¸Šçš„çº¿æ€§å˜æ¢ã€‚éé›¶å‘é‡ $\mathbf{v} \in V$ ç§°ä¸º $T$ çš„ç‰¹å¾å‘é‡ï¼Œè‹¥

$$
T(\mathbf{v}) = \lambda \mathbf{v}
$$

for some scalar $\lambda \in \mathbb{R}$ (or $\mathbb{C}$). The scalar $\lambda$ is the eigenvalue corresponding to $\mathbf{v}$.
æŸä¸ªæ ‡é‡ $\lambda \in \mathbb{R}$ ï¼ˆæˆ– $\mathbb{C}$ ï¼‰ã€‚æ ‡é‡ $\lambda$ æ˜¯å¯¹åº”äº $\mathbf{v}$ çš„ç‰¹å¾å€¼ã€‚

Equivalently, if $A$ is the matrix of $T$, then eigenvalues and eigenvectors satisfy
ç­‰ä»·åœ°ï¼Œå¦‚æœ $A$ æ˜¯ $T$ çš„çŸ©é˜µï¼Œåˆ™ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æ»¡è¶³

$$
A\mathbf{v} = \lambda \mathbf{v}.
$$

### Basic Examples
åŸºæœ¬ç¤ºä¾‹

Example 8.1.1. Let
ä¾‹ 8.1.1. è®¾

$$
A = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}.
$$

Then
ç„¶å

$$
A(1,0)^T = 2(1,0)^T, \quad A(0,1)^T = 3(0,1)^T.
$$

So $(1,0)$ is an eigenvector with eigenvalue $2$, and $(0,1) is an eigenvector with eigenvalue \\3$.
å› æ­¤ $(1,0)$ æ˜¯ç‰¹å¾å€¼ä¸º $2 çš„ç‰¹å¾å‘é‡ï¼Œ $, and $ (0,1) æ˜¯ç‰¹å¾å€¼ä¸º \\ 3$ çš„ç‰¹å¾å‘é‡ ã€‚

Example 8.1.2. Rotation matrix in $\mathbb{R}^2$:
ä¾‹ 8.1.2ã€‚ $\mathbb{R}^2$ ä¸­çš„æ—‹è½¬çŸ©é˜µï¼š

$$
R_\theta = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}.
$$

If $\theta \neq 0, \pi$, $R_\theta$ has no real eigenvalues: every vector is rotated, not scaled. Over $\mathbb{C}$, however, it has eigenvalues $e^{i\theta}, e^{-i\theta}$.
å¦‚æœ $\theta \neq 0, \pi$ ï¼Œğ‘… ğœƒ R Î¸ â€‹ æ²¡æœ‰å®æ•°ç‰¹å¾å€¼ï¼šæ¯ä¸ªå‘é‡éƒ½ç»è¿‡æ—‹è½¬ï¼Œè€Œä¸æ˜¯ç¼©æ”¾ã€‚ç„¶è€Œï¼Œåœ¨ $\mathbb{C}$ ä¸Šï¼Œå®ƒçš„ç‰¹å¾å€¼ä¸º $e^{i\theta}, e^{-i\theta}$ ã€‚

### Algebraic Formulation
ä»£æ•°å…¬å¼

Eigenvalues arise from solving the characteristic equation:
ç‰¹å¾å€¼ç”±æ±‚è§£ç‰¹å¾æ–¹ç¨‹å¾—å‡ºï¼š

$$
\det(A - \lambda I) = 0.
$$

This polynomial in $\lambda$ is the characteristic polynomial. Its roots are the eigenvalues.
$\lambda$ ä¸­çš„è¿™ä¸ªå¤šé¡¹å¼æ˜¯ç‰¹å¾å¤šé¡¹å¼ã€‚å®ƒçš„æ ¹å°±æ˜¯ç‰¹å¾å€¼ã€‚

### Geometric Intuition
å‡ ä½•ç›´è§‰

*   Eigenvectors are directions that remain unchanged in orientation under a transformation; only their length is scaled.
    ç‰¹å¾å‘é‡æ˜¯åœ¨å˜æ¢ä¸‹æ–¹å‘ä¿æŒä¸å˜çš„æ–¹å‘ï¼›åªæœ‰å®ƒä»¬çš„é•¿åº¦è¢«ç¼©æ”¾ã€‚
*   Eigenvalues tell us the scaling factor along those directions.
    ç‰¹å¾å€¼å‘Šè¯‰æˆ‘ä»¬æ²¿è¿™äº›æ–¹å‘çš„ç¼©æ”¾å› å­ã€‚
*   If a matrix has many independent eigenvectors, it can often be simplified (diagonalized) by changing basis.
    å¦‚æœçŸ©é˜µå…·æœ‰è®¸å¤šç‹¬ç«‹çš„ç‰¹å¾å‘é‡ï¼Œåˆ™é€šå¸¸å¯ä»¥é€šè¿‡æ”¹å˜åŸºæ¥ç®€åŒ–ï¼ˆå¯¹è§’åŒ–ï¼‰ã€‚

### Applications in Geometry and Science
å‡ ä½•å’Œç§‘å­¦ä¸­çš„åº”ç”¨

*   Stretching along principal axes of an ellipse (quadratic forms).
    æ²¿æ¤­åœ†çš„ä¸»è½´æ‹‰ä¼¸ï¼ˆäºŒæ¬¡å‹ï¼‰ã€‚
*   Stable directions of dynamical systems.
    åŠ¨åŠ›ç³»ç»Ÿçš„ç¨³å®šæ–¹å‘ã€‚
*   Principal components in statistics and machine learning.
    ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ä¸­çš„ä¸»è¦æˆåˆ†ã€‚
*   Quantum mechanics, where observables correspond to operators with eigenvalues.
    é‡å­åŠ›å­¦ï¼Œå…¶ä¸­å¯è§‚æµ‹é‡å¯¹åº”äºå…·æœ‰ç‰¹å¾å€¼çš„ç®—å­ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Eigenvalues and eigenvectors are a bridge between algebra and geometry. They provide a lens for understanding linear transformations in their simplest form. Nearly every application of linear algebra-differential equations, statistics, physics, computer science-relies on eigen-analysis.
ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æ˜¯ä»£æ•°å’Œå‡ ä½•ä¹‹é—´çš„æ¡¥æ¢ã€‚å®ƒä»¬ä¸ºç†è§£æœ€ç®€å½¢å¼çš„çº¿æ€§å˜æ¢æä¾›äº†ä¸€ä¸ªè§†è§’ã€‚å‡ ä¹æ‰€æœ‰çº¿æ€§ä»£æ•°çš„åº”ç”¨â€”â€”å¾®åˆ†æ–¹ç¨‹ã€ç»Ÿè®¡å­¦ã€ç‰©ç†å­¦ã€è®¡ç®—æœºç§‘å­¦â€”â€”éƒ½ä¾èµ–äºç‰¹å¾åˆ†æã€‚

### Exercises 8.1
ç»ƒä¹  8.1

1.  Find the eigenvalues and eigenvectors of $\begin{bmatrix} 4 & 0 \\ 0 & -1 \end{bmatrix}$.
    æ‰¾åˆ°ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ $\begin{bmatrix} 4 & 0 \\ 0 & -1 \end{bmatrix}$ .
2.  Show that every scalar multiple of an eigenvector is again an eigenvector for the same eigenvalue.
    è¯æ˜ç‰¹å¾å‘é‡çš„æ¯ä¸ªæ ‡é‡å€æ•°åˆæ˜¯åŒä¸€ç‰¹å¾å€¼çš„ç‰¹å¾å‘é‡ã€‚
3.  Verify that the rotation matrix $R_\theta$ has no real eigenvalues unless $\theta = 0$ or $\pi$.
    éªŒè¯æ—‹è½¬çŸ©é˜µğ‘… ğœƒ R Î¸ â€‹ é™¤é $\theta = 0$ æˆ– $\pi$ ï¼Œå¦åˆ™æ²¡æœ‰å®æ•°ç‰¹å¾å€¼ã€‚
4.  Compute the characteristic polynomial of $\begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}$.
    è®¡ç®—ç‰¹å¾å¤šé¡¹å¼ $\begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}$ .
5.  Explain geometrically what eigenvectors and eigenvalues represent for the shear matrix $\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$.
    ä»å‡ ä½•è§’åº¦è§£é‡Šç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼å¯¹äºå‰ªåˆ‡çŸ©é˜µçš„æ„ä¹‰ $\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$ .

## 8.2 Diagonalization
8.2 å¯¹è§’åŒ–

A central goal in linear algebra is to simplify the action of a matrix by choosing a good basis. Diagonalization is the process of rewriting a matrix so that it acts by simple scaling along independent directions. This makes computations such as powers, exponentials, and solving differential equations far easier.
çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒç›®æ ‡æ˜¯é€šè¿‡é€‰æ‹©åˆé€‚çš„åŸºæ¥ç®€åŒ–çŸ©é˜µçš„è¿ç®—ã€‚å¯¹è§’åŒ–æ˜¯å°†çŸ©é˜µé‡å†™ï¼Œä½¿å…¶èƒ½å¤Ÿæ²¿ç‹¬ç«‹æ–¹å‘è¿›è¡Œç®€å•çš„ç¼©æ”¾ã€‚è¿™ä½¿å¾—å¹‚ã€æŒ‡æ•°å’Œå¾®åˆ†æ–¹ç¨‹ç­‰è®¡ç®—å˜å¾—æ›´åŠ å®¹æ˜“ã€‚

### Definition
å®šä¹‰

A square matrix $A \in \mathbb{R}^{n \times n}$ is diagonalizable if there exists an invertible matrix $P$ such that
å¦‚æœå­˜åœ¨å¯é€†çŸ©é˜µ $P$ å¹¶ä¸”æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼Œåˆ™æ–¹é˜µ $A \in \mathbb{R}^{n \times n}$ å¯å¯¹è§’åŒ–

$$
P^{-1} A P = D,
$$

where $D$ is a diagonal matrix.
å…¶ä¸­ $D$ æ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µã€‚

The diagonal entries of $D$ are eigenvalues of $A$, and the columns of $P$ are the corresponding eigenvectors.
$D$ çš„å¯¹è§’çº¿é¡¹æ˜¯ $A$ çš„ç‰¹å¾å€¼ï¼Œ $P$ çš„åˆ—æ˜¯ç›¸åº”çš„ç‰¹å¾å‘é‡ã€‚

### When is a Matrix Diagonalizable?
çŸ©é˜µä½•æ—¶å¯å¯¹è§’åŒ–ï¼Ÿ

*   A matrix is diagonalizable if it has $n$ linearly independent eigenvectors.
    å¦‚æœçŸ©é˜µå…·æœ‰ $n$ ä¸ªçº¿æ€§æ— å…³çš„ç‰¹å¾å‘é‡ï¼Œåˆ™è¯¥çŸ©é˜µå¯å¯¹è§’åŒ–ã€‚
*   Equivalently, the sum of the dimensions of its eigenspaces equals $n$.
    ç­‰æ•ˆåœ°ï¼Œå…¶ç‰¹å¾ç©ºé—´çš„ç»´æ•°ä¹‹å’Œç­‰äº $n$ ã€‚
*   Symmetric matrices (over $\mathbb{R}$) are always diagonalizable, with an orthonormal basis of eigenvectors.
    å¯¹ç§°çŸ©é˜µï¼ˆåœ¨ $\mathbb{R}$ ä¸Šï¼‰å§‹ç»ˆå¯å¯¹è§’åŒ–ï¼Œä¸”å…·æœ‰ç‰¹å¾å‘é‡çš„æ­£äº¤åŸºã€‚

### Example 8.2.1
ä¾‹ 8.2.1

Let
è®©

$$
A = \begin{bmatrix} 4 & 1 \\ 0 & 2 \end{bmatrix}.
$$

1.  Characteristic polynomial:
    ç‰¹å¾å¤šé¡¹å¼ï¼š

$$
\det(A - \lambda I) = (4-\lambda)(2-\lambda).
$$

So eigenvalues are $\lambda_1 = 4$, $\lambda_2 = 2$.
æ‰€ä»¥ç‰¹å¾å€¼æ˜¯ $\lambda_1 = 4$ ï¼Œ $\lambda_2 = 2$ ã€‚

2.  Eigenvectors:
    ç‰¹å¾å‘é‡ï¼š

*   For $\lambda = 4$, solve $(A-4I)\mathbf{v}=0$: $\begin{bmatrix} 0 & 1 \\ 0 & -2 \end{bmatrix}\mathbf{v} = 0$, giving $\mathbf{v}_1 = (1,0)$.
    å¯¹äº $\lambda = 4$ ï¼Œæ±‚è§£ $(A-4I)\mathbf{v}=0$ ï¼š $\begin{bmatrix} 0 & 1 \\ 0 & -2 \end{bmatrix}\mathbf{v} = 0$ ï¼Œå¾—åˆ° $\mathbf{v}_1 = (1,0)$ ã€‚
*   For $\lambda = 2$: $(A-2I)\mathbf{v}=0$, giving $\mathbf{v}_2 = (1,-2)$.
    å¯¹äº $\lambda = 2$ ï¼š $(A-2I)\mathbf{v}=0$ ï¼Œç»™å‡º $\mathbf{v}_2 = (1,-2)$ ã€‚

3.  Construct $P = \begin{bmatrix} 1 & 1 \\ 0 & -2 \end{bmatrix}$. Then
    æ„é€  $P = \begin{bmatrix} 1 & 1 \\ 0 & -2 \end{bmatrix}$ ã€‚ç„¶å

$$
P^{-1} A P = \begin{bmatrix} 4 & 0 \\ 0 & 2 \end{bmatrix}.
$$

Thus, $A$ is diagonalizable.
å› æ­¤ï¼Œ $A$ æ˜¯å¯å¯¹è§’åŒ–çš„ã€‚

### Why Diagonalize?
ä¸ºä»€ä¹ˆè¦å¯¹è§’åŒ–ï¼Ÿ

*   Computing powers: If $A = P D P^{-1}$, then
    è®¡ç®—èƒ½åŠ›ï¼š å¦‚æœ $A = P D P^{-1}$ ï¼Œåˆ™
    
    $$
    A^k = P D^k P^{-1}.
    $$
    
    Since $D$ is diagonal, $D^k$ is easy to compute.
    ç”±äº $D$ æ˜¯å¯¹è§’çº¿ï¼Œå› æ­¤ $D^k$ å¾ˆå®¹æ˜“è®¡ç®—ã€‚
    
*   Matrix exponentials: $e^A = P e^D P^{-1}$, useful in solving differential equations.
    çŸ©é˜µæŒ‡æ•°ï¼š $e^A = P e^D P^{-1}$ ï¼Œæœ‰åŠ©äºè§£å†³å¾®åˆ†æ–¹ç¨‹ã€‚
    
*   Understanding geometry: Diagonalization reveals the directions along which a transformation stretches or compresses space independently.
    ç†è§£å‡ ä½•ï¼šå¯¹è§’åŒ–æ­ç¤ºäº†å˜æ¢ç‹¬ç«‹æ‹‰ä¼¸æˆ–å‹ç¼©ç©ºé—´çš„æ–¹å‘ã€‚
    

### Non-Diagonalizable Example
ä¸å¯å¯¹è§’åŒ–çš„ä¾‹å­

Not all matrices can be diagonalized.
å¹¶éæ‰€æœ‰çŸ©é˜µéƒ½å¯ä»¥å¯¹è§’åŒ–ã€‚

$$
A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}
$$

has only one eigenvalue $\lambda = 1$, with eigenspace dimension 1. Since $n=2$ but we only have 1 independent eigenvector, $A$ is not diagonalizable.
åªæœ‰ä¸€ä¸ªç‰¹å¾å€¼ $\lambda = 1$ ï¼Œç‰¹å¾ç©ºé—´ç»´æ•°ä¸º 1ã€‚ç”±äº $n=2$ ä½†æˆ‘ä»¬åªæœ‰ 1 ä¸ªç‹¬ç«‹ç‰¹å¾å‘é‡ï¼Œå› æ­¤ $A$ ä¸å¯å¯¹è§’åŒ–ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

Diagonalization means we have found a basis of eigenvectors. In this basis, the matrix acts by simple scaling along each coordinate axis. It transforms complicated motion into independent 1D motions.
å¯¹è§’åŒ–æ„å‘³ç€æˆ‘ä»¬æ‰¾åˆ°äº†ç‰¹å¾å‘é‡çš„åŸºã€‚åœ¨æ­¤åŸºä¸Šï¼ŒçŸ©é˜µé€šè¿‡æ²¿æ¯ä¸ªåæ ‡è½´è¿›è¡Œç®€å•çš„ç¼©æ”¾æ¥å‘æŒ¥ä½œç”¨ã€‚å®ƒå°†å¤æ‚çš„è¿åŠ¨è½¬åŒ–ä¸ºç‹¬ç«‹çš„ä¸€ç»´è¿åŠ¨ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Diagonalization is a cornerstone of linear algebra. It simplifies computation, reveals structure, and is the starting point for the spectral theorem, Jordan form, and many applications in physics, engineering, and data science.
å¯¹è§’åŒ–æ˜¯çº¿æ€§ä»£æ•°çš„åŸºçŸ³ã€‚å®ƒç®€åŒ–äº†è®¡ç®—ï¼Œæ­ç¤ºäº†ç»“æ„ï¼Œå¹¶ä¸”æ˜¯è°±å®šç†ã€è‹¥å°”å½“å½¢å¼ä»¥åŠç‰©ç†ã€å·¥ç¨‹å’Œæ•°æ®ç§‘å­¦ä¸­è®¸å¤šåº”ç”¨çš„èµ·ç‚¹ã€‚

### Exercises 8.2
ç»ƒä¹  8.2

1.  Diagonalize
    å¯¹è§’åŒ–
    
    $$
    A = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}.
    $$
    
2.  Determine whether
    ç¡®å®šæ˜¯å¦
    
    $$
    A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}
    $$
    
    is diagonalizable. Why or why not?
    æ˜¯å¯å¯¹è§’åŒ–çš„ã€‚ä¸ºä»€ä¹ˆæˆ–ä¸ºä»€ä¹ˆä¸ï¼Ÿ
    
3.  Find $A^5$ for
    æŸ¥æ‰¾ $A^5$
    
    $$
    A = \begin{bmatrix} 4 & 1 \\ 0 & 2 \end{bmatrix}
    $$
    
    using diagonalization.
    ä½¿ç”¨å¯¹è§’åŒ–ã€‚
    
4.  Show that any $n \times n$ matrix with $n$ distinct eigenvalues is diagonalizable.
    è¯æ˜ä»»ä½•å…·æœ‰ $n$ ä¸ªä¸åŒç‰¹å¾å€¼çš„ $n \times n$ çŸ©é˜µéƒ½æ˜¯å¯å¯¹è§’åŒ–çš„ã€‚
    
5.  Explain why real symmetric matrices are always diagonalizable.
    è§£é‡Šä¸ºä»€ä¹ˆå®å¯¹ç§°çŸ©é˜µæ€»æ˜¯å¯å¯¹è§’åŒ–çš„ã€‚
    

## 8.3 Characteristic Polynomials
8.3 ç‰¹å¾å¤šé¡¹å¼

The key to finding eigenvalues is the characteristic polynomial of a matrix. This polynomial encodes the values of $\lambda$ for which the matrix $A - \lambda I$ fails to be invertible.
å¯»æ‰¾ç‰¹å¾å€¼çš„å…³é”®æ˜¯çŸ©é˜µçš„ç‰¹å¾å¤šé¡¹å¼ã€‚è¯¥å¤šé¡¹å¼å¯¹å€¼è¿›è¡Œç¼–ç  çŸ©é˜µ $A - \lambda I$ ä¸å¯é€†ï¼Œå…¶ä¸­ $\lambda$ ã€‚

### Definition
å®šä¹‰

For an $n \times n$ matrix $A$, the characteristic polynomial is
å¯¹äº $n \times n$ çŸ©é˜µ $A$ ï¼Œç‰¹å¾å¤šé¡¹å¼ä¸º

$$
p_A(\lambda) = \det(A - \lambda I).
$$

The roots of $p_A(\lambda)$ are the eigenvalues of $A$.
$p_A(\lambda)$ çš„æ ¹æ˜¯ $A$ çš„ç‰¹å¾å€¼ã€‚

### Examples
ç¤ºä¾‹

Example 8.3.1. Let
ä¾‹ 8.3.1. è®¾

$$
A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}.
$$

Then
ç„¶å

$$
p_A(\lambda) = \det\!\begin{bmatrix} 2-\lambda & 1 \\ 1 & 2-\lambda \end{bmatrix}= (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3.
$$

Thus eigenvalues are $\lambda = 1, 3$.
å› æ­¤ç‰¹å¾å€¼ä¸º $\lambda = 1, 3$ ã€‚

Example 8.3.2. For
ä¾‹ 8.3.2. å¯¹äº

$$
A = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}
$$

(rotation by 90Â°),
ï¼ˆæ—‹è½¬ 90Â°ï¼‰ï¼Œ

$$
p_A(\lambda) = \det\!\begin{bmatrix} -\lambda & -1 \\ 1 & -\lambda \end{bmatrix}= \lambda^2 + 1.
$$

Eigenvalues are $\lambda = \pm i$. No real eigenvalues exist, consistent with pure rotation.
ç‰¹å¾å€¼ä¸º $\lambda = \pm i$ ã€‚ä¸å­˜åœ¨å®æ•°ç‰¹å¾å€¼ï¼Œä¸çº¯æ—‹è½¬ä¸€è‡´ã€‚

Example 8.3.3. For a triangular matrix
ä¾‹ 8.3.3. å¯¹äºä¸‰è§’çŸ©é˜µ

$$
A = \begin{bmatrix} 2 & 1 & 0 \\ 0 & 3 & 5 \\ 0 & 0 & 4 \end{bmatrix},
$$

the determinant is simply the product of diagonal entries minus $\lambda$:
è¡Œåˆ—å¼ä»…ä»…æ˜¯å¯¹è§’çº¿é¡¹çš„ä¹˜ç§¯å‡å» $\lambda$ ï¼š

$$
p_A(\lambda) = (2-\lambda)(3-\lambda)(4-\lambda).
$$

So eigenvalues are 2,3,4.
æ‰€ä»¥ç‰¹å¾å€¼ä¸º 2,3,4 ã€‚

### Properties
ç‰¹æ€§

1.  The characteristic polynomial of an $n \times n$ matrix has degree $n$.
    $n \times n$ çŸ©é˜µçš„ç‰¹å¾å¤šé¡¹å¼çš„åº¦ä¸º $n$ ã€‚
    
2.  The sum of the eigenvalues (counted with multiplicity) equals the trace of $A$:
    ç‰¹å¾å€¼ï¼ˆæŒ‰é‡æ•°è®¡ç®—ï¼‰çš„å’Œç­‰äº $A$ çš„è¿¹ï¼š
    
    $$
    \text{tr}(A) = \lambda_1 + \cdots + \lambda_n.
    $$
    
3.  The product of the eigenvalues equals the determinant of $A$:
    ç‰¹å¾å€¼çš„ä¹˜ç§¯ç­‰äº $A$ çš„è¡Œåˆ—å¼ï¼š
    
    $$
    \det(A) = \lambda_1 \cdots \lambda_n.
    $$
    
4.  Similar matrices have the same characteristic polynomial, hence the same eigenvalues.
    ç›¸ä¼¼çš„çŸ©é˜µå…·æœ‰ç›¸åŒçš„ç‰¹å¾å¤šé¡¹å¼ï¼Œå› æ­¤å…·æœ‰ç›¸åŒçš„ç‰¹å¾å€¼ã€‚
    

### Geometric Interpretation
å‡ ä½•è§£é‡Š

The characteristic polynomial captures when $A - \lambda I$ collapses space: its determinant is zero precisely when the transformation $A - \lambda I$ is singular. Thus, eigenvalues mark the critical scalings where the matrix loses invertibility.
ç‰¹å¾å¤šé¡¹å¼æ•æ‰äº† $A - \lambda I$ ä½•æ—¶ä½¿ç©ºé—´åç¼©ï¼šå½“å˜æ¢ $A - \lambda I$ ä¸ºå¥‡å¼‚æ—¶ï¼Œå…¶è¡Œåˆ—å¼æ°å¥½ä¸ºé›¶ã€‚å› æ­¤ï¼Œç‰¹å¾å€¼æ ‡è®°äº†çŸ©é˜µå¤±å»å¯é€†æ€§çš„ä¸´ç•Œå°ºåº¦ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Characteristic polynomials provide the computational tool to extract eigenvalues. They connect matrix invariants (trace and determinant) with geometry, and form the foundation for diagonalization, spectral theorems, and stability analysis in dynamical systems.
ç‰¹å¾å¤šé¡¹å¼æä¾›äº†æå–ç‰¹å¾å€¼çš„è®¡ç®—å·¥å…·ã€‚å®ƒä»¬å°†çŸ©é˜µä¸å˜é‡ï¼ˆè¿¹å’Œè¡Œåˆ—å¼ï¼‰ä¸å‡ ä½•è”ç³»èµ·æ¥ï¼Œå¹¶æ„æˆäº†åŠ¨åŠ›ç³»ç»Ÿä¸­å¯¹è§’åŒ–ã€è°±å®šç†å’Œç¨³å®šæ€§åˆ†æçš„åŸºç¡€ã€‚

### Exercises 8.3
ç»ƒä¹  8.3

1.  Compute the characteristic polynomial of
    è®¡ç®—ç‰¹å¾å¤šé¡¹å¼
    
    $$
    A = \begin{bmatrix} 4 & 2 \\ 1 & 3 \end{bmatrix}.
    $$
    
2.  Verify that the sum of the eigenvalues of $\begin{bmatrix} 5 & 0 \\ 0 & -2 \end{bmatrix}$ equals its trace, and their product equals its determinant.
    éªŒè¯ç‰¹å¾å€¼ä¹‹å’Œ $\begin{bmatrix} 5 & 0 \\ 0 & -2 \end{bmatrix}$ ç­‰äºå®ƒçš„è¿¹ï¼Œå®ƒä»¬çš„ä¹˜ç§¯ç­‰äºå®ƒçš„è¡Œåˆ—å¼ã€‚
    
3.  Show that for any triangular matrix, the eigenvalues are just the diagonal entries.
    è¯æ˜å¯¹äºä»»ä½•ä¸‰è§’çŸ©é˜µï¼Œç‰¹å¾å€¼åªæ˜¯å¯¹è§’çº¿é¡¹ã€‚
    
4.  Prove that if $A$ and $B$ are similar matrices, then $p_A(\lambda) = p_B(\lambda)$.
    è¯æ˜å¦‚æœ $A$ å’Œ $B$ æ˜¯ç›¸ä¼¼çŸ©é˜µï¼Œåˆ™ $p_A(\lambda) = p_B(\lambda)$ ã€‚
    
5.  Compute the characteristic polynomial of $\begin{bmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{bmatrix}$.
    è®¡ç®—ç‰¹å¾å¤šé¡¹å¼ \[ 1 1 0 0 1 1 0 0 1 \] â€‹ 1 0 0 â€‹ 1 1 0 â€‹ 0 1 1 â€‹ â€‹ .
    

## 8.4 Applications (Differential Equations, Markov Chains)
8.4 åº”ç”¨ï¼ˆå¾®åˆ†æ–¹ç¨‹ã€é©¬å°”å¯å¤«é“¾ï¼‰

Eigenvalues and eigenvectors are not only central to the theory of linear algebra-they are indispensable tools across mathematics and applied science. Two classic applications are solving systems of differential equations and analyzing Markov chains.
ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ä¸ä»…æ˜¯çº¿æ€§ä»£æ•°ç†è®ºçš„æ ¸å¿ƒï¼Œä¹Ÿæ˜¯æ•°å­¦å’Œåº”ç”¨ç§‘å­¦é¢†åŸŸä¸­ä¸å¯æˆ–ç¼ºçš„å·¥å…·ã€‚ä¸¤ä¸ªç»å…¸çš„åº”ç”¨æ˜¯æ±‚è§£å¾®åˆ†æ–¹ç¨‹ç»„å’Œåˆ†æé©¬å°”å¯å¤«é“¾ã€‚

### Linear Differential Equations
çº¿æ€§å¾®åˆ†æ–¹ç¨‹

Consider the system
è€ƒè™‘ç³»ç»Ÿ

$$
\frac{d\mathbf{x}}{dt} = A \mathbf{x},
$$

where $A$ is an $n \times n$ matrix and $\mathbf{x}(t)$ is a vector-valued function.
å…¶ä¸­ $A$ æ˜¯ $n \times n$ çŸ©é˜µï¼Œ $\mathbf{x}(t)$ æ˜¯çŸ¢é‡å€¼å‡½æ•°ã€‚

If $\mathbf{v}$ is an eigenvector of $A$ with eigenvalue $\lambda$, then the function
å¦‚æœ $\mathbf{v}$ æ˜¯ $A$ çš„ç‰¹å¾å‘é‡ï¼Œå…¶ç‰¹å¾å€¼ä¸º $\lambda$ ï¼Œåˆ™å‡½æ•°

$$
\mathbf{x}(t) = e^{\lambda t}\mathbf{v}
$$

is a solution.
æ˜¯ä¸€ä¸ªè§£å†³æ–¹æ¡ˆã€‚

*   Eigenvalues determine the growth or decay rate:
    ç‰¹å¾å€¼å†³å®šå¢é•¿ç‡æˆ–è¡°å‡ç‡ï¼š
    
    *   If $\lambda < 0$, solutions decay (stable).
        å¦‚æœ $\lambda < 0$ ï¼Œåˆ™è§£å†³æ–¹æ¡ˆè¡°å‡ï¼ˆç¨³å®šï¼‰ã€‚
    *   If $\lambda > 0$, solutions grow (unstable).
        å¦‚æœ $\lambda > 0$ ï¼Œåˆ™è§£å†³æ–¹æ¡ˆä¼šå¢é•¿ï¼ˆä¸ç¨³å®šï¼‰ã€‚
    *   If $\lambda$ is complex, oscillations occur.
        å¦‚æœ $\lambda$ æ˜¯å¤æ•°ï¼Œåˆ™ä¼šå‘ç”ŸæŒ¯è¡ã€‚

By combining eigenvector solutions, we can solve general initial conditions.
é€šè¿‡ç»“åˆç‰¹å¾å‘é‡è§£ï¼Œæˆ‘ä»¬å¯ä»¥è§£å†³ä¸€èˆ¬çš„åˆå§‹æ¡ä»¶ã€‚

Example 8.4.1. Let
ä¾‹ 8.4.1. è®¾

$$
A = \begin{bmatrix}2 & 0 \\0 & -1 \end{bmatrix}.
$$

Then eigenvalues are $2, -1$with eigenvectors$(1,0)$, $(0,1)$. Solutions are
åˆ™ç‰¹å¾å€¼ä¸º $2, -1 $with eigenvectors$ (1,0) $, $ (0,1)$ã€‚è§£ä¸º

$$
\mathbf{x}(t) = c_1 e^{2t}(1,0) + c_2 e^{-t}(0,1).
$$

Thus one component grows exponentially, the other decays.
å› æ­¤ï¼Œä¸€ä¸ªéƒ¨åˆ†å‘ˆæŒ‡æ•°å¢é•¿ï¼Œå¦ä¸€ä¸ªéƒ¨åˆ†åˆ™è¡°å‡ã€‚

### Markov Chains
é©¬å°”å¯å¤«é“¾

A Markov chain is described by a stochastic matrix $P$, where each column sums to 1 and entries are nonnegative. If $\mathbf{x}_k$ represents the probability distribution after $k$ steps, then
é©¬å°”å¯å¤«é“¾å¯ä»¥ç”¨éšæœºçŸ©é˜µ $P$ æ¥æè¿°ï¼Œå…¶ä¸­æ¯åˆ—å’Œä¸º 1ï¼Œä¸”å…ƒç´ ä¸ºéè´Ÿå€¼ã€‚å¦‚æœ ğ‘¥ ğ‘˜ x k â€‹ è¡¨ç¤º $k$ æ­¥åçš„æ¦‚ç‡åˆ†å¸ƒï¼Œåˆ™

$$
\mathbf{x}_{k+1} = P \mathbf{x}_k.
$$

Iterating gives
è¿­ä»£å¾—åˆ°

$$
\mathbf{x}_k = P^k \mathbf{x}_0.
$$

Understanding long-term behavior reduces to analyzing powers of $P$.
ç†è§£é•¿æœŸè¡Œä¸ºå¯ä»¥å½’ç»“ä¸ºåˆ†æ $P$ çš„åŠ›é‡ã€‚

*   The eigenvalue $\lambda = 1$ always exists. Its eigenvector gives the steady-state distribution.
    ç‰¹å¾å€¼ $\lambda = 1$ å§‹ç»ˆå­˜åœ¨ã€‚å…¶ç‰¹å¾å‘é‡ç»™å‡ºäº†ç¨³æ€åˆ†å¸ƒã€‚
*   All other eigenvalues satisfy $|\lambda| \leq 1$. Their influence decays as $k \to \infty$.
    æ‰€æœ‰å…¶ä»–ç‰¹å¾å€¼éƒ½æ»¡è¶³ $|\lambda| \leq 1$ ã€‚å®ƒä»¬çš„å½±å“è¡°å‡ä¸º $k \to \infty$ ã€‚

Example 8.4.2. Consider
ä¾‹ 8.4.2. è€ƒè™‘

$$
P = \begin{bmatrix}0.9 & 0.5 \\0.1 & 0.5 \end{bmatrix}.
$$

Eigenvalues are $\lambda_1 = 1$, $\lambda_2 = 0.4$. The eigenvector for $\lambda = 1$ is proportional to $(5,1)$. Normalizing gives the steady state
ç‰¹å¾å€¼ä¸º $\lambda_1 = 1$ , $\lambda_2 = 0.4$ ã€‚ $\lambda = 1$ çš„ç‰¹å¾å‘é‡ä¸ $(5,1)$ æˆæ­£æ¯”ã€‚å½’ä¸€åŒ–åå¯å¾—åˆ°ç¨³æ€

$$
\pi = \left(\tfrac{5}{6}, \tfrac{1}{6}\right).
$$

Thus, regardless of the starting distribution, the chain converges to $\pi$.
å› æ­¤ï¼Œæ— è®ºèµ·å§‹åˆ†å¸ƒå¦‚ä½•ï¼Œé“¾éƒ½ä¼šæ”¶æ•›åˆ° $\pi$ ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

*   In differential equations, eigenvalues determine the time evolution: exponential growth, decay, or oscillation.
    åœ¨å¾®åˆ†æ–¹ç¨‹ä¸­ï¼Œç‰¹å¾å€¼å†³å®šæ—¶é—´çš„æ¼”å˜ï¼šæŒ‡æ•°å¢é•¿ã€è¡°å‡æˆ–æŒ¯è¡ã€‚
*   In Markov chains, eigenvalues determine the long-term equilibrium of stochastic processes.
    åœ¨é©¬å°”å¯å¤«é“¾ä¸­ï¼Œç‰¹å¾å€¼å†³å®šäº†éšæœºè¿‡ç¨‹çš„é•¿æœŸå‡è¡¡ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Eigenvalue methods turn complex iterative or dynamical systems into tractable problems. In physics, engineering, and finance, they describe stability and resonance. In computer science and statistics, they power algorithms from Googleâ€™s PageRank to modern machine learning.
ç‰¹å¾å€¼æ–¹æ³•å°†å¤æ‚çš„è¿­ä»£æˆ–åŠ¨æ€ç³»ç»Ÿè½¬åŒ–ä¸ºæ˜“äºå¤„ç†çš„é—®é¢˜ã€‚åœ¨ç‰©ç†å­¦ã€å·¥ç¨‹å­¦å’Œé‡‘èå­¦é¢†åŸŸï¼Œå®ƒä»¬æè¿°ç¨³å®šæ€§å’Œå…±æŒ¯ã€‚åœ¨è®¡ç®—æœºç§‘å­¦å’Œç»Ÿè®¡å­¦é¢†åŸŸï¼Œå®ƒä»¬ä¸ºä»è°·æ­Œçš„ PageRank åˆ°ç°ä»£æœºå™¨å­¦ä¹ ç­‰å„ç§ç®—æ³•æä¾›æ”¯æŒã€‚

### Exercises 8.4
ç»ƒä¹  8.4

1.  Solve $\tfrac{d}{dt}\mathbf{x} = \begin{bmatrix} 3 & 0 \\ 0 & -2 \end{bmatrix}\mathbf{x}$.
    è§£å‡º $\tfrac{d}{dt}\mathbf{x} = \begin{bmatrix} 3 & 0 \\ 0 & -2 \end{bmatrix}\mathbf{x}$ ã€‚
    
2.  Show that if $A$ has a complex eigenvalue $\alpha \pm i\beta$, then solutions of $\tfrac{d}{dt}\mathbf{x} = A\mathbf{x}$ involve oscillations of frequency $\beta$.
    è¯æ˜å¦‚æœ $A$ å…·æœ‰å¤ç‰¹å¾å€¼ $\alpha \pm i\beta$ ï¼Œåˆ™ $\tfrac{d}{dt}\mathbf{x} = A\mathbf{x}$ çš„è§£æ¶‰åŠé¢‘ç‡ $\beta$ çš„æŒ¯è¡ã€‚
    
3.  Find the steady-state distribution of
    æ‰¾åˆ°ç¨³æ€åˆ†å¸ƒ
    
    $$
    P = \begin{bmatrix} 0.7 & 0.2 \\ 0.3 & 0.8 \end{bmatrix}.
    $$
    
4.  Prove that for any stochastic matrix $P$, 1 is always an eigenvalue.
    è¯æ˜å¯¹äºä»»ä½•éšæœºçŸ©é˜µ $P$ ï¼Œ 1 å§‹ç»ˆæ˜¯ç‰¹å¾å€¼ã€‚
    
5.  Explain why all eigenvalues of a stochastic matrix satisfy $|\lambda| \leq 1$.
    è§£é‡Šä¸ºä»€ä¹ˆéšæœºçŸ©é˜µçš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½æ»¡è¶³ $|\lambda| \leq 1$ ã€‚
    

# Chapter 9. Quadratic Forms and Spectral Theorems
ç¬¬ä¹ç« äºŒæ¬¡å‹å’Œè°±å®šç†

## 9.1 Quadratic Forms
9.1 äºŒæ¬¡å‹

A quadratic form is a polynomial of degree two in several variables, expressed neatly using matrices. Quadratic forms appear throughout mathematics: in optimization, geometry of conic sections, statistics (variance), and physics (energy functions).
äºŒæ¬¡å‹æ˜¯å¤šå…ƒäºŒæ¬¡å¤šé¡¹å¼ï¼Œå¯ä»¥ç”¨çŸ©é˜µç®€æ´åœ°è¡¨ç¤ºã€‚äºŒæ¬¡å‹åœ¨æ•°å­¦ä¸­éšå¤„å¯è§ï¼šä¼˜åŒ–ã€åœ†é”¥æ›²çº¿å‡ ä½•ã€ç»Ÿè®¡å­¦ï¼ˆæ–¹å·®ï¼‰å’Œç‰©ç†å­¦ï¼ˆèƒ½é‡å‡½æ•°ï¼‰ã€‚

### Definition
å®šä¹‰

Let $A$ be an $n \times n$ symmetric matrix and $\mathbf{x} \in \mathbb{R}^n$. The quadratic form associated with $A$ is
ä»¤ $A$ ä¸º $n \times n$ å¯¹ç§°çŸ©é˜µï¼Œ $\mathbf{x} \in \mathbb{R}^n$ ã€‚ä¸ $A$ ç›¸å…³çš„äºŒæ¬¡å¼ä¸º

$$
Q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x}.
$$

Expanded,
æ‰©å±•ï¼Œ

$$
Q(\mathbf{x}) = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j.
$$

Because $A$ is symmetric ($a_{ij} = a_{ji}$), the cross-terms can be grouped naturally.
å› ä¸º $A$ æ˜¯å¯¹ç§°çš„ (ğ‘ ğ‘– ğ‘— = ğ‘ ğ‘— ğ‘– a ä¼Šå¥‡ â€‹ =a å§¬ â€‹ )ï¼Œäº¤å‰é¡¹å¯ä»¥è‡ªç„¶åˆ†ç»„ã€‚

### Examples
ç¤ºä¾‹

Example 9.1.1. For
ä¾‹ 9.1.1. å¯¹äº

$$
A = \begin{bmatrix}2 & 1 \\1 & 3 \end{bmatrix}, \quad \mathbf{x} = \begin{bmatrix}x \\y \end{bmatrix},
$$

 

$$
Q(x,y) = \begin{bmatrix} x & y \end{bmatrix}\begin{bmatrix}2 & 1 \\1 & 3 \end{bmatrix}\begin{bmatrix}x \\y \end{bmatrix}= 2x^2 + 2xy + 3y^2.
$$

Example 9.1.2. The quadratic form
ä¾‹ 9.1.2. äºŒæ¬¡å‹

$$
Q(x,y) = x^2 + y^2
$$

corresponds to the matrix $A = I_2$. It measures squared Euclidean distance from the origin.
å¯¹åº”äºçŸ©é˜µğ´ = ğ¼ 2 A=I 2 â€‹ . å®ƒæµ‹é‡è·ç¦»åŸç‚¹çš„å¹³æ–¹æ¬§å‡ é‡Œå¾—è·ç¦»ã€‚

Example 9.1.3. The conic section equation
ä¾‹ 9.1.3 åœ†é”¥æ›²çº¿æ–¹ç¨‹

$$
4x^2 + 2xy + 5y^2 = 1
$$

is described by the quadratic form $\mathbf{x}^T A \mathbf{x} = 1$ with
ç”±äºŒæ¬¡å‹ $\mathbf{x}^T A \mathbf{x} = 1$ æè¿°

$$
A = \begin{bmatrix}4 & 1 \\1 & 5\end{bmatrix}.
$$

### Diagonalization of Quadratic Forms
äºŒæ¬¡å‹çš„å¯¹è§’åŒ–

By choosing a new basis consisting of eigenvectors of $A$, we can rewrite the quadratic form without cross terms. If $A = PDP^{-1}$ with $D$ diagonal, then
é€šè¿‡é€‰æ‹©ç”± $A$ çš„ç‰¹å¾å‘é‡ç»„æˆçš„æ–°åŸºï¼Œæˆ‘ä»¬å¯ä»¥é‡å†™æ²¡æœ‰äº¤å‰é¡¹çš„äºŒæ¬¡å‹ã€‚å¦‚æœ $A = PDP^{-1}$ ä»¥ $D$ ä¸ºå¯¹è§’çº¿ï¼Œåˆ™

$$
Q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x} = (P^{-1}\mathbf{x})^T D (P^{-1}\mathbf{x}).
$$

Thus quadratic forms can always be expressed as a sum of weighted squares:
å› æ­¤äºŒæ¬¡å‹æ€»æ˜¯å¯ä»¥è¡¨ç¤ºä¸ºåŠ æƒå¹³æ–¹å’Œï¼š

$$
Q(\mathbf{y}) = \lambda_1 y_1^2 + \cdots + \lambda_n y_n^2,
$$

where $\lambda_i$ are the eigenvalues of $A$.
å…¶ä¸­ğœ† ğ‘– Î» i â€‹ æ˜¯ $A$ çš„ç‰¹å¾å€¼ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

Quadratic forms describe geometric shapes:
äºŒæ¬¡å‹æè¿°å‡ ä½•å½¢çŠ¶ï¼š

*   In 2D: ellipses, parabolas, hyperbolas.
    äºŒç»´ï¼šæ¤­åœ†ã€æŠ›ç‰©çº¿ã€åŒæ›²çº¿ã€‚
*   In 3D: ellipsoids, paraboloids, hyperboloids.
    åœ¨ 3D ä¸­ï¼šæ¤­åœ†ä½“ã€æŠ›ç‰©é¢ã€åŒæ›²é¢ã€‚
*   In higher dimensions: generalizations of ellipsoids.
    åœ¨æ›´é«˜ç»´åº¦ä¸­ï¼šæ¤­åœ†ä½“çš„æ¦‚æ‹¬ã€‚

Diagonalization aligns the coordinate axes with the principal axes of the shape.
å¯¹è§’åŒ–å°†åæ ‡è½´ä¸å½¢çŠ¶çš„ä¸»è½´å¯¹é½ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Quadratic forms unify geometry and algebra. They are central in optimization (minimizing energy functions), statistics ( covariance matrices and variance), mechanics (kinetic energy), and numerical analysis. Understanding quadratic forms leads directly to the spectral theorem.
äºŒæ¬¡å‹ç»Ÿä¸€äº†å‡ ä½•å’Œä»£æ•°ã€‚å®ƒä»¬åœ¨ä¼˜åŒ–ï¼ˆæœ€å°åŒ–èƒ½é‡å‡½æ•°ï¼‰ã€ç»Ÿè®¡å­¦ï¼ˆåæ–¹å·®çŸ©é˜µå’Œæ–¹å·®ï¼‰ã€åŠ›å­¦ï¼ˆåŠ¨èƒ½ï¼‰å’Œæ•°å€¼åˆ†æä¸­éƒ½è‡³å…³é‡è¦ã€‚ç†è§£äºŒæ¬¡å‹å¯ä»¥ç›´æ¥å¼•å‡ºè°±å®šç†ã€‚

### Exercises 9.1
ç»ƒä¹  9.1

1.  Write the quadratic form $Q(x,y) = 3x^2 + 4xy + y^2$ as $\mathbf{x}^T A \mathbf{x}$ for some symmetric matrix $A$.
    å¯¹äºæŸäº›å¯¹ç§°çŸ©é˜µ $A$ ï¼Œå°†äºŒæ¬¡å‹ $Q(x,y) = 3x^2 + 4xy + y^2$ å†™ä¸º $\mathbf{x}^T A \mathbf{x}$ ã€‚
2.  For $A = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}$, compute $Q(x,y)$ explicitly.
    å¯¹äº $A = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}$ ï¼Œæ˜ç¡®è®¡ç®— $Q(x,y)$ ã€‚
3.  Diagonalize the quadratic form $Q(x,y) = 2x^2 + 2xy + 3y^2$.
    å°†äºŒæ¬¡å‹ $Q(x,y) = 2x^2 + 2xy + 3y^2$ å¯¹è§’åŒ–ã€‚
4.  Identify the conic section given by $Q(x,y) = x^2 - y^2$.
    ç¡®å®šç”± $Q(x,y) = x^2 - y^2$ ç»™å‡ºçš„åœ†é”¥æˆªé¢ã€‚
5.  Show that if $A$ is symmetric, quadratic forms defined by $A$ and $A^T$ are identical.
    è¯æ˜å¦‚æœ $A$ æ˜¯å¯¹ç§°çš„ï¼Œåˆ™ç”± $A$ å’Œ $A^T$ å®šä¹‰çš„äºŒæ¬¡å‹æ˜¯ç›¸åŒçš„ã€‚

## 9.2 Positive Definite Matrices
9.2 æ­£å®šçŸ©é˜µ

Quadratic forms are especially important when their associated matrices are positive definite, since these guarantee positivity of energy, distance, or variance. Positive definiteness is a cornerstone in optimization, numerical analysis, and statistics.
å½“äºŒæ¬¡å‹çš„ç›¸å…³çŸ©é˜µä¸ºæ­£å®šçŸ©é˜µæ—¶ï¼Œå®ƒä»¬å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥ä¿è¯èƒ½é‡ã€è·ç¦»æˆ–æ–¹å·®çš„æ­£æ€§ã€‚æ­£å®šæ€§æ˜¯ä¼˜åŒ–ã€æ•°å€¼åˆ†æå’Œç»Ÿè®¡å­¦çš„åŸºçŸ³ã€‚

### Definition
å®šä¹‰

A symmetric matrix $A \in \mathbb{R}^{n \times n}$ is called:
å¯¹ç§°çŸ©é˜µ $A \in \mathbb{R}^{n \times n}$ ç§°ä¸ºï¼š

*   Positive definite if
    æ­£å®šå¦‚æœ
    
    $$
    \mathbf{x}^T A \mathbf{x} > 0 \quad \text{for all nonzero } \mathbf{x} \in \mathbb{R}^n.
    $$
    
*   Positive semidefinite if
    æ­£åŠå®šçš„ï¼Œå¦‚æœ
    
    $$
    \mathbf{x}^T A \mathbf{x} \geq 0 \quad \text{for all } \mathbf{x}.
    $$
    

Similarly, negative definite (always < 0) and indefinite (can be both < 0 and > 0) matrices are defined.
ç±»ä¼¼åœ°ï¼Œå®šä¹‰äº†è´Ÿå®šï¼ˆå§‹ç»ˆ < 0ï¼‰å’Œä¸å®šï¼ˆå¯ä»¥åŒæ—¶ < 0 å’Œ > 0ï¼‰çŸ©é˜µã€‚

### Examples
ç¤ºä¾‹

Example 9.2.1.
ä¾‹ 9.2.1ã€‚

$$
A = \begin{bmatrix}2 & 0 \\0 & 3 \end{bmatrix}
$$

is positive definite, since
æ˜¯æ­£å®šçš„ï¼Œå› ä¸º

$$
Q(x,y) = 2x^2 + 3y^2 > 0
$$

for all $(x,y) \neq (0,0)$.
å¯¹äºæ‰€æœ‰ $(x,y) \neq (0,0)$ ã€‚

Example 9.2.2.
ä¾‹ 9.2.2ã€‚

$$
A = \begin{bmatrix}1 & 2 \\2 & 1 \end{bmatrix}
$$

has quadratic form
å…·æœ‰äºŒæ¬¡å½¢å¼

$$
Q(x,y) = x^2 + 4xy + y^2.
$$

This matrix is not positive definite, since $Q(1,-1) = -2 < 0$.
è¯¥çŸ©é˜µä¸æ˜¯æ­£å®šçš„ï¼Œå› ä¸º $Q(1,-1) = -2 < 0$ ã€‚

### Characterizations
ç‰¹å¾

For a symmetric matrix $A$:
å¯¹äºå¯¹ç§°çŸ©é˜µ $A$ ï¼š

1.  Eigenvalue test: $A$ is positive definite if and only if all eigenvalues of $A$ are positive.
    ç‰¹å¾å€¼æ£€éªŒï¼šå½“ä¸”ä»…å½“ $A$ çš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½ä¸ºæ­£æ—¶ï¼Œ $A$ æ‰æ˜¯æ­£å®šçš„ã€‚
    
2.  Principal minors test (Sylvesterâ€™s criterion): $A$ is positive definite if and only if all leading principal minors ( determinants of top-left $k \times k$ submatrices) are positive.
    ä¸»å­å¼æ£€éªŒï¼ˆè¥¿å°”ç»´æ–¯ç‰¹æ ‡å‡†ï¼‰ï¼šå½“ä¸”ä»…å½“æ‰€æœ‰é¦–é¡¹ä¸»å­å¼ï¼ˆå·¦ä¸Šè§’ $k \times k$ å­çŸ©é˜µçš„è¡Œåˆ—å¼ï¼‰å‡ä¸ºæ­£æ—¶ï¼Œ $A$ æ‰æ˜¯æ­£å®šçš„ã€‚
    
3.  Cholesky factorization: $A$ is positive definite if and only if it can be written as
    Cholesky åˆ†è§£ï¼š $A$ ä¸ºæ­£å®šå½“ä¸”ä»…å½“å®ƒå¯ä»¥å†™æˆ
    
    $$
    A = R^T R,
    $$
    
    where $R$ is an upper triangular matrix with positive diagonal entries.
    å…¶ä¸­ $R$ æ˜¯å…·æœ‰æ­£å¯¹è§’çº¿é¡¹çš„ä¸Šä¸‰è§’çŸ©é˜µã€‚
    

### Geometric Interpretation
å‡ ä½•è§£é‡Š

*   Positive definite matrices correspond to quadratic forms that define ellipsoids centered at the origin.
    æ­£å®šçŸ©é˜µå¯¹åº”äºå®šä¹‰ä»¥åŸç‚¹ä¸ºä¸­å¿ƒçš„æ¤­åœ†ä½“çš„äºŒæ¬¡å‹ã€‚
*   Positive semidefinite matrices define flattened ellipsoids (possibly degenerate).
    æ­£åŠå®šçŸ©é˜µå®šä¹‰æ‰å¹³çš„æ¤­çƒä½“ï¼ˆå¯èƒ½æ˜¯é€€åŒ–çš„ï¼‰ã€‚
*   Indefinite matrices define hyperbolas or saddle-shaped surfaces.
    ä¸å®šçŸ©é˜µå®šä¹‰åŒæ›²çº¿æˆ–é©¬éå½¢æ›²é¢ã€‚

### Applications
åº”ç”¨

*   Optimization: Hessians of convex functions are positive semidefinite; strict convexity corresponds to positive definite Hessians.
    ä¼˜åŒ–ï¼šå‡¸å‡½æ•°çš„ Hessian çŸ©é˜µæ˜¯æ­£åŠå®šçš„ï¼›ä¸¥æ ¼å‡¸æ€§å¯¹åº”äºæ­£å®šçš„ Hessian çŸ©é˜µã€‚
*   Statistics: Covariance matrices are positive semidefinite.
    ç»Ÿè®¡ï¼šåæ–¹å·®çŸ©é˜µæ˜¯æ­£åŠå®šçš„ã€‚
*   Numerical methods: Cholesky decomposition is widely used to solve systems with positive definite matrices efficiently.
    æ•°å€¼æ–¹æ³•ï¼šCholesky åˆ†è§£è¢«å¹¿æ³›ç”¨äºæœ‰æ•ˆåœ°è§£å†³å…·æœ‰æ­£å®šçŸ©é˜µçš„ç³»ç»Ÿã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Positive definiteness provides stability and guarantees in mathematics and computation. It ensures energy functions are bounded below, optimization problems have unique solutions, and statistical models are meaningful.
æ­£å®šæ€§åœ¨æ•°å­¦å’Œè®¡ç®—ä¸­æä¾›äº†ç¨³å®šæ€§å’Œä¿è¯ã€‚å®ƒç¡®ä¿èƒ½é‡å‡½æ•°æœ‰ç•Œï¼Œä¼˜åŒ–é—®é¢˜æœ‰å”¯ä¸€è§£ï¼Œç»Ÿè®¡æ¨¡å‹æœ‰æ„ä¹‰ã€‚

### Exercises 9.2
ç»ƒä¹  9.2

1.  Use Sylvesterâ€™s criterion to check whether
    ä½¿ç”¨ Sylvester æ ‡å‡†æ£€æŸ¥
    
    $$
    A = \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}
    $$
    
    is positive definite.
    æ˜¯æ­£å®šçš„ã€‚
    
2.  Determine whether
    ç¡®å®šæ˜¯å¦
    
    $$
    A = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}
    $$
    
    is positive definite, semidefinite, or indefinite.
    æ˜¯æ­£å®šçš„ã€åŠå®šçš„æˆ–ä¸å®šçš„ã€‚
    
3.  Find the eigenvalues of
    æ‰¾åˆ°ç‰¹å¾å€¼
    
    $$
    A = \begin{bmatrix} 4 & 2 \\ 2 & 3 \end{bmatrix},
    $$
    
    and use them to classify definiteness.
    å¹¶ç”¨å®ƒä»¬æ¥å¯¹ç¡®å®šæ€§è¿›è¡Œåˆ†ç±»ã€‚
    
4.  Prove that all diagonal matrices with positive entries are positive definite.
    è¯æ˜æ‰€æœ‰å…·æœ‰æ­£é¡¹çš„å¯¹è§’çŸ©é˜µéƒ½æ˜¯æ­£å®šçš„ã€‚
    
5.  Show that if $A$ is positive definite, then so is $P^T A P$ for any invertible matrix $P$.
    è¯æ˜å¦‚æœ $A$ ä¸ºæ­£å®šçŸ©é˜µï¼Œåˆ™å¯¹äºä»»ä½•å¯é€†çŸ©é˜µ $P$ ï¼Œ $P^T A P$ ä¹Ÿä¸ºæ­£å®šçŸ©é˜µã€‚
    

## 9.3 Spectral Theorem
9.3 è°±å®šç†

The spectral theorem is one of the most powerful results in linear algebra. It states that symmetric matrices can always be diagonalized by an orthogonal basis of eigenvectors. This links algebra (eigenvalues), geometry (orthogonal directions), and applications (stability, optimization, statistics).
è°±å®šç†æ˜¯çº¿æ€§ä»£æ•°ä¸­æœ€æœ‰åŠ›çš„ç»“è®ºä¹‹ä¸€ã€‚å®ƒæŒ‡å‡ºå¯¹ç§°çŸ©é˜µæ€»æ˜¯å¯ä»¥é€šè¿‡ç‰¹å¾å‘é‡çš„æ­£äº¤åŸºå¯¹è§’åŒ–ã€‚è¿™è¿æ¥äº†ä»£æ•°ï¼ˆç‰¹å¾å€¼ï¼‰ã€å‡ ä½•ï¼ˆæ­£äº¤æ–¹å‘ï¼‰å’Œåº”ç”¨ï¼ˆç¨³å®šæ€§ã€ä¼˜åŒ–ã€ç»Ÿè®¡ï¼‰ã€‚

### Statement of the Spectral Theorem
è°±å®šç†è¡¨è¿°

If $A \in \mathbb{R}^{n \times n}$ is symmetric ($A^T = A$), then:
å¦‚æœ $A \in \mathbb{R}^{n \times n}$ æ˜¯å¯¹ç§°çš„ï¼ˆ $A^T = A$ ï¼‰ï¼Œåˆ™ï¼š

1.  All eigenvalues of $A$ are real.
    $A$ çš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½æ˜¯å®æ•°ã€‚
    
2.  There exists an orthonormal basis of $\mathbb{R}^n$ consisting of eigenvectors of $A$.
    å­˜åœ¨ç”± $A$ çš„ç‰¹å¾å‘é‡ç»„æˆçš„ $\mathbb{R}^n$ æ­£äº¤åŸºã€‚
    
3.  Thus, $A$ can be written as
    å› æ­¤ï¼Œ $A$ å¯ä»¥å†™æˆ
    
    $$
    A = Q \Lambda Q^T,
    $$
    
    where $Q$ is an orthogonal matrix ($Q^T Q = I$) and $\Lambda$ is diagonal with eigenvalues of $A$ on the diagonal.
    å…¶ä¸­ $Q$ æ˜¯æ­£äº¤çŸ©é˜µ ( $Q^T Q = I$ )ï¼Œ $\Lambda$ æ˜¯å¯¹è§’çŸ©é˜µï¼Œå…¶ç‰¹å¾å€¼ $A$ ä½äºå¯¹è§’çº¿ä¸Šã€‚
    

### Consequences
ç»“æœ

*   Symmetric matrices are always diagonalizable, and the diagonalization is numerically stable.
    å¯¹ç§°çŸ©é˜µæ€»æ˜¯å¯å¯¹è§’åŒ–çš„ï¼Œå¹¶ä¸”å¯¹è§’åŒ–åœ¨æ•°å€¼ä¸Šæ˜¯ç¨³å®šçš„ã€‚
*   Quadratic forms $\mathbf{x}^T A \mathbf{x}$ can be expressed in terms of eigenvalues and eigenvectors, showing ellipsoids aligned with eigen-directions.
    äºŒæ¬¡å‹ $\mathbf{x}^T A \mathbf{x}$ å¯ä»¥ç”¨ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æ¥è¡¨ç¤ºï¼Œæ˜¾ç¤ºä¸ç‰¹å¾æ–¹å‘å¯¹é½çš„æ¤­åœ†ä½“ã€‚
*   Positive definiteness can be checked by confirming that all eigenvalues are positive.
    å¯ä»¥é€šè¿‡ç¡®è®¤æ‰€æœ‰ç‰¹å¾å€¼éƒ½ä¸ºæ­£æ¥æ£€æŸ¥æ­£å®šæ€§ã€‚

### Example 9.3.1
ä¾‹ 9.3.1

Let
è®©

$$
A = \begin{bmatrix}2 & 1 \\1 & 2 \end{bmatrix}.
$$

1.  Characteristic polynomial:
    ç‰¹å¾å¤šé¡¹å¼ï¼š

$$
p(\lambda) = (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3.
$$

Eigenvalues: $\lambda_1 = 1, \ \lambda_2 = 3$.
ç‰¹å¾å€¼ï¼š $\lambda_1 = 1, \ \lambda_2 = 3$ ã€‚

2.  Eigenvectors:
    ç‰¹å¾å‘é‡ï¼š

*   For $\lambda=1$: solve $(A-I)\mathbf{v} = 0$, giving $(1,-1)$.
    å¯¹äº $\lambda=1$ ï¼šæ±‚è§£ $(A-I)\mathbf{v} = 0$ ï¼Œå¾—åˆ° $(1,-1)$ ã€‚
*   For $\lambda=3$: solve $(A-3I)\mathbf{v} = 0$, giving $(1,1)$.
    å¯¹äº $\lambda=3$ ï¼šæ±‚è§£ $(A-3I)\mathbf{v} = 0$ ï¼Œå¾—åˆ° $(1,1)$ ã€‚

3.  Normalize eigenvectors:
    å½’ä¸€åŒ–ç‰¹å¾å‘é‡ï¼š

$$
\mathbf{u}_1 = \tfrac{1}{\sqrt{2}}(1,-1), \quad \mathbf{u}_2 = \tfrac{1}{\sqrt{2}}(1,1).
$$

4.  Then
    ç„¶å

$$
Q =\begin{bmatrix}\tfrac{1}{\sqrt{2}} & \tfrac{1}{\sqrt{2}} \[6pt] -\tfrac{1}{\sqrt{2}} & \tfrac{1}{\sqrt{2}}\end{bmatrix}, \quad\Lambda =\begin{bmatrix}1 & 0 \\0 & 3\end{bmatrix}.
$$

So
æ‰€ä»¥

$$
A = Q \Lambda Q^T.
$$

### Geometric Interpretation
å‡ ä½•è§£é‡Š

The spectral theorem says every symmetric matrix acts like independent scaling along orthogonal directions. In geometry, this corresponds to stretching space along perpendicular axes.
è°±å®šç†æŒ‡å‡ºï¼Œæ¯ä¸ªå¯¹ç§°çŸ©é˜µéƒ½åƒæ²¿æ­£äº¤æ–¹å‘çš„ç‹¬ç«‹ç¼©æ”¾ä¸€æ ·ã€‚åœ¨å‡ ä½•å­¦ä¸­ï¼Œè¿™ç›¸å½“äºæ²¿å‚ç›´è½´æ‹‰ä¼¸ç©ºé—´ã€‚

*   Ellipses, ellipsoids, and quadratic surfaces can be fully understood via eigenvalues and eigenvectors.
    é€šè¿‡ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡å¯ä»¥å……åˆ†ç†è§£æ¤­åœ†ã€æ¤­åœ†ä½“å’ŒäºŒæ¬¡æ›²é¢ã€‚
*   Orthogonality ensures directions remain perpendicular after transformation.
    æ­£äº¤æ€§ç¡®ä¿æ–¹å‘åœ¨å˜æ¢åä¿æŒå‚ç›´ã€‚

### Applications
åº”ç”¨

*   Optimization: The spectral theorem underlies classification of critical points via eigenvalues of the Hessian.
    ä¼˜åŒ–ï¼šè°±å®šç†æ˜¯é€šè¿‡ Hessian çš„ç‰¹å¾å€¼å¯¹ä¸´ç•Œç‚¹è¿›è¡Œåˆ†ç±»çš„åŸºç¡€ã€‚
*   PCA (Principal Component Analysis): Data covariance matrices are symmetric, and PCA finds orthogonal directions of maximum variance.
    PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰ï¼šæ•°æ®åæ–¹å·®çŸ©é˜µæ˜¯å¯¹ç§°çš„ï¼ŒPCA æ‰¾åˆ°æœ€å¤§æ–¹å·®çš„æ­£äº¤æ–¹å‘ã€‚
*   Differential equations & physics: Symmetric operators correspond to measurable quantities with real eigenvalues ( stability, energy).
    å¾®åˆ†æ–¹ç¨‹å’Œç‰©ç†å­¦ï¼šå¯¹ç§°ç®—å­å¯¹åº”äºå…·æœ‰å®ç‰¹å¾å€¼ï¼ˆç¨³å®šæ€§ã€èƒ½é‡ï¼‰çš„å¯æµ‹é‡é‡ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

The spectral theorem guarantees that symmetric matrices are as simple as possible: they can always be analyzed in terms of real, orthogonal eigenvectors. This provides both deep theoretical insight and powerful computational tools.
è°±å®šç†ä¿è¯å¯¹ç§°çŸ©é˜µå°½å¯èƒ½ç®€å•ï¼šå®ƒä»¬æ€»æ˜¯å¯ä»¥ç”¨å®æ•°æ­£äº¤ç‰¹å¾å‘é‡æ¥åˆ†æã€‚è¿™æ—¢æä¾›äº†æ·±åˆ»çš„ç†è®ºè§è§£ï¼Œä¹Ÿæä¾›äº†å¼ºå¤§çš„è®¡ç®—å·¥å…·ã€‚

### Exercises 9.3
ç»ƒä¹  9.3

1.  Diagonalize
    å¯¹è§’åŒ–
    
    $$
    A = \begin{bmatrix} 4 & 2 \\ 2 & 3 \end{bmatrix}
    $$
    
    using the spectral theorem.
    ä½¿ç”¨è°±å®šç†ã€‚
    
2.  Prove that all eigenvalues of a real symmetric matrix are real.
    è¯æ˜å®å¯¹ç§°çŸ©é˜µçš„æ‰€æœ‰ç‰¹å¾å€¼éƒ½æ˜¯å®æ•°ã€‚
    
3.  Show that eigenvectors corresponding to distinct eigenvalues of a symmetric matrix are orthogonal.
    è¯æ˜å¯¹ç§°çŸ©é˜µçš„ä¸åŒç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡æ˜¯æ­£äº¤çš„ã€‚
    
4.  Explain geometrically how the spectral theorem describes ellipsoids defined by quadratic forms.
    ä»å‡ ä½•è§’åº¦è§£é‡Šè°±å®šç†å¦‚ä½•æè¿°ç”±äºŒæ¬¡å‹å®šä¹‰çš„æ¤­çƒä½“ã€‚
    
5.  Apply the spectral theorem to the covariance matrix
    å°†è°±å®šç†åº”ç”¨äºåæ–¹å·®çŸ©é˜µ
    
    $$
    \Sigma = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix},
    $$
    
    and interpret the eigenvectors as principal directions of variance.
    å¹¶å°†ç‰¹å¾å‘é‡è§£é‡Šä¸ºæ–¹å·®çš„ä¸»æ–¹å‘ã€‚
    

## 9.4 Principal Component Analysis (PCA)
9.4 ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰

Principal Component Analysis (PCA) is a widely used technique in data science, machine learning, and statistics. At its core, PCA is an application of the spectral theorem to covariance matrices: it finds orthogonal directions (principal components) that capture the maximum variance in data.
ä¸»æˆåˆ†åˆ†æ (PCA) æ˜¯æ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ å’Œç»Ÿè®¡å­¦ä¸­å¹¿æ³›ä½¿ç”¨çš„æŠ€æœ¯ã€‚PCA çš„æ ¸å¿ƒæ˜¯è°±å®šç†åœ¨åæ–¹å·®çŸ©é˜µä¸­çš„åº”ç”¨ï¼šå®ƒæ‰¾åˆ°èƒ½å¤Ÿæ•æ‰æ•°æ®ä¸­æœ€å¤§æ–¹å·®çš„æ­£äº¤æ–¹å‘ï¼ˆä¸»æˆåˆ†ï¼‰ã€‚

### The Idea
ç†å¿µ

Given a dataset of vectors $\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_m \in \mathbb{R}^n$:
ç»™å®šå‘é‡æ•°æ®é›† $\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_m \in \mathbb{R}^n$ ï¼š

1.  Center the data by subtracting the mean vector $\bar{\mathbf{x}}$.
    é€šè¿‡å‡å»å¹³å‡å‘é‡ $\bar{\mathbf{x}}$ ä½¿æ•°æ®å±…ä¸­ã€‚
    
2.  Form the covariance matrix
    å½¢æˆåæ–¹å·®çŸ©é˜µ
    
    $$
    \Sigma = \frac{1}{m} \sum_{i=1}^m (\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})^T.
    $$
    
3.  Apply the spectral theorem: $\Sigma = Q \Lambda Q^T$.
    åº”ç”¨è°±å®šç†ï¼š $\Sigma = Q \Lambda Q^T$ ã€‚
    
    *   Columns of $Q$ are orthonormal eigenvectors (principal directions).
        $Q$ çš„åˆ—æ˜¯æ­£äº¤ç‰¹å¾å‘é‡ï¼ˆä¸»æ–¹å‘ï¼‰ã€‚
    *   Eigenvalues in $\Lambda$ measure variance explained by each direction.
        $\Lambda$ ä¸­çš„ç‰¹å¾å€¼æµ‹é‡æ¯ä¸ªæ–¹å‘è§£é‡Šçš„æ–¹å·®ã€‚

The first principal component is the eigenvector corresponding to the largest eigenvalue; it is the direction of maximum variance.
ç¬¬ä¸€ä¸ªä¸»æˆåˆ†æ˜¯æœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œæ˜¯æ–¹å·®æœ€å¤§çš„æ–¹å‘ã€‚

### Example 9.4.1
ä¾‹ 9.4.1

Suppose we have two-dimensional data points roughly aligned along the line $y = x$. The covariance matrix is approximately
å‡è®¾æˆ‘ä»¬æœ‰äºŒç»´æ•°æ®ç‚¹å¤§è‡´æ²¿ç€ç›´çº¿ $y = x$ æ’åˆ—ã€‚åæ–¹å·®çŸ©é˜µå¤§çº¦ä¸º

$$
\Sigma =\begin{bmatrix}2 & 1.9 \\1.9 & 2\end{bmatrix}.
$$

Eigenvalues are about $3.9 and \\0.1$. The eigenvector for $\\lambda = 3.9$is approximately$(1,1)/\\sqrt{2}$.
ç‰¹å¾å€¼çº¦ä¸º $3.9 å’Œ \\ 0.1 $. The eigenvector for $ \\lambda = 3.9 $is approximately$ (1,1)/\\sqrt{2}$ã€‚

*   First principal component: the line $y = x$.
    ç¬¬ä¸€ä¸ªä¸»æˆåˆ†ï¼šçº¿ $y = x$ ã€‚
*   Most variance lies along this direction.
    å¤§éƒ¨åˆ†å·®å¼‚éƒ½å‘ç”Ÿåœ¨è¿™ä¸ªæ–¹å‘ã€‚
*   Second component is nearly orthogonal ($y = -x$), but variance there is tiny.
    ç¬¬äºŒä¸ªæˆåˆ†å‡ ä¹æ­£äº¤ï¼ˆ $y = -x$ ï¼‰ï¼Œä½†é‚£é‡Œçš„æ–¹å·®å¾ˆå°ã€‚

Thus PCA reduces the data to essentially one dimension.
å› æ­¤ï¼ŒPCA å°†æ•°æ®ç®€åŒ–ä¸ºä¸€ä¸ªç»´åº¦ã€‚

### Applications of PCA
PCA çš„åº”ç”¨

1.  Dimensionality reduction: Represent data with fewer features while retaining most variance.
    é™ç»´ï¼šç”¨è¾ƒå°‘çš„ç‰¹å¾è¡¨ç¤ºæ•°æ®ï¼ŒåŒæ—¶ä¿ç•™å¤§éƒ¨åˆ†çš„æ–¹å·®ã€‚
2.  Noise reduction: Small eigenvalues correspond to noise; discarding them filters data.
    é™å™ªï¼šè¾ƒå°çš„ç‰¹å¾å€¼å¯¹åº”å™ªå£°ï¼›ä¸¢å¼ƒå®ƒä»¬å¯ä»¥è¿‡æ»¤æ•°æ®ã€‚
3.  Visualization: Projecting high-dimensional data onto top 2 or 3 principal components reveals structure.
    å¯è§†åŒ–ï¼šå°†é«˜ç»´æ•°æ®æŠ•å½±åˆ°å‰ 2 ä¸ªæˆ– 3 ä¸ªä¸»æˆåˆ†ä¸Šå¯ä»¥æ­ç¤ºç»“æ„ã€‚
4.  Compression: PCA is used in image and signal compression.
    å‹ç¼©ï¼šPCA ç”¨äºå›¾åƒå’Œä¿¡å·å‹ç¼©ã€‚

### Connection to the Spectral Theorem
ä¸è°±å®šç†çš„è”ç³»

The covariance matrix $\Sigma$ is always symmetric and positive semidefinite. Hence by the spectral theorem, it has an orthonormal basis of eigenvectors and nonnegative real eigenvalues. PCA is nothing more than re-expressing data in this eigenbasis.
åæ–¹å·®çŸ©é˜µ $\Sigma$ å§‹ç»ˆæ˜¯å¯¹ç§°çš„ï¼Œä¸”ä¸ºåŠæ­£å®šçŸ©é˜µã€‚å› æ­¤ï¼Œæ ¹æ®è°±å®šç†ï¼Œå®ƒæœ‰ä¸€ä¸ªç”±ç‰¹å¾å‘é‡å’Œéè´Ÿå®ç‰¹å¾å€¼ç»„æˆçš„æ­£äº¤åŸºã€‚PCA åªä¸è¿‡æ˜¯åœ¨è¿™ä¸ªç‰¹å¾åŸºä¸Šé‡æ–°è¡¨è¾¾æ•°æ®ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

PCA demonstrates how abstract linear algebra directly powers modern applications. Eigenvalues and eigenvectors give a practical method for simplifying data, revealing patterns, and reducing complexity. It is one of the most important algorithms derived from the spectral theorem.
PCA å±•ç¤ºäº†æŠ½è±¡çº¿æ€§ä»£æ•°å¦‚ä½•ç›´æ¥é©±åŠ¨ç°ä»£åº”ç”¨ã€‚ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æä¾›äº†ä¸€ç§ç®€åŒ–æ•°æ®ã€æ­ç¤ºæ¨¡å¼å’Œé™ä½å¤æ‚æ€§çš„å®ç”¨æ–¹æ³•ã€‚å®ƒæ˜¯ä»è°±å®šç†ä¸­æ¨å¯¼å‡ºçš„æœ€é‡è¦çš„ç®—æ³•ä¹‹ä¸€ã€‚

### Exercises 9.4
ç»ƒä¹  9.4

1.  Show that the covariance matrix is symmetric and positive semidefinite.
    è¯æ˜åæ–¹å·®çŸ©é˜µæ˜¯å¯¹ç§°çš„å’ŒåŠæ­£å®šçš„ã€‚
2.  Compute the covariance matrix of the dataset $(1,2), (2,3), (3,4)$, and find its eigenvalues and eigenvectors.
    è®¡ç®—æ•°æ®é›† $(1,2), (2,3), (3,4)$ çš„åæ–¹å·®çŸ©é˜µï¼Œå¹¶æ‰¾åˆ°å…¶ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ã€‚
3.  Explain why the first principal component captures the maximum variance.
    è§£é‡Šä¸ºä»€ä¹ˆç¬¬ä¸€ä¸ªä¸»æˆåˆ†æ•è·æœ€å¤§æ–¹å·®ã€‚
4.  In image compression, explain how PCA can reduce storage by keeping only the top $k$ principal components.
    åœ¨å›¾åƒå‹ç¼©ä¸­ï¼Œè§£é‡Š PCA å¦‚ä½•é€šè¿‡ä»…ä¿ç•™å‰ $k$ ä¸ªä¸»æˆåˆ†æ¥å‡å°‘å­˜å‚¨ã€‚
5.  Prove that the sum of the eigenvalues of the covariance matrix equals the total variance of the dataset.
    è¯æ˜åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼ä¹‹å’Œç­‰äºæ•°æ®é›†çš„æ€»æ–¹å·®ã€‚

# Chapter 10. Linear Algebra in Practice
ç¬¬ 10 ç«  çº¿æ€§ä»£æ•°å®è·µ

## 10.1 Computer Graphics (Rotations, Projections)
10.1 è®¡ç®—æœºå›¾å½¢å­¦ï¼ˆæ—‹è½¬ã€æŠ•å½±ï¼‰

Linear algebra is the language of modern computer graphics. Every image rendered on a screen, every 3D model rotated or projected, is ultimately the result of applying matrices to vectors. Rotations, reflections, scalings, and projections are all linear transformations, making matrices the natural tool for manipulating geometry.
çº¿æ€§ä»£æ•°æ˜¯ç°ä»£è®¡ç®—æœºå›¾å½¢å­¦çš„è¯­è¨€ã€‚å±å¹•ä¸Šæ¸²æŸ“çš„æ¯ä¸€å¹…å›¾åƒï¼Œä»¥åŠæ—‹è½¬æˆ–æŠ•å½±çš„æ¯ä¸€ä¸ª 3D æ¨¡å‹ï¼Œæœ€ç»ˆéƒ½æ˜¯å°†çŸ©é˜µåº”ç”¨äºå‘é‡çš„ç»“æœã€‚æ—‹è½¬ã€åå°„ã€ç¼©æ”¾å’ŒæŠ•å½±éƒ½æ˜¯çº¿æ€§å˜æ¢ï¼Œè¿™ä½¿å¾—çŸ©é˜µæˆä¸ºå¤„ç†å‡ ä½•å›¾å½¢çš„å¤©ç„¶å·¥å…·ã€‚

### Rotations in 2D
äºŒç»´æ—‹è½¬

A counterclockwise rotation by an angle $\theta$ in the plane is represented by
åœ¨å¹³é¢ä¸Šé€†æ—¶é’ˆæ—‹è½¬è§’åº¦ $\theta$ è¡¨ç¤ºä¸º

$$
R_\theta =\begin{bmatrix}\cos\theta & -\sin\theta \\\sin\theta & \cos\theta\end{bmatrix}.
$$

For any vector $\mathbf{v} \in \mathbb{R}^2$, the rotated vector is
å¯¹äºä»»æ„å‘é‡ $\mathbf{v} \in \mathbb{R}^2$ ï¼Œæ—‹è½¬åçš„å‘é‡ä¸º

$$
\mathbf{v}' = R_\theta \mathbf{v}.
$$

This preserves lengths and angles, since $R_\theta$ is orthogonal with determinant 1.
è¿™ä¿ç•™äº†é•¿åº¦å’Œè§’åº¦ï¼Œå› ä¸ºğ‘… ğœƒ R Î¸ â€‹ ä¸è¡Œåˆ—å¼ 1 æ­£äº¤ã€‚

### Rotations in 3D
3D æ—‹è½¬

In three dimensions, rotations are represented by $3 \\times 3 orthogonal matrices with determinant \\1$. For example, arotation about the $z$-axis is
åœ¨ä¸‰ç»´ç©ºé—´ä¸­ï¼Œæ—‹è½¬ç”±$3 \\times 3 æ­£äº¤çŸ©é˜µè¡¨ç¤ºï¼Œå…¶è¡Œåˆ—å¼ä¸º \\ 1 $. For example, arotation about the $ z$è½´ä¸º

$$
R_z(\theta) =\begin{bmatrix}\cos\theta & -\sin\theta & 0 \\\sin\theta & \cos\theta & 0 \\0 & 0 & 1\end{bmatrix}.
$$

Similar formulas exist for rotations about the $x$\- and $y$\-axes.
å¯¹äºç»• $x$ è½´å’Œ $y$ è½´çš„æ—‹è½¬ä¹Ÿå­˜åœ¨ç±»ä¼¼çš„å…¬å¼ã€‚

More general 3D rotations can be described by axisâ€“angle representation or quaternions, but the underlying idea is still linear transformations represented by matrices.
æ›´ä¸€èˆ¬çš„ 3D æ—‹è½¬å¯ä»¥ç”¨è½´è§’è¡¨ç¤ºæˆ–å››å…ƒæ•°æ¥æè¿°ï¼Œä½†å…¶åŸºæœ¬æ€æƒ³ä»ç„¶æ˜¯çŸ©é˜µè¡¨ç¤ºçš„çº¿æ€§å˜æ¢ã€‚

### Projections
é¢„æµ‹

To display 3D objects on a 2D screen, we use projections:
ä¸ºäº†åœ¨ 2D å±å¹•ä¸Šæ˜¾ç¤º 3D å¯¹è±¡ï¼Œæˆ‘ä»¬ä½¿ç”¨æŠ•å½±ï¼š

1.  Orthogonal projection: drops the $z$\-coordinate, mapping $(x,y,z) \mapsto (x,y)$.
    æ­£äº¤æŠ•å½±ï¼šåˆ é™¤ $z$ åæ ‡ï¼Œæ˜ å°„ $(x,y,z) \mapsto (x,y)$ ã€‚
    
    $$
    P = \begin{bmatrix}1 & 0 & 0 \\0 & 1 & 0\end{bmatrix}.
    $$
    
2.  Perspective projection: mimics the effect of a camera. A point $(x,y,z)$ projects to
    é€è§†æŠ•å½±ï¼šæ¨¡æ‹Ÿç›¸æœºçš„æ•ˆæœã€‚ç‚¹ $(x,y,z)$ æŠ•å½±åˆ°
    
    $$
    \left(\frac{x}{z}, \frac{y}{z}\right),
    $$
    
    capturing how distant objects appear smaller.
    æ•æ‰è¿œå¤„ç‰©ä½“å¦‚ä½•æ˜¾å¾—æ›´å°ã€‚
    

These operations are linear (orthogonal projection) or nearly linear (perspective projection becomes linear in homogeneous coordinates).
è¿™äº›æ“ä½œæ˜¯çº¿æ€§çš„ï¼ˆæ­£äº¤æŠ•å½±ï¼‰æˆ–è¿‘ä¼¼çº¿æ€§çš„ï¼ˆé€è§†æŠ•å½±åœ¨é½æ¬¡åæ ‡ä¸­å˜ä¸ºçº¿æ€§ï¼‰ã€‚

### Homogeneous Coordinates
é½æ¬¡åæ ‡

To unify translations and projections with linear transformations, computer graphics uses homogeneous coordinates. A 3D point $(x,y,z)$ is represented as a 4D vector $(x,y,z,1)$. Transformations are then 4Ã—4 matrices, which can represent rotations, scalings, and translations in a single framework.
ä¸ºäº†å°†å¹³ç§»å’ŒæŠ•å½±ä¸çº¿æ€§å˜æ¢ç»Ÿä¸€èµ·æ¥ï¼Œè®¡ç®—æœºå›¾å½¢å­¦ä½¿ç”¨é½æ¬¡åæ ‡ã€‚3D ç‚¹ $(x,y,z)$ è¡¨ç¤ºä¸ºå››ç»´å‘é‡ $(x,y,z,1)$ ã€‚å˜æ¢åˆ™è¡¨ç¤ºä¸ºçŸ©é˜µ 4Ã—4 ï¼Œå¯ä»¥åœ¨å•ä¸ªæ¡†æ¶ä¸­è¡¨ç¤ºæ—‹è½¬ã€ç¼©æ”¾å’Œå¹³ç§»ã€‚

Example: Translation by $(a,b,c)$:
ä¾‹å¦‚ï¼š $(a,b,c)$ ç¿»è¯‘ï¼š

$$
T = \begin{bmatrix}1 & 0 & 0 & a \\0 & 1 & 0 & b \\0 & 0 & 1 & c \\0 & 0 & 0 & 1\end{bmatrix}.
$$

### Geometric Interpretation
å‡ ä½•è§£é‡Š

*   Rotations preserve shape and size, only changing orientation.
    æ—‹è½¬ä¿æŒå½¢çŠ¶å’Œå¤§å°ï¼Œä»…æ”¹å˜æ–¹å‘ã€‚
*   Projections reduce dimension: from 3D world space to 2D screen space.
    æŠ•å½±å‡å°‘ç»´åº¦ï¼šä» 3D ä¸–ç•Œç©ºé—´åˆ° 2D å±å¹•ç©ºé—´ã€‚
*   Homogeneous coordinates allow us to combine multiple transformations (rotation + translation + projection) into a single matrix multiplication.
    é½æ¬¡åæ ‡å…è®¸æˆ‘ä»¬å°†å¤šä¸ªå˜æ¢ï¼ˆæ—‹è½¬+å¹³ç§»+æŠ•å½±ï¼‰ç»„åˆæˆå•ä¸ªçŸ©é˜µä¹˜æ³•ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Linear algebra enables all real-time graphics: video games, simulations, CAD software, and movie effects. By chaining simple matrix operations, complex transformations are applied efficiently to millions of points per second.
çº¿æ€§ä»£æ•°æ”¯æŒæ‰€æœ‰å®æ—¶å›¾å½¢ï¼šè§†é¢‘æ¸¸æˆã€æ¨¡æ‹Ÿã€CAD è½¯ä»¶å’Œç”µå½±ç‰¹æ•ˆã€‚é€šè¿‡é“¾æ¥ç®€å•çš„çŸ©é˜µè¿ç®—ï¼Œå¤æ‚çš„å˜æ¢å¯ä»¥é«˜æ•ˆåœ°åº”ç”¨äºæ¯ç§’æ•°ç™¾ä¸‡ä¸ªç‚¹ã€‚

### Exercises 10.1
ç»ƒä¹ 10.1

1.  Write the rotation matrix for a 90Â° counterclockwise rotation in $\mathbb{R}^2$. Apply it to $(1,0)$.
    åœ¨ $\mathbb{R}^2$ ä¸­å†™å‡ºé€†æ—¶é’ˆæ—‹è½¬ 90Â° çš„æ—‹è½¬çŸ©é˜µã€‚å°†å…¶åº”ç”¨åˆ° $(1,0)$ ã€‚
2.  Rotate the point $(1,1,0)$ about the $z$\-axis by 180Â°.
    å°†ç‚¹ $(1,1,0)$ ç»• $z$ è½´æ—‹è½¬ 180Â°ã€‚
3.  Show that the determinant of any 2D or 3D rotation matrix is 1.
    è¯æ˜ä»»ä½•äºŒç»´æˆ–ä¸‰ç»´æ—‹è½¬çŸ©é˜µçš„è¡Œåˆ—å¼ä¸º 1ã€‚
4.  Derive the orthogonal projection matrix from $\mathbb{R}^3$ to the $xy$\-plane.
    æ¨å¯¼ä» $\mathbb{R}^3$ åˆ° $xy$ å¹³é¢çš„æ­£äº¤æŠ•å½±çŸ©é˜µã€‚
5.  Explain how homogeneous coordinates allow translations to be represented as matrix multiplications.
    è§£é‡Šé½æ¬¡åæ ‡å¦‚ä½•å…è®¸å¹³ç§»è¡¨ç¤ºä¸ºçŸ©é˜µä¹˜æ³•ã€‚

## 10.2 Data Science (Dimensionality Reduction, Least Squares)
10.2 æ•°æ®ç§‘å­¦ï¼ˆé™ç»´ã€æœ€å°äºŒä¹˜ï¼‰

Linear algebra provides the foundation for many data science techniques. Two of the most important are dimensionality reduction, where high-dimensional datasets are compressed while preserving essential information, and the least squares method, which underlies regression and model fitting.
çº¿æ€§ä»£æ•°ä¸ºè®¸å¤šæ•°æ®ç§‘å­¦æŠ€æœ¯å¥ å®šäº†åŸºç¡€ã€‚å…¶ä¸­æœ€é‡è¦çš„ä¸¤ä¸ªæŠ€æœ¯æ˜¯é™ç»´ï¼ˆåœ¨ä¿ç•™åŸºæœ¬ä¿¡æ¯çš„åŒæ—¶å‹ç¼©é«˜ç»´æ•°æ®é›†ï¼‰å’Œæœ€å°äºŒä¹˜æ³•ï¼ˆå›å½’å’Œæ¨¡å‹æ‹Ÿåˆçš„åŸºç¡€ï¼‰ã€‚

### Dimensionality Reduction
é™ç»´

High-dimensional data often contains redundancy: many features are correlated, meaning the data essentially lies near a lower-dimensional subspace. Dimensionality reduction identifies these subspaces.
é«˜ç»´æ•°æ®é€šå¸¸åŒ…å«å†—ä½™ï¼šè®¸å¤šç‰¹å¾ç›¸äº’å…³è”ï¼Œè¿™æ„å‘³ç€æ•°æ®æœ¬è´¨ä¸Šä½äºä½ç»´å­ç©ºé—´é™„è¿‘ã€‚é™ç»´å¯ä»¥è¯†åˆ«è¿™äº›å­ç©ºé—´ã€‚

*   PCA (Principal Component Analysis): As introduced earlier, PCA diagonalizes the covariance matrix of the data.
    PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰ï¼šå¦‚å‰æ‰€è¿°ï¼ŒPCA å°†æ•°æ®çš„åæ–¹å·®çŸ©é˜µå¯¹è§’åŒ–ã€‚
    
    *   Eigenvectors (principal components) define orthogonal directions of maximum variance.
        ç‰¹å¾å‘é‡ï¼ˆä¸»æˆåˆ†ï¼‰å®šä¹‰æœ€å¤§æ–¹å·®çš„æ­£äº¤æ–¹å‘ã€‚
    *   Eigenvalues measure how much variance lies along each direction.
        ç‰¹å¾å€¼è¡¡é‡æ¯ä¸ªæ–¹å‘ä¸Šçš„æ–¹å·®ã€‚
    *   Keeping only the top $k$ components reduces data from $n$\-dimensional space to $k$\-dimensional space while retaining most variability.
        ä»…ä¿ç•™å‰ $k$ ä¸ªæˆåˆ†å¯å°†æ•°æ®ä» $n$ ç»´ç©ºé—´å‡å°‘åˆ° $k$ ç»´ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å¤§éƒ¨åˆ†å¯å˜æ€§ã€‚

Example 10.2.1. A dataset of 1000 images, each with 1024 pixels, may have most variance captured by just 50 eigenvectors of the covariance matrix. Projecting onto these components compresses the data while preserving essential features.
ä¾‹ 10.2.1ã€‚ä¸€ä¸ªåŒ…å« 1000 å¹…å›¾åƒçš„æ•°æ®é›†ï¼Œæ¯å¹…å›¾åƒæœ‰ 1024 ä¸ªåƒç´ ï¼Œå…¶å¤§éƒ¨åˆ†æ–¹å·®å¯èƒ½ä»…ç”±åæ–¹å·®çŸ©é˜µçš„ 50 ä¸ªç‰¹å¾å‘é‡æ•è·ã€‚æŠ•å½±åˆ°è¿™äº›åˆ†é‡ä¸Šå¯ä»¥å‹ç¼©æ•°æ®ï¼ŒåŒæ—¶ä¿ç•™åŸºæœ¬ç‰¹å¾ã€‚

### Least Squares
æœ€å°äºŒä¹˜æ³•

Often, we have more equations than unknowns-an overdetermined system:
é€šå¸¸ï¼Œæˆ‘ä»¬çš„æ–¹ç¨‹æ¯”æœªçŸ¥æ•°è¿˜å¤šâ€”â€”ä¸€ä¸ªè¶…å®šç³»ç»Ÿï¼š

$$
A\mathbf{x} \approx \mathbf{b}, \quad A \in \mathbb{R}^{m \times n}, \ m > n.
$$

An exact solution may not exist. Instead, we seek $\mathbf{x}$ that minimizes the error
ç²¾ç¡®è§£å¯èƒ½ä¸å­˜åœ¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯»æ±‚æœ€å°åŒ–è¯¯å·®çš„ $\mathbf{x}$

$$
\|A\mathbf{x} - \mathbf{b}\|^2.
$$

This leads to the normal equations:
è¿™å¯¼è‡´äº†æ­£è§„æ–¹ç¨‹ï¼š

$$
A^T A \mathbf{x} = A^T \mathbf{b}.
$$

The solution is the orthogonal projection of $\mathbf{b}$ onto the column space of $A$.
è§£å†³æ–¹æ¡ˆæ˜¯å°† $\mathbf{b}$ æ­£äº¤æŠ•å½±åˆ° $A$ çš„åˆ—ç©ºé—´ä¸Šã€‚

### Example 10.2.2
ä¾‹ 10.2.2

Fit a line $y = mx + c$ to data points $(x_i, y_i)$.
å°†çº¿ $y = mx + c$ ä¸æ•°æ®ç‚¹ $(x_i, y_i)$ æ‹Ÿåˆã€‚

Matrix form:
çŸ©é˜µå½¢å¼ï¼š

$$
A = \begin{bmatrix}x_1 & 1 \\x_2 & 1 \\\vdots & \vdots \\x_m & 1\end{bmatrix},\quad\mathbf{b} =\begin{bmatrix}y_1 \\y_2 \\\vdots \\y_m \end{bmatrix},\quad\mathbf{x} =\begin{bmatrix}m \\c \end{bmatrix}.
$$

Solve $A^T A \mathbf{x} = A^T \mathbf{b}$. This yields the best-fit line in the least squares sense.
æ±‚è§£ $A^T A \mathbf{x} = A^T \mathbf{b}$ ã€‚è¿™å°†å¾—å‡ºæœ€å°äºŒä¹˜æ„ä¹‰ä¸Šçš„æœ€ä½³æ‹Ÿåˆçº¿ã€‚

### Geometric Interpretation
å‡ ä½•è§£é‡Š

*   Dimensionality reduction: Find the best subspace capturing most variance.
    é™ç»´ï¼šæ‰¾åˆ°æ•è·æœ€å¤šæ–¹å·®çš„æœ€ä½³å­ç©ºé—´ã€‚
*   Least squares: Project the target vector onto the subspace spanned by predictors.
    æœ€å°äºŒä¹˜ï¼šå°†ç›®æ ‡å‘é‡æŠ•å½±åˆ°é¢„æµ‹å˜é‡æ‰€è·¨è¶Šçš„å­ç©ºé—´ä¸Šã€‚

Both are projection problems, solved using inner products and orthogonality.
ä¸¤è€…éƒ½æ˜¯æŠ•å½±é—®é¢˜ï¼Œä½¿ç”¨å†…ç§¯å’Œæ­£äº¤æ€§æ¥è§£å†³ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Dimensionality reduction makes large datasets tractable, filters noise, and reveals structure. Least squares fitting powers regression, statistics, and machine learning. Both rely directly on eigenvalues, eigenvectors, and projections-core tools of linear algebra.
é™ç»´ä½¿å¤§å‹æ•°æ®é›†æ›´æ˜“äºå¤„ç†ï¼Œè¿‡æ»¤å™ªå£°å¹¶æ­ç¤ºç»“æ„ã€‚æœ€å°äºŒä¹˜æ‹Ÿåˆä¸ºå›å½’ã€ç»Ÿè®¡å’Œæœºå™¨å­¦ä¹ æä¾›æ”¯æŒã€‚ä¸¤è€…éƒ½ç›´æ¥ä¾èµ–äºç‰¹å¾å€¼ã€ç‰¹å¾å‘é‡å’ŒæŠ•å½±â€”â€”çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒå·¥å…·ã€‚

### Exercises 10.2
ç»ƒä¹ 10.2

1.  Explain why PCA reduces noise in datasets by discarding small eigenvalue components.
    è§£é‡Šä¸ºä»€ä¹ˆ PCA é€šè¿‡ä¸¢å¼ƒè¾ƒå°çš„ç‰¹å¾å€¼åˆ†é‡æ¥å‡å°‘æ•°æ®é›†ä¸­çš„å™ªå£°ã€‚
2.  Compute the least squares solution to fitting a line through $(0,0), (1,1), (2,2)$.
    è®¡ç®—é€šè¿‡ $(0,0), (1,1), (2,2)$ æ‹Ÿåˆç›´çº¿çš„æœ€å°äºŒä¹˜è§£ã€‚
3.  Show that the least squares solution is unique if and only if $A^T A$ is invertible.
    è¯æ˜æœ€å°äºŒä¹˜è§£æ˜¯å”¯ä¸€çš„å½“ä¸”ä»…å½“ $A^T A$ å¯é€†ã€‚
4.  Prove that the least squares solution minimizes the squared error by projection arguments.
    è¯æ˜æœ€å°äºŒä¹˜è§£é€šè¿‡æŠ•å½±å‚æ•°æœ€å°åŒ–å¹³æ–¹è¯¯å·®ã€‚
5.  Apply PCA to the data points $(1,0), (2,1), (3,2)$ and find the first principal component.
    å°† PCA åº”ç”¨äºæ•°æ®ç‚¹ $(1,0), (2,1), (3,2)$ å¹¶æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸»æˆåˆ†ã€‚

## 10.3 Networks and Markov Chains
10.3 ç½‘ç»œå’Œé©¬å°”å¯å¤«é“¾

Graphs and networks provide a natural setting where linear algebra comes to life. From modeling flows and connectivity to predicting long-term behavior, matrices translate network structure into algebraic form. Markov chains, already introduced in Section 8.4, are a central example of networks evolving over time.
å›¾å’Œç½‘ç»œä¸ºçº¿æ€§ä»£æ•°çš„è¿ç”¨æä¾›äº†è‡ªç„¶çš„å¹³å°ã€‚ä»å»ºæ¨¡æµå’Œè¿æ¥åˆ°é¢„æµ‹é•¿æœŸè¡Œä¸ºï¼ŒçŸ©é˜µå°†ç½‘ç»œç»“æ„è½¬åŒ–ä¸ºä»£æ•°å½¢å¼ã€‚é©¬å°”å¯å¤«é“¾ï¼ˆå·²åœ¨ 8.4 èŠ‚ä»‹ç»ï¼‰æ˜¯ç½‘ç»œéšæ—¶é—´æ¼”åŒ–çš„ä¸€ä¸ªå…¸å‹ä¾‹å­ã€‚

### Adjacency Matrices
é‚»æ¥çŸ©é˜µ

A network (graph) with $n$ nodes can be represented by an adjacency matrix $A \in \mathbb{R}^{n \times n}$:
å…·æœ‰ $n$ ä¸ªèŠ‚ç‚¹çš„ç½‘ç»œï¼ˆå›¾ï¼‰å¯ä»¥ç”¨é‚»æ¥çŸ©é˜µ $A \in \mathbb{R}^{n \times n}$ è¡¨ç¤ºï¼š

$$
A_{ij} =\begin{cases}1 & \text{if there is an edge from node \(i\) to node \(j\)} \\0 & \text{otherwise.}\end{cases}
$$

For weighted graphs, entries may be positive weights instead of 0/1.
å¯¹äºåŠ æƒå›¾ï¼Œæ¡ç›®å¯èƒ½æ˜¯æ­£æƒé‡è€Œä¸æ˜¯ 0/1 ã€‚

*   The number of walks of length $k$ from node $i$ to node $j$ is given by the entry $(A^k)_{ij}$.
    ä»èŠ‚ç‚¹ $i$ åˆ°èŠ‚ç‚¹ $j$ çš„é•¿åº¦ä¸º $k$ çš„æ­¥è¡Œæ¬¡æ•°ç”±æ¡ç›® ( ğ´ ğ‘˜ ) ğ‘– ğ‘— ï¼ˆä¸€ä¸ª k ) ä¼Šå¥‡ â€‹ .
*   Powers of adjacency matrices thus encode connectivity over time.
    å› æ­¤ï¼Œé‚»æ¥çŸ©é˜µçš„å¹‚å¯ä»¥å¯¹éšæ—¶é—´å˜åŒ–çš„è¿é€šæ€§è¿›è¡Œç¼–ç ã€‚

### Laplacian Matrices
æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ

Another important matrix is the graph Laplacian:
å¦ä¸€ä¸ªé‡è¦çš„çŸ©é˜µæ˜¯å›¾æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µï¼š

$$
L = D - A,
$$

where $D$ is the diagonal degree matrix ( $D_{ii} = \text{degree}(i)$ ).
å…¶ä¸­ $D$ æ˜¯å¯¹è§’åº¦çŸ©é˜µ ( $D_{ii} = \text{degree}(i)$ )ã€‚

*   $L$ is symmetric and positive semidefinite.
    $L$ æ˜¯å¯¹ç§°çš„å¹¶ä¸”æ˜¯æ­£åŠå®šçš„ã€‚
*   The smallest eigenvalue is always $0$, with eigenvector $(1,1,\\dots,1)$.
    æœ€å°ç‰¹å¾å€¼å§‹ç»ˆæ˜¯ $0 $, with eigenvector $ (1,1,\\dots,1)$ã€‚
*   The multiplicity of eigenvalue 0 equals the number of connected components in the graph.
    ç‰¹å¾å€¼ 0 çš„å¤šé‡æ€§ç­‰äºå›¾ä¸­è¿é€šåˆ†é‡çš„æ•°é‡ã€‚

This connection between eigenvalues and connectivity forms the basis of spectral graph theory.
ç‰¹å¾å€¼å’Œè¿é€šæ€§ä¹‹é—´çš„è¿™ç§è”ç³»æ„æˆäº†è°±å›¾ç†è®ºçš„åŸºç¡€ã€‚

### Markov Chains on Graphs
å›¾ä¸Šçš„é©¬å°”å¯å¤«é“¾

A Markov chain can be viewed as a random walk on a graph. If $P$ is the transition matrix where $P_{ij}$ is the probability of moving from node $i$ to node $j$, then
é©¬å°”å¯å¤«é“¾å¯ä»¥çœ‹ä½œå›¾ä¸Šçš„éšæœºæ¸¸åŠ¨ã€‚è®¾ $P$ ä¸ºè½¬ç§»çŸ©é˜µï¼Œå…¶ä¸­ ğ‘ƒ ğ‘– ğ‘— P ä¼Šå¥‡ â€‹ æ˜¯ä»èŠ‚ç‚¹ $i$ ç§»åŠ¨åˆ°èŠ‚ç‚¹ $j$ çš„æ¦‚ç‡ï¼Œé‚£ä¹ˆ

$$
\mathbf{x}_{k+1} = P \mathbf{x}_k
$$

describes the distribution of positions after $k$ steps.
æè¿° $k$ æ­¥ä¹‹åçš„ä½ç½®åˆ†å¸ƒã€‚

*   The steady-state distribution is given by the eigenvector of $P$ with eigenvalue 1.
    ç¨³æ€åˆ†å¸ƒç”±ç‰¹å¾å‘é‡ $P$ ç»™å‡ºï¼Œç‰¹å¾å€¼ä¸º 1 ã€‚
*   The speed of convergence depends on the gap between the largest eigenvalue (which is always 1) and the second largest eigenvalue.
    æ”¶æ•›é€Ÿåº¦å–å†³äºæœ€å¤§ç‰¹å¾å€¼ï¼ˆå§‹ç»ˆä¸º 1 ï¼‰ä¸ç¬¬äºŒå¤§ç‰¹å¾å€¼ä¹‹é—´çš„å·®è·ã€‚

### Example 10.3.1
ä¾‹ 10.3.1

Consider a simple 3-node cycle graph:
è€ƒè™‘ä¸€ä¸ªç®€å•çš„ 3 èŠ‚ç‚¹å¾ªç¯å›¾ï¼š

$$
P = \begin{bmatrix}0 & 1 & 0 \\0 & 0 & 1 \\1 & 0 & 0\end{bmatrix}.
$$

This Markov chain cycles deterministically among the nodes. Eigenvalues are the cube roots of unity: $1, e^{2\\pi i/3}, e^{4\\pi i/3}. The eigenvalue \\1$corresponds to the steady state, which is the uniformdistribution$(1/3,1/3,1/3)$.
è¿™ä¸ªé©¬å°”å¯å¤«é“¾åœ¨èŠ‚ç‚¹ä¹‹é—´ç¡®å®šæ€§åœ°å¾ªç¯ã€‚ç‰¹å¾å€¼æ˜¯ ç»Ÿä¸€ï¼š$1,e^{2\\pi i/3},e^{4\\pi i/3} ã€‚ç‰¹å¾å€¼ \\ 1 $corresponds to the steady state, which is the uniformdistribution$ (1/3,1/3,1/3)$ã€‚

### Applications
åº”ç”¨

*   Search engines: Googleâ€™s PageRank algorithm models the web as a Markov chain, where steady-state probabilities rank pages.
    æœç´¢å¼•æ“ï¼šGoogle çš„ PageRank ç®—æ³•å°†ç½‘ç»œå»ºæ¨¡ä¸ºé©¬å°”å¯å¤«é“¾ï¼Œå…¶ä¸­ç¨³æ€æ¦‚ç‡å¯¹ç½‘é¡µè¿›è¡Œæ’åã€‚
*   Network analysis: Eigenvalues of adjacency or Laplacian matrices reveal communities, bottlenecks, and robustness.
    ç½‘ç»œåˆ†æï¼šé‚»æ¥çŸ©é˜µæˆ–æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾å€¼æ­ç¤ºç¤¾åŒºã€ç“¶é¢ˆå’Œç¨³å¥æ€§ã€‚
*   Epidemiology and information flow: Random walks model how diseases or ideas spread through networks.
    æµè¡Œç—…å­¦å’Œä¿¡æ¯æµï¼šéšæœºæ¸¸åŠ¨æ¨¡æ‹Ÿç–¾ç—…æˆ–æ€æƒ³å¦‚ä½•é€šè¿‡ç½‘ç»œä¼ æ’­ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Linear algebra transforms network problems into matrix problems. Eigenvalues and eigenvectors reveal connectivity, flow, stability, and long-term dynamics. Networks are everywhere-social media, biology, finance, and the internet-so these tools are indispensable.
çº¿æ€§ä»£æ•°å°†ç½‘ç»œé—®é¢˜è½¬åŒ–ä¸ºçŸ©é˜µé—®é¢˜ã€‚ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æ­ç¤ºäº†è¿é€šæ€§ã€æµåŠ¨ã€ç¨³å®šæ€§å’Œé•¿æœŸåŠ¨æ€ã€‚ç½‘ç»œæ— å¤„ä¸åœ¨â€”â€”ç¤¾äº¤åª’ä½“ã€ç”Ÿç‰©ã€é‡‘èå’Œäº’è”ç½‘â€”â€”å› æ­¤è¿™äº›å·¥å…·ä¸å¯æˆ–ç¼ºã€‚

### Exercises 10.3
ç»ƒä¹ 10.3

1.  Write the adjacency matrix of a square graph with 4 nodes. Compute $A^2$ and interpret the entries.
    å†™å‡ºä¸€ä¸ªæœ‰ 4 ä¸ªèŠ‚ç‚¹çš„æ­£æ–¹å½¢å›¾çš„é‚»æ¥çŸ©é˜µã€‚è®¡ç®— $A^2$ å¹¶è§£é‡Šå…¶ä¸­çš„å…ƒç´ ã€‚
    
2.  Show that the Laplacian of a connected graph has exactly one zero eigenvalue.
    è¯æ˜è¿é€šå›¾çš„æ‹‰æ™®æ‹‰æ–¯ç®—å­æ°å¥½æœ‰ä¸€ä¸ªé›¶ç‰¹å¾å€¼ã€‚
    
3.  Find the steady-state distribution of the Markov chain with
    æ‰¾åˆ°é©¬å°”å¯å¤«é“¾çš„ç¨³æ€åˆ†å¸ƒ
    
    $$
    P = \begin{bmatrix} 0.5 & 0.5 \\ 0.4 & 0.6 \end{bmatrix}.
    $$
    
4.  Explain how eigenvalues of the Laplacian can detect disconnected components of a graph.
    è§£é‡Šæ‹‰æ™®æ‹‰æ–¯ç®—å­çš„ç‰¹å¾å€¼å¦‚ä½•æ£€æµ‹å›¾ä¸­ä¸è¿ç»­çš„ç»„æˆéƒ¨åˆ†ã€‚
    
5.  Describe how PageRank modifies the transition matrix of the web graph to ensure a unique steady-state distribution.
    æè¿° PageRank å¦‚ä½•ä¿®æ”¹ç½‘ç»œå›¾çš„è½¬æ¢çŸ©é˜µä»¥ç¡®ä¿å”¯ä¸€çš„ç¨³æ€åˆ†å¸ƒã€‚
    

## 10.4 Machine Learning Connections
10.4 æœºå™¨å­¦ä¹ è¿æ¥

Modern machine learning is built on linear algebra. From the representation of data as matrices to the optimization of large-scale models, nearly every step relies on concepts such as vector spaces, projections, eigenvalues, and matrix decompositions.
ç°ä»£æœºå™¨å­¦ä¹ å»ºç«‹åœ¨çº¿æ€§ä»£æ•°çš„åŸºç¡€ä¸Šã€‚ä»æ•°æ®çŸ©é˜µè¡¨ç¤ºåˆ°å¤§è§„æ¨¡æ¨¡å‹çš„ä¼˜åŒ–ï¼Œå‡ ä¹æ¯ä¸€æ­¥éƒ½ä¾èµ–äºå‘é‡ç©ºé—´ã€æŠ•å½±ã€ç‰¹å¾å€¼å’ŒçŸ©é˜µåˆ†è§£ç­‰æ¦‚å¿µã€‚

### Data as Matrices
æ•°æ®ä½œä¸ºçŸ©é˜µ

A dataset with $m$ examples and $n$ features is represented as a matrix $X \in \mathbb{R}^{m \times n}$:
å…·æœ‰ $m$ ä¸ªç¤ºä¾‹å’Œ $n$ ä¸ªç‰¹å¾çš„æ•°æ®é›†è¡¨ç¤ºä¸ºçŸ©é˜µ $X \in \mathbb{R}^{m \times n}$ ï¼š

$$
X =\begin{bmatrix}\- & \mathbf{x}_1^T & - \\\- & \mathbf{x}_2^T & - \\& \vdots & \\\- & \mathbf{x}_m^T & -\end{bmatrix},
$$

where each row $\mathbf{x}_i \in \mathbb{R}^n$ is a feature vector. Linear algebra provides tools to analyze, compress, and transform this data.
å…¶ä¸­æ¯è¡Œ $\mathbf{x}_i \in \mathbb{R}^n$ æ˜¯ä¸€ä¸ªç‰¹å¾å‘é‡ã€‚çº¿æ€§ä»£æ•°æä¾›äº†åˆ†æã€å‹ç¼©å’Œè½¬æ¢æ­¤ç±»æ•°æ®çš„å·¥å…·ã€‚

### Linear Models
çº¿æ€§æ¨¡å‹

At the heart of machine learning are linear predictors:
æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ˜¯çº¿æ€§é¢„æµ‹å™¨ï¼š

$\hat{y} = X\mathbf{w},$

where $\mathbf{w}$ is the weight vector. Training often involves solving a least squares problem or a regularized variant such as ridge regression:
å…¶ä¸­ $\mathbf{w}$ æ˜¯æƒé‡å‘é‡ã€‚è®­ç»ƒé€šå¸¸æ¶‰åŠæ±‚è§£æœ€å°äºŒä¹˜é—®é¢˜æˆ–æ­£åˆ™åŒ–å˜ä½“ï¼Œä¾‹å¦‚å²­å›å½’ï¼š

$\min_{\mathbf{w}} \|X\mathbf{w} - \mathbf{y}\|^2 + \lambda \|\mathbf{w}\|^2.$

This is solved efficiently using matrix factorizations.
ä½¿ç”¨çŸ©é˜µåˆ†è§£å¯ä»¥æœ‰æ•ˆåœ°è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

### Singular Value Decomposition (SVD)
å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰

The SVD of a matrix $X$ is
çŸ©é˜µ $X$ çš„ SVD ä¸º

$X = U \Sigma V^T,$

where $U, V$ are orthogonal and $\Sigma$ is diagonal with nonnegative entries (singular values).
å…¶ä¸­ $U, V$ æ˜¯æ­£äº¤çš„ï¼Œ $\Sigma$ æ˜¯å¯¹è§’çš„ï¼Œå…·æœ‰éè´Ÿé¡¹ï¼ˆå¥‡å¼‚å€¼ï¼‰ã€‚

*   Singular values measure the importance of directions in feature space.
    å¥‡å¼‚å€¼è¡¡é‡ç‰¹å¾ç©ºé—´ä¸­æ–¹å‘çš„é‡è¦æ€§ã€‚
*   SVD is used for dimensionality reduction (low-rank approximations), topic modeling, and recommender systems.
    SVD ç”¨äºé™ç»´ï¼ˆä½ç§©è¿‘ä¼¼ï¼‰ã€ä¸»é¢˜å»ºæ¨¡å’Œæ¨èç³»ç»Ÿã€‚

### Eigenvalues in Machine Learning
æœºå™¨å­¦ä¹ ä¸­çš„ç‰¹å¾å€¼

*   PCA (Principal Component Analysis): diagonalization of the covariance matrix identifies directions of maximal variance.
    PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰ï¼šåæ–¹å·®çŸ©é˜µçš„å¯¹è§’åŒ–ç¡®å®šäº†æœ€å¤§æ–¹å·®çš„æ–¹å‘ã€‚
*   Spectral clustering: uses eigenvectors of the Laplacian to group data points into clusters.
    è°±èšç±»ï¼šä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯ç®—å­çš„ç‰¹å¾å‘é‡å°†æ•°æ®ç‚¹åˆ†ç»„æˆèšç±»ã€‚
*   Stability analysis: eigenvalues of Hessian matrices determine whether optimization converges to a minimum.
    ç¨³å®šæ€§åˆ†æï¼šHessian çŸ©é˜µçš„ç‰¹å¾å€¼å†³å®šä¼˜åŒ–æ˜¯å¦æ”¶æ•›åˆ°æœ€å°å€¼ã€‚

### Neural Networks
ç¥ç»ç½‘ç»œ

Even deep learning, though nonlinear, uses linear algebra at its core:
å³ä½¿æ˜¯æ·±åº¦å­¦ä¹ ï¼Œå°½ç®¡æ˜¯éçº¿æ€§çš„ï¼Œå…¶æ ¸å¿ƒä¹Ÿä½¿ç”¨çº¿æ€§ä»£æ•°ï¼š

*   Each layer is a matrix multiplication followed by a nonlinear activation.
    æ¯ä¸€å±‚éƒ½æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œç„¶åæ˜¯éçº¿æ€§æ¿€æ´»ã€‚
*   Training requires computing gradients, which are expressed in terms of matrix calculus.
    è®­ç»ƒéœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œä»¥çŸ©é˜µå¾®ç§¯åˆ†æ¥è¡¨ç¤ºã€‚
*   Backpropagation is essentially repeated applications of the chain rule with linear algebra.
    åå‘ä¼ æ’­æœ¬è´¨ä¸Šæ˜¯é“¾å¼æ³•åˆ™ä¸çº¿æ€§ä»£æ•°çš„é‡å¤åº”ç”¨ã€‚

### Why this matters
ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦

Machine learning models often involve datasets with millions of features and parameters. Linear algebra provides the algorithms and abstractions that make training and inference possible. Without it, large-scale computation in AI would be intractable.
æœºå™¨å­¦ä¹ æ¨¡å‹é€šå¸¸æ¶‰åŠå…·æœ‰æ•°ç™¾ä¸‡ä¸ªç‰¹å¾å’Œå‚æ•°çš„æ•°æ®é›†ã€‚çº¿æ€§ä»£æ•°æä¾›äº†ä½¿è®­ç»ƒå’Œæ¨ç†æˆä¸ºå¯èƒ½çš„ç®—æ³•å’ŒæŠ½è±¡ã€‚å¦‚æœæ²¡æœ‰å®ƒï¼Œäººå·¥æ™ºèƒ½ä¸­çš„å¤§è§„æ¨¡è®¡ç®—å°†å˜å¾—éš¾ä»¥å¤„ç†ã€‚

### Exercises 10.4
ç»ƒä¹ 10.4

1.  Show that ridge regression leads to the normal equations
    è¯æ˜å²­å›å½’å¯ä»¥å¾—å‡ºæ­£æ€æ–¹ç¨‹

$$
(X^T X + \lambda I)\mathbf{w} = X^T \mathbf{y}.
$$

2.  Explain how SVD can be used to compress an image represented as a matrix of pixel intensities.
    è§£é‡Šå¦‚ä½•ä½¿ç”¨ SVD æ¥å‹ç¼©ä»¥åƒç´ å¼ºåº¦çŸ©é˜µè¡¨ç¤ºçš„å›¾åƒã€‚
    
3.  For a covariance matrix $\Sigma$, show why its eigenvalues represent variances along principal components.
    å¯¹äºåæ–¹å·®çŸ©é˜µ $\Sigma$ ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆå®ƒçš„ç‰¹å¾å€¼è¡¨ç¤ºæ²¿ä¸»æˆåˆ†çš„æ–¹å·®ã€‚
    
4.  Give an example of how eigenvectors of the Laplacian matrix can be used for clustering a small graph.
    ä¸¾ä¾‹è¯´æ˜å¦‚ä½•ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾å‘é‡å¯¹å°å›¾è¿›è¡Œèšç±»ã€‚
    
5.  In a neural network with one hidden layer, write the forward pass in matrix form.
    åœ¨å…·æœ‰ä¸€ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œä¸­ï¼Œä»¥çŸ©é˜µå½¢å¼å†™å‡ºå‰å‘ä¼ é€’ã€‚